[
  {
    "question": "What kind of tasks can robots perform using language-vision data?",
    "chunk": "Table 5.Summary of the reviewed papers in this study.\nName Explanation Ref.\nReward Design in RL\n• Eureka automatically generates and improves reward\nfunctions based on the virtual environment source\ncode.\n• DrEureka builds reward-aware physics priors using\nEureka and supports effective operation in the real\nworld through domain randomization.\n• LLMs design and reﬁne reward functions based on\nnatural language input.\n• LLMs and VLMs integrate multimodal data to\ngenerate reward functions.\n[11,134,136–139,176–180]\nLow-level\nControl\n• Generating commands to control actuators capable of\nlow-level control.\n• RT-1 and RT-2 enable robots to perform complex tasks\nbased on language-vision data.\n• AutoRT establishes a system where robots can\nautonomously collect and utilize data.\n[8–10,144–148,181–183]\nHigh-level\nPlanning\n• LLMs provide an effective methodology for tasks\nrelated to high-level planning within robotic systems.\n• By using natural language, LLMs can formulate plans\nto solve tasks that require long-horizon reasoning.\n• LLMs assess the feasibility of actions to determine and\nexecute the optimal robotic behavior.\n• LLMs generate behavior trees to structure complex\nrobotic actions accurately.\n[149–160,184–207]\nManipulation\n• Using LLMs and VLMs to integrate language and\nvision data allows various manipulations.\n• LLMs interpret high-level instructions to generate the\nnecessary robot actions and assess their feasibility.\n• VLMs extract object information from images to assist\nin performing manipulations.\n[161–167,208–215]\nScene\nUnderstanding\n• To solve VQA problems, use VLMs to extract\nhigh-level information from vision data.\n• For scene understanding, estimate and identify objects\nand evaluate relationships between objects.\n• For navigation, convert natural language instructions\nand combine them with vision data to identify the\nimage through probability distributions.\n[168–175,216–223]\n5. Discussion and Future Directions\nThe review revealed two potentials of foundation models: (1) commonsense reasoning\nfor planning and (2) the ability to generate code.\nThe ﬁrst ﬁnding from this review study is the potential to enhance robot intelligence\nthrough foundation models. Beyond the studies mentioned here, numerous recent stud-\nies have shown that pre-trained models such as LLMs and VLMs can enhance various\naspects of robot intelligence, such as situational awareness, high-level task planning, and\nhuman interaction. LLMs allow communication with humans in natural languages, objectAppl. Sci.2024, 14, 8868 28 of 39\nutilization based on extensive information, and high-level planning using that information.\nVLMs can describe tasks in text and understand visual information. Furthermore, the\ninformation from VLMs can be supplemented by connecting to knowledge databases via\nLLMs. These capabilities are crucial for enhancing robot intelligence, broadening the scope\nof robot applications, and maximizing robot utility.\nThe second ﬁnding is the code generation capability of LLMs, which has the potential\nto automate the robot development process traditionally performed by humans. Addi-\ntionally, robots that can autonomously update their own algorithms are no longer just\nscience ﬁction. Although limitations exist for robots to self-update, frameworks such as\nEureka and DrEureka, which automatically enhanced reinforcement learning performance\nfor robot motion control, demonstrate the potential for future advancements. This suggests\nthat LLMs may not only enhance human interactions but could also pave the way for\nself-improvement without human intervention."
  },
  {
    "question": "Can you explain how LLMs are used in robot task planning?",
    "chunk": "aim of this diversity evaluation is to conﬁrm that, unlike simulations, real-world data\ncollection by robots is labor-intensive, making it essential to gather data across a broad\nspectrum of tasks. Experimental outcomes illustrate that AutoRT achieves higher visual\nand linguistic diversity compared to RT-1 or BC-Z [143].\nOther researchers include Tang [144], who developed an approach that connects\nnatural language user commands with a locomotion controller using foot contact patterns as\nan interface for low-level commands. This innovative interface translates human commands\ninto the robot’s foot contact patterns, allowing the robot to move at a speciﬁed speed with\nprecise timing for each foot’s contact with the ground. To achieve this, the robot used a\ncyclic sliding window to extract foot contact ﬂags from a pattern template, thus generating\nthe required foot contact patterns. During training, a random pattern generator created\nfoot contact patterns, and during testing, an LLM translated human commands into these\npatterns. The robot then adjusted its movements based on the foot contact patterns it\nlearned through deep reinforcement learning, closely adhering to the intended foot contact\npatterns and speed commands. This approach demonstrated a 50% higher success rate in\ntask evaluation (across 30 tasks, including standing still) compared to two baselines (which\nemployed discrete gaits and sinusoidal functions as interfaces), successfully solving 10\nmore tasks than the baselines.\nMandi [145] introduced a novel method for multi-robot collaboration that utilizes\nLLMs for both high-level communication and low-level path planning. In this method, the\nrobots employ the LLM to discuss and reason about task strategies. They generate sub-taskAppl. Sci.2024, 14, 8868 16 of 39\nplans and task space waypoint paths, which a multi-arm motion planner then uses to expe-\ndite trajectory planning. Additionally, environmental feedback, such as collision detection,\nprompts the LLM agent to reﬁne plans and waypoints contextually. This method achieved\na high success rate across all tasks in the RoCoBench (including duties such as sweeping\nthe ﬂoor), effectively adapting to variations in task semantics. In real-world experiments,\nspeciﬁcally the block-sorting task, RoCo demonstrated its ability to communicate and\ncollaborate with other robot agents to successfully complete the tasks.\nWang [146] proposed a novel paradigm for utilizing few-shot prompts in physical\nenvironments. This method involved gathering observation and action pairs from existing\nmodel-based or learning-based controllers to form the initial text prompts. Data included\nsensor readings, such as IMU and joint encoders, coupled with target joint positions.\nThese data formed the starting input for LLM inference. As the robot interacted with its\nenvironment and collected new observational data, these initial data were updated with\noutputs from the LLM. In the subsequent prompt engineering phase, observation and action\npairs, along with explanatory prompts, were crafted to enable the LLM to function as a\nfeedback policy. The explanatory prompts provided clear descriptions of the robot walking\ntask and control design details, while the observation and action prompts delineated the\nformat and signiﬁcance of each observation and action. This method allowed the LLM\nto directly output low-level target joint positions for robot walking. The approach was\ntested using the ANYmal robot in MuJoCo and Isaac Gym simulators for robot walking,\nindicating that the LLM could act as a low-level feedback controller for dynamic motion\ncontrol within sophisticated robot systems.\nLiang [147] introduced a new framework namedCode as Policies(CaP) that directly\nconstructs robot policies from executable code generated by a code LLM. This framework"
  },
  {
    "question": "How does Instruct2Act improve robot action mapping?",
    "chunk": "In a real kitchen setting, the plan success rate decreased slightly to 81% and the execution\nsuccess rate fell to 60%, demonstrating that the policy and value functions generalize well\nto real-world settings.\nHuang [167] introduced the Instruct2Act framework, which employs LLMs to se-\nquentially map multi-modality instructions to robot actions. The previous method, CaP,\ngenerated robot policy program code directly from in-context examples based on language\ninstructions. However, this approach was constrained by the capabilities of the generated\ncode and encountered difﬁculties with longer, more complex commands due to the required\nhigh precision of code. To overcome these limitations, Instruct2Act introduced a novel\nstrategy that used multi-modality models and LLMs to simultaneously address recognition,\ntask planning, and low-level control modules. Instruct2Act utilized the segment anything\nmodel for identifying potential objects in input images for multi-modality recognition\nand the CLIP model for object classiﬁcation. As a result, Instruct2Act developed an inte-\ngrated search system capable of managing various input modalities and instruction types,\nincluding both pure language instructions and combined language-visual instructions,\nfacilitating the integration of diverse instruction types into a uniﬁed architecture. Moreover,\nfor pointer-language instructions, the framework supported task segmentation based on\nthe user’s clicks.\n4.5. Scene Understanding in LLMs and VLMs\nTo address the VQA problem, robotics research increasingly uses pre-trained VLMs\nto derive high-level information from visual data. This method is advantageous for scene\nunderstanding as it helps determine affordances that describe the relationship between the\ncurrent state and the next action based on images from cameras. Related studies focus on\naspects of scene understanding.Appl. Sci.2024, 14, 8868 25 of 39\nChen [168] explored methods to integrate commonsense into scene understanding\nusing LLMs and introduced three paradigms for classifying room types within indoor\nenvironments based on included objects. The zero-shot approach utilized a pre-trained\nlanguage model to identify the objects in a room and estimate their types. The feed-\nforward classiﬁer approach involved inputting sentences that listed a room’s objects into\nthe language model to generate embedding vectors, which were subsequently input into a\npre-trained shallow multilayer perceptron to predict each room type. Lastly, the classiﬁer\napproach embedded images of rooms alongside textual descriptions to identify the best-\nmatching description, thereby determining the room type. These paradigms demonstrated\nthe capacity to generalize to objects not presented in the training set and to make inferences\nwithin a space larger than that deﬁned by the trained object labels.\nYang [169] introduced the innovative zero-shot, open-vocabulary, LLM-based 3D\nvisual grounding pipeline called LLM-Grounder. This method breaks down complex natu-\nral language queries into semantic components and uses visual grounding tools such as\nOpenScene or LERF to locate objects within 3D scenes. Subsequently, the LLM evaluates\nspatial and commonsense relationships among these objects to achieve the ﬁnal grounding.\nRemarkably, LLM-Grounder operates without labeled training data and has proven its ca-\npacity to adapt to new 3D scenes and diverse text queries, enhancing grounding capabilities\nfor complex language queries and establishing itself as an effective solution.\nChen [170] developed NLMap, an open-vocabulary, queryable scene representation\nsystem. Designed to accumulate and incorporate contextual data within a scene repre-\nsentation for natural language queries, this system allows an LLM planner to visualize"
  },
  {
    "question": "Can robots autonomously solve problems with the help of language models?",
    "chunk": "mand systems. Consequently, robots can respond more adaptably and intelligently in\ninteractions with human users, allowing them to engage in complex problem-solving and\ndecision-making processes beyond simple mechanical tasks.\nAdditionally, LLMs not only enhance a robot’s communication skills to improve HRI\nusability but also boost the robot’s planning abilities. Planning involves setting goals\nand devising a sequence of actions to achieve them, which are essential in determining a\nrobot’s autonomy and efﬁciency. LLMs interpret natural language from users and complex\ncommands, enabling robots to establish and execute suitable plans in various situations.\nMoreover, LLMs adapt ﬂexibly to new situations through a zero-shot approach and utilize\npast data for learning. These capabilities indicate that robots can play a vital role in\nautonomously navigating changing environments and resolving unexpected issues.\nMoreover, VLMs such as CLIP [7], which are trained to solve vision question answering\n(VQA) tasks, have the ability to process visual and linguistic information simultaneously.\nThis ability allows robots to visually perceive their surroundings and integrate this infor-\nmation into linguistic descriptions, enabling more sophisticated situational awareness. For\ninstance, using VLMs, a robot can recognize objects and provide descriptions, as well as\nunderstand and execute user commands based on visual cues. This integrated approach\nsigniﬁcantly enhances a robot’s autonomy and interaction capabilities.\nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which en-\nable low-level actuator control using LLMs and VLMs, Google has introduced AutoRT [10].\nAutoRT is a system where robots interact with real-world objects to collect motion data. ItAppl. Sci.2024, 14, 8868 3 of 39\nbegins by exploring the surrounding space to identify feasible tasks, then uses a VLM to\nunderstand the situation and an LLM to propose possible tasks. By inputting the robot’s\noperational guidelines and safety constraints into the LLM as prompts, AutoRT assesses\nthe validity of the proposed tasks and the necessity for human intervention. Throughout\nthis process, AutoRT safely selects and executes feasible tasks while collecting relevant\ndata.\nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for\nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical causal-\nity in the real world, problem-solving through trial-and-error feedback, and code generation\nabilities. Eureka can autonomously generate reward functions for a variety of tasks and\nrobots without needing speciﬁc templates for each. This allows for the generation of\nhuman-level reward functions for diverse robots and tasks without human input. Further-\nmore, Eureka has demonstrated the ability to solve complex problems that were previously\nunsolved by expert-designed reward functions.\nGiven these research outcomes, integrating language models into robotic intelligence\npresents signiﬁcant potential to enhance robot capabilities and applications dramatically,\nthereby redeﬁning their roles in diverse industries and everyday life. Therefore, this survey\npaper explores recent research trends in LLM- and VLM-based robot intelligence, aiming to\nprovide a comprehensive understanding of future development possibilities by examining\nthe application of language models in various robotic research ﬁelds. It also seeks to\nhighlight research cases, identify current limitations, and suggest future research directions.\nTo chronicle this advancement in robotics research ﬁelds, this review paper presents\nthe following contributions:\n• This paper summarizes and introduces the foundational elements and tuning methods\nof LLM architecture.\n• It explores and arranges prompt techniques to enhance the problem-solving abilities\nof LLMs."
  },
  {
    "question": "What is the significance of a zero-shot approach in robot learning?",
    "chunk": "including 6-DoF end-eﬀector waypoints, for various manipulation tasks using an open set \nof instructions and objects. Huang note d that LLMs were skilled at deriving a ﬀordances \nand constraints from free-form language in structions. Further, by harnessing code \ngeneration capabilities, Huang developed 3D value maps for the agent’s observation \nspace through interactions with VLMs. Thes e 3D value maps were integrated into a \nmodel-based planning framework to generate closed-loop robot trajectories robust to \ndynamic perturbations in a zero-shot approach. The proposed framework demonstrated \neﬃcient learning of the dynamics model for sc enes with contact-rich interactions and \nprovided advantages in these complex scenarios. \n \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM to \ngenerate 3D a ﬀordance and constraint maps and design robot trajectories without additional \ntraining [165]. \nAhn [166] introduced a framework named SayCan, which integrates LLMs with \nreinforcement learning value functions, enabling robots to follow high-level text \ninstructions. SayCan comprises two primary components: Say, which uses an LLM for \ntask-based decision-making, and Can, which evaluates the feasibility of these decisions \nvia reinforcement learning. Say leverages task-based knowledge from the LLM and \nreinforcement learning functionality to assess the feasibility of task execution by robots in \nreal-world scenarios. The LLM determines th e actions necessary to achieve high-level \ngoals and evaluates the eﬀectiveness of each action in fulﬁlling the instructions. Learned \nthrough reinforcement learning, the a ﬀordance function estimate s each action’s success \nprobability in the current state, con ﬁrming the executability of actions proposed by the \nLLM. This process allows the LLM to assess the robot’s current state and capabilities, \nultimately generating an interpretable action plan. SayCan was evaluated across 101 robot \ntasks, achieving an 84% plan success rate and a 74% execution success rate in a simulated \nkitchen environment. In a real kitchen setting, the plan success rate decreased slightly to \n81% and the execution success rate fell to 60%, demonstrating that the policy and value \nfunctions generalize well to real-world settings. \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM\nto generate 3D affordance and constraint maps and design robot trajectories without additional\ntraining [165].\nAhn [166] introduced a framework named SayCan, which integrates LLMs with rein-\nforcement learning value functions, enabling robots to follow high-level text instructions.\nSayCan comprises two primary components: Say, which uses an LLM for task-based\ndecision-making, and Can, which evaluates the feasibility of these decisions via reinforce-\nment learning. Say leverages task-based knowledge from the LLM and reinforcement\nlearning functionality to assess the feasibility of task execution by robots in real-world\nscenarios. The LLM determines the actions necessary to achieve high-level goals and\nevaluates the effectiveness of each action in fulﬁlling the instructions. Learned through re-\ninforcement learning, the affordance function estimates each action’s success probability in\nthe current state, conﬁrming the executability of actions proposed by the LLM. This process\nallows the LLM to assess the robot’s current state and capabilities, ultimately generating\nan interpretable action plan. SayCan was evaluated across 101 robot tasks, achieving an\n84% plan success rate and a 74% execution success rate in a simulated kitchen environment."
  },
  {
    "question": "What advancements have been made in the usability of robotic systems with LLMs?",
    "chunk": "prompts have the potential to cause the entire robotic system to malfunction. To defend\nagainst this critical threat to the reliability and safety of robotic systems, various techniques\nhave been proposed, such as input validation, which ﬁlters the model’s input, and context\nlocking, which restricts access based on the history and content of the prompt. Furthermore,\nstrict guardrails that restrict harmful or unsafe outputs from models can be an alternative\nto improve the reliability of robotic systems. However, it is essential to recognize that the\nsecurity techniques may potentially lead to a decline in the performance of the robot system.\nConsequently, the trade-off between performance and safety must be carefully considered.\nSince the emergence of ChatGPT and Microsoft’s implementation of robot systems\nusing ChatGPT [2], artiﬁcial intelligence components have been applied more widely and\nintensively in robotics research. Despite existing challenges, it is expected that research\ninvolving foundation models to improve robot intelligence will persist across various\ndomains and methods, which will likely enhance the usability and market potential of\nrobot systems inﬂuenced by these advancements.\n6. Conclusions\nIn this paper, we have explored the potential impact and applicability of LLMs on\nrobotics research ﬁelds by summarizing studies that applied LLMs and VLMs to robots.\nFundamentally, LLMs can enhance the capability of robots in natural language processing\nto interact with humans and to improve the robots’ autonomy in various task scenarios.\nIn particular, the ability of LLMs to understand and generate natural language plays a\ncrucial role in enabling robots to comprehend and execute complex commands. This survey\nconﬁrmed that the scope of utilizing LLMs in robotics was not limited to simple natural\nlanguage processing but also extended to broader research areas. This study explored\nextensive LLM applications in the robotics literature, such as planning, manipulation, and\nscene understanding, as well as reinforcement learning automation frameworks such as\nEureka, and included robot actions in language models such as AutoRT. Moreover, the\nresearch direction of current generative AI models is transitioning towards multimodalAppl. Sci.2024, 14, 8868 30 of 39\nlanguage models, moving beyond information acquisition and cognition aspects such as\ntext, images, and videos to include actuator actions within large models in therobotics ﬁeld.\nWhile the surveyed studies indicated that LLMs play a promising role in the future\nof robotics, certain limitations were also identiﬁed. First, the increased computational\nresources and energy consumption associated with embedding LLMs into robotic systems\nmust be addressed. Second, biases in language models and ethical considerations are\nsigniﬁcant issues that need to be tackled in robotics. Therefore, continual efforts will be\nnecessary in future research to resolve these challenges.\nOverall, LLMs are valuable tools that can signiﬁcantly advance robotics. This review\nhas revealed that innovative robot applications are possible through the integration of\nLLMs and VLMs. Moreover, these foundation models are expected to serve as critical\nelements for future robot research and practical applications in the real world.\nAuthor Contributions:Conceptualization, S.S. and C.K.; methodology, S.S.; formal analysis, H.J.,\nH.L. and S.S.; investigation, H.J., H.L. and S.S.; resources, H.J., H.L., S.S. and C.K.; writing—original\ndraft preparation, H.J., H.L., S.S. and C.K.; writing—review and editing, H.J., H.L., S.S. and C.K.; vi-"
  },
  {
    "question": "Can you explain how reward design works in reinforcement learning for robots?",
    "chunk": "more, Eureka has demonstrated the ability to solve complex problems that were previ-\nously unsolved by expert-designed reward functions. \nGiven these research outcomes, integrating language models into robotic intelligence \npresents signi ﬁcant potential to enhance robot capabilities and applications dramatically, \nthereby rede ﬁning their roles in diverse industries and everyday life. Therefore, this sur-\nvey paper explores recent research trends in LLM- and VLM-based robot intelligence, \naiming to provide a comprehensive understanding of future development possibilities by \nexamining the application of language models in various robotic research ﬁelds. It also \nseeks to highlight research cases, identify cu rrent limitations, and suggest future research \ndirections. \nTo chronicle this advancement in robotics research ﬁelds, this review paper presents \nthe following contributions: \n• This paper summarizes and introduces the foundational elements and tuning meth-\nods of LLM architecture. \n• It explores and arranges prompt techniques  to enhance the problem-solving abilities \nof LLMs. \n• It  reviews and encapsulates how LLMs and VLMs have been employed to augment \nrobot intelligence across ﬁve topics as shown in Figure 1: (1) reward design for rein-\nforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation, \nand (5) scene understanding. \n \nFigure 1. Five categories for robot intelligence with large language models in this study. \nFigure 1.Five categories for robot intelligence with large language models in this study.Appl. Sci.2024, 14, 8868 4 of 39\nThe reward design in RLcategory represents a research ﬁeld in which an LLM devel-\nops and enhances reward functions employed in reinforcement learning via code-based\ndescriptions and natural language input. This enables robots to learn optimal policies\nfor speciﬁc tasks through reinforcement learning, even in complex environments. The\nlow-level controlcategory includes a research area in which LLMs and VLMs generate\ncommand sequences that directly control the robot’s actuators through natural language\nand visual input. Thehigh-level planningcategory is a research area where the LLM\nidentiﬁes the present circumstances and objective of the tasks, subsequently developing an\nexplainable plan based on the reasoning required for problem-solving. In this research area,\nthe LLM is also tasked with developing the optimal robot behavior plan, which entails\nevaluating the feasibility of the established plan. In themanipulation category, the LLM in-\nterprets high-level instructions and the VLM (and LLM) analyzes various conditions based\non their understanding of the surroundings to assist robot arms in performing the speciﬁc\ntasks. While this category can be broadly included in the high-level planning category,\nthere are numerous studies that are speciﬁcally related to manipulation with a robot arm,\nwhich is why the manipulation category was separated. Thescene understandingcategory\nrepresents a research area that seeks to combine LLMs and VLMs with the objective of\nassisting robots in comprehending their surrounding environment. This is accomplished\nby identifying objects based on natural language instructions and visual information, as\nwell as by evaluating the relationships between them. This research area is also closely\nrelated to the ﬁeld of autonomous visual navigation. From a boarder perspective, there is an\noverlap between the scene understanding category and the perception-related components\nof the high-level planning category. However, in this review, the scene understanding\ncategory was considered a distinct category due to its prevalence as an application of\nVLM models.\nTable 1 lists resources that aid in understanding robot intelligence based on language"
  },
  {
    "question": "How have AI models progressed from 2019 to now?",
    "chunk": "AI [45]\n2019-02 GPT-2 OpenAI [ 35] 2024-02 StarCoder2 Hugging Face [ 46]\n2019-10 T5 Google [ 47] 2024-03 Claude 3 Anthropic [ 48]\n2020-05 GPT-3 OpenAI [ 49] 2024-03 InternLM2 Shanghai AI Lab [ 50]\n2021-07 Codex OpenAI [ 51] 2024-03 Jamba AI21Labs [ 52]\n2021-09 FLAN Google [ 53] 2024-04 Stabe Code Stability AI [ 54]\n2021-10 T0 Hugging Face [ 37] 2024-04 HyperCLOVA Naver [ 55]\n2021-12 Gopher DeepMind [ 56] 2024-04 Grok-1.5 xAI [ 57]\n2022-03 InstructGPT OpenAI [ 58] 2024-04 Llama3 Meta AI Research [ 59]\n2022-04 PaLM Google [ 60] 2024-04 Phi-3 Microsoft [ 61]\n2022-05 OPT Meta AI Research [ 62] 2024-05 GPT-4o OpenAI [ 1]\n2023-02 LLaMA Meta AI Research [ 63] 2024-06 Claude 3.5 Anthropic [ 64]\n2023-03 Alpaca Stanford Univ. [ 65] 2024-07 GPT-4o mini OpenAI [ 66]\n2023-03 GPT-4 OpenAI [ 50] 2024-07 Falcon2-11B TII [ 67]\n2023-05 StarCoder Hugging Face [ 68] 2024-07 Llama 3.1 405B Meta AI Research [ 69]\n2023-07 LLaMA2 Meta AI Research [ 70] 2024-07 Large2 Mistral AI [ 71]\n2023-09 Baichuan2 Baidu [ 72] 2024-07 Gemma2 Gemma Team,\nGoogle DeepMind [73]\n2023-10 Mistrial Mistral AI [ 74] 2024-08 EXAONE 3 LG AI Research [ 75]\n2024-01 DeepSeek-\nCoder DeepSeek-AI [ 76] 2024-08 Grok-2 and\nGrok-2 mini xAI [ 77]\nTable 3.Chronicle of VLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref.\n2020-05 DETR Facebook AI [ 78] 2023-04 LLaVA UW–Madison [ 79]\n2020-12 DeiT Facebook AI [ 80] 2023-04 MiniGPT-4 KAUST [ 81]\n2021-02 DALL-E OpenAI [ 82] 2023-09 GPT-4V OpenAI [ 83]\n2021-02 CLIP OpenAI [ 7] 2023-11 Florence-2 Microsoft [ 84]\n2021-03 Swin\nTransformer Microsoft [ 85] 2024-01 Lumiere Google Research [ 86]\n2021-05 SegFormer Univ. of Hong\nKong [87] 2024-01 Fuyu Adept [ 88]\n2021-06 Vision\nTransformer\nGoogle Research,\nBrain [89] 2024-03 Gemini 1.5 Gemini Team,\nGoogle [90]\n2021-06 BEiT HIT, Microsoft"
  },
  {
    "question": "In what ways do robots retry tasks after facing failure?",
    "chunk": "paper details experiments in a simulated tabletop rearrangement, a mini-grid 2D maze,\nand real-world kitchen mobile manipulation settings to evaluate long-horizon reasoning\nperformance. Comparative experiments with SayCan revealed that while SayCan limits\nthe range of robot actions, GD can represent a wider array of actions. In contrast to\nCLIPort, which executes high-level language instructions directly, GD achieves enhanced\nperformance through detailed, step-by-step planning.\nHuang [153], as shown in Figure8, proposed the inner monologue method, which\nallowed LLMs to plan and adjust based on feedback from the environment. This approach\nenabled robots to formulate plans in dynamic environments, retry upon facing failure,\nor seek human feedback to reﬁne their strategies. The author clariﬁed that this method\nemerged from integrating the LLM’s high-level planning capabilities with perceptual feed-\nback and low-level control, thereby facilitating more adaptable and intelligent interactions.\nInner monologue integrated various feedback sources into the language model to assist the\nrobot in executing given instructions, including text-based indicators of the robot’s action\nsuccess or failure, object recognition and descriptions within the scene, the robot’s ability to\nask questions to gather additional information, breaking down instructions into multiple\nsteps to establish an execution plan, and enabling the robot to interact with humans to\nexecute and reﬁne the instructions. The inner monologue method was evaluated in both\nsimulated and real-world environments, such as tabletop rearrangement tasks and manip-\nulation tasks in a real kitchen. The results showed that inner monologue was an effective\nframework, enabling robots to act intelligently in complex interactive settings by effectively\nintegrating environmental feedback to plan and execute tasks.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 20 of 39 \n \n \nFigure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable"
  },
  {
    "question": "How does the computational efficiency of SLMs benefit robotics?",
    "chunk": "• For scene understanding, estimate and identify objects\nand evaluate relationships between objects.\n• For navigation, convert natural language instructions\nand combine them with vision data to identify the\nimage through probability distributions.\n[168–175,216–223]\n5. Discussion and Future Directions\nThe review revealed two potentials of foundation models: (1) commonsense reasoning\nfor planning and (2) the ability to generate code.\nThe ﬁrst ﬁnding from this review study is the potential to enhance robot intelligence\nthrough foundation models. Beyond the studies mentioned here, numerous recent stud-\nies have shown that pre-trained models such as LLMs and VLMs can enhance various\naspects of robot intelligence, such as situational awareness, high-level task planning, and\nhuman interaction. LLMs allow communication with humans in natural languages, objectAppl. Sci.2024, 14, 8868 28 of 39\nutilization based on extensive information, and high-level planning using that information.\nVLMs can describe tasks in text and understand visual information. Furthermore, the\ninformation from VLMs can be supplemented by connecting to knowledge databases via\nLLMs. These capabilities are crucial for enhancing robot intelligence, broadening the scope\nof robot applications, and maximizing robot utility.\nThe second ﬁnding is the code generation capability of LLMs, which has the potential\nto automate the robot development process traditionally performed by humans. Addi-\ntionally, robots that can autonomously update their own algorithms are no longer just\nscience ﬁction. Although limitations exist for robots to self-update, frameworks such as\nEureka and DrEureka, which automatically enhanced reinforcement learning performance\nfor robot motion control, demonstrate the potential for future advancements. This suggests\nthat LLMs may not only enhance human interactions but could also pave the way for\nself-improvement without human intervention.\nWhile foundation models offer considerable potential for advancing robotics intelli-\ngence, several limitations and future considerations remain. These include (1) the speed\nof inference required for real-time applications, (2) the computational efﬁciency necessary\nfor embedded systems, (3) the ability to handle multi-modality information, and (4) the\nnecessity of addressing safety and ethical considerations.\nFirst of all, LLMs and VLMs hold considerable potential for enhancing robot intelli-\ngence. Nonetheless, several critical issues remain to be addressed. Foundation models,\ncharacterized as large-scale models pre-trained on extensive datasets, face challenges re-\nlated to real-time requirements and limited computational resources in robotic applications.\nMoreover, concerns such as personal information protection, privacy, and security from\nexternal attacks need resolution to enable cloud-based LLMs for robotics.\nSecondly, to enhance the computational efﬁciency and usability of LMs, there is\nongoing research into small language models (SLMs). Despite having fewer parameters,\nSLMs can achieve performance comparable to LLMs in speciﬁc applications. Several\nSLMs have been introduced, including DistilBERT [224], which is a compact version of\nGoogle’s BERT; Phi-3 [61], another SLM; Florence-2 [84], a small VLM model from Microsoft;\nMobileBERT [225], which is optimized for mobile platforms; and compact open-source\nversions of OpenAI’s GPT models such as GPT-Neo [226] and GPT-J [227]. Generally, SLMs\nare streamlined models with fewer parameters compared to LLMs, which can number in the\nbillions. SLMs utilize smaller, domain-speciﬁc datasets and require shorter training periods,\ntypically just a few weeks, unlike LLMs, which demand vast datasets for broad learning"
  },
  {
    "question": "What are the implications of using LLMs for enhancing exploration in reinforcement learning?",
    "chunk": "main randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 5.DrEureka leverages LLM to design reward functions and solves the sim-to-real problem\nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134].\nXie [136] introducedText2Reward, a framework that automatically generated dense\nreward functions for reinforcement learning using LLMs. Provided with a goal expressed\nin natural language, Text2Reward produced executable dense reward functions derived\nfrom a compact representation of the environment. This framework generated free-form\ndense reward codes and delivered performance comparable to or surpassing that of policies\ntrained with expert-designed codes across a variety of tasks, including 17 manipulator-\nrelated tasks and six novel locomotion behaviors. Additionally, Text2Reward incorporated\nuser feedback to iteratively enhance the generated reward functions, thereby increasing the\nsuccess rate of the learned policies.\nDi Palo [137] explored the use of LLMs and VLMs to improve reinforcement learning\nagents’ understanding of human intentions. They developed a framework that utilized\nlanguage as a primary inference tool, investigating how it could address key challengesAppl. Sci.2024, 14, 8868 14 of 39\nin reinforcement learning, such as efﬁcient exploration, data reuse in experience, skill\nscheduling, and observational learning. This framework employed LLMs and VLMs to\naddress these reinforcement learning challenges by (1) efﬁciently exploring environments\nwith sparse rewards, (2) reusing collected data to sequentially bootstrap the learning of\nnew tasks, (3) scheduling learned skills for novel tasks, and (4) acquiring knowledge from\nobserving expert agents.\nDu [138] developed success detectors that identiﬁed whether actions or tasks were\nsuccessfully completed, utilizing the large multimodal language model Flamingo and\nhuman reward annotations. The study on success detection spanned three distinct do-\nmains: (1) interactive language-conditioned agents in simulated households, (2) real-world\nrobotic manipulation tasks (inserting and removing small, medium, and large gears), and\n(3) “in-the-wild” human egocentric videos. These success detectors adapted to new lan-\nguage instructions and visual changes using VLMs such as Flamingo, which were trained\non a broad range of language and visual data. Furthermore, success detection was reframed\nas a VQA problem, enabling the tracking of task progress through multiple frames to ascer-\ntain whether tasks had been successfully completed. The proposed method proved to be\nmore accurate in detecting success compared to custom reward models in the ﬁrst two do-\nmains, even with new language instructions or visual changes. However, success detection\nin unseen real-world videos in the third domain posed a more challenging generalization\ntask, underscoring the need for additional research."
  },
  {
    "question": "How does Instruct2Act facilitate the integration of language and visual instructions?",
    "chunk": "In a real kitchen setting, the plan success rate decreased slightly to 81% and the execution\nsuccess rate fell to 60%, demonstrating that the policy and value functions generalize well\nto real-world settings.\nHuang [167] introduced the Instruct2Act framework, which employs LLMs to se-\nquentially map multi-modality instructions to robot actions. The previous method, CaP,\ngenerated robot policy program code directly from in-context examples based on language\ninstructions. However, this approach was constrained by the capabilities of the generated\ncode and encountered difﬁculties with longer, more complex commands due to the required\nhigh precision of code. To overcome these limitations, Instruct2Act introduced a novel\nstrategy that used multi-modality models and LLMs to simultaneously address recognition,\ntask planning, and low-level control modules. Instruct2Act utilized the segment anything\nmodel for identifying potential objects in input images for multi-modality recognition\nand the CLIP model for object classiﬁcation. As a result, Instruct2Act developed an inte-\ngrated search system capable of managing various input modalities and instruction types,\nincluding both pure language instructions and combined language-visual instructions,\nfacilitating the integration of diverse instruction types into a uniﬁed architecture. Moreover,\nfor pointer-language instructions, the framework supported task segmentation based on\nthe user’s clicks.\n4.5. Scene Understanding in LLMs and VLMs\nTo address the VQA problem, robotics research increasingly uses pre-trained VLMs\nto derive high-level information from visual data. This method is advantageous for scene\nunderstanding as it helps determine affordances that describe the relationship between the\ncurrent state and the next action based on images from cameras. Related studies focus on\naspects of scene understanding.Appl. Sci.2024, 14, 8868 25 of 39\nChen [168] explored methods to integrate commonsense into scene understanding\nusing LLMs and introduced three paradigms for classifying room types within indoor\nenvironments based on included objects. The zero-shot approach utilized a pre-trained\nlanguage model to identify the objects in a room and estimate their types. The feed-\nforward classiﬁer approach involved inputting sentences that listed a room’s objects into\nthe language model to generate embedding vectors, which were subsequently input into a\npre-trained shallow multilayer perceptron to predict each room type. Lastly, the classiﬁer\napproach embedded images of rooms alongside textual descriptions to identify the best-\nmatching description, thereby determining the room type. These paradigms demonstrated\nthe capacity to generalize to objects not presented in the training set and to make inferences\nwithin a space larger than that deﬁned by the trained object labels.\nYang [169] introduced the innovative zero-shot, open-vocabulary, LLM-based 3D\nvisual grounding pipeline called LLM-Grounder. This method breaks down complex natu-\nral language queries into semantic components and uses visual grounding tools such as\nOpenScene or LERF to locate objects within 3D scenes. Subsequently, the LLM evaluates\nspatial and commonsense relationships among these objects to achieve the ﬁnal grounding.\nRemarkably, LLM-Grounder operates without labeled training data and has proven its ca-\npacity to adapt to new 3D scenes and diverse text queries, enhancing grounding capabilities\nfor complex language queries and establishing itself as an effective solution.\nChen [170] developed NLMap, an open-vocabulary, queryable scene representation\nsystem. Designed to accumulate and incorporate contextual data within a scene repre-\nsentation for natural language queries, this system allows an LLM planner to visualize"
  },
  {
    "question": "What future advancements are expected in robotic intelligence?",
    "chunk": "Table 5.Summary of the reviewed papers in this study.\nName Explanation Ref.\nReward Design in RL\n• Eureka automatically generates and improves reward\nfunctions based on the virtual environment source\ncode.\n• DrEureka builds reward-aware physics priors using\nEureka and supports effective operation in the real\nworld through domain randomization.\n• LLMs design and reﬁne reward functions based on\nnatural language input.\n• LLMs and VLMs integrate multimodal data to\ngenerate reward functions.\n[11,134,136–139,176–180]\nLow-level\nControl\n• Generating commands to control actuators capable of\nlow-level control.\n• RT-1 and RT-2 enable robots to perform complex tasks\nbased on language-vision data.\n• AutoRT establishes a system where robots can\nautonomously collect and utilize data.\n[8–10,144–148,181–183]\nHigh-level\nPlanning\n• LLMs provide an effective methodology for tasks\nrelated to high-level planning within robotic systems.\n• By using natural language, LLMs can formulate plans\nto solve tasks that require long-horizon reasoning.\n• LLMs assess the feasibility of actions to determine and\nexecute the optimal robotic behavior.\n• LLMs generate behavior trees to structure complex\nrobotic actions accurately.\n[149–160,184–207]\nManipulation\n• Using LLMs and VLMs to integrate language and\nvision data allows various manipulations.\n• LLMs interpret high-level instructions to generate the\nnecessary robot actions and assess their feasibility.\n• VLMs extract object information from images to assist\nin performing manipulations.\n[161–167,208–215]\nScene\nUnderstanding\n• To solve VQA problems, use VLMs to extract\nhigh-level information from vision data.\n• For scene understanding, estimate and identify objects\nand evaluate relationships between objects.\n• For navigation, convert natural language instructions\nand combine them with vision data to identify the\nimage through probability distributions.\n[168–175,216–223]\n5. Discussion and Future Directions\nThe review revealed two potentials of foundation models: (1) commonsense reasoning\nfor planning and (2) the ability to generate code.\nThe ﬁrst ﬁnding from this review study is the potential to enhance robot intelligence\nthrough foundation models. Beyond the studies mentioned here, numerous recent stud-\nies have shown that pre-trained models such as LLMs and VLMs can enhance various\naspects of robot intelligence, such as situational awareness, high-level task planning, and\nhuman interaction. LLMs allow communication with humans in natural languages, objectAppl. Sci.2024, 14, 8868 28 of 39\nutilization based on extensive information, and high-level planning using that information.\nVLMs can describe tasks in text and understand visual information. Furthermore, the\ninformation from VLMs can be supplemented by connecting to knowledge databases via\nLLMs. These capabilities are crucial for enhancing robot intelligence, broadening the scope\nof robot applications, and maximizing robot utility.\nThe second ﬁnding is the code generation capability of LLMs, which has the potential\nto automate the robot development process traditionally performed by humans. Addi-\ntionally, robots that can autonomously update their own algorithms are no longer just\nscience ﬁction. Although limitations exist for robots to self-update, frameworks such as\nEureka and DrEureka, which automatically enhanced reinforcement learning performance\nfor robot motion control, demonstrate the potential for future advancements. This suggests\nthat LLMs may not only enhance human interactions but could also pave the way for\nself-improvement without human intervention."
  },
  {
    "question": "What are some applications of LLMs in robotic decision-making?",
    "chunk": "supporting the creation of high-level policies for robots and accommodating a variety of\nrobotic tasks. Speciﬁcally, CaP interpreted natural language instructions through descrip-\ntions and formulated an action plan for the robot. Moreover, it utilized VLMs such as ViLD\nand MDETR to identify objects and ascertain their locations. Based on this information,\nthe framework controlled the robot’s movements to carry out speciﬁed tasks. The paper\ndemonstrated the CaP framework across diverse domains, including whiteboard drawing,\ntabletop manipulation, and mobile robot navigation and manipulation. Experimental\nresults showed that CaP achieved similar or better success rates than existing systems\nsuch as CLIPort, displaying notably strong generalization capabilities for new tasks. These\nﬁndings underscored the ﬂexibility and efﬁcacy of the CaP framework, establishing its\neffectiveness across various robotic systems.\nMirchandani [148], shown in Figure6, suggested that pre-trained LLMs could autore-\ngressively complete complex token sequences and function as general sequence modelers\nthrough in-context learning without needing additional training. Expanding on this con-\ncept, the study evaluated LLMs’ ability to operate as pattern machines in three domains:\nsequence transformation, sequence completion, and sequence improvement. In sequence\ntransformation, the research demonstrated that LLMs could generalize speciﬁc sequence\ntransformations using benchmarks such as ARC (abstraction and reasoning corpus) and\nPCFG (probabilistic context-free grammar), thereby proving their utility in spatial reasoning\ntasks for robotics. In sequence completion, the study examined whether LLMs could ﬁnish\npatterns in elementary functions (e.g., sinusoids), illustrating their utility in robotic tasks\nsuch as extending a wiping motion from kinesthetic demonstrations or creating drawings\non a whiteboard. Finally, in sequence improvement, the research revealed that by utilizing\nreward-labeled trajectories as context and incorporating online interaction, LLM-based\nagents could explore small grids and reﬁne simple trajectories using human-in-the-loop\nmethods, such as optimizing a CartPole controller.Appl. Sci.2024, 14, 8868 17 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 17 of 39 \n \nsequence transformation, the research demonstrated that LLMs could generalize speci ﬁc \nsequence transformations using benchmarks such as ARC (abstraction and reasoning \ncorpus) and PCFG (probabilistic context-free grammar), thereby proving their utility in \nspatial reasoning tasks for robotics. In sequence completion, the study examined whether \nLLMs could ﬁnish pa tterns in elementary functions (e.g., sinusoids), illustrating their \nutility in robotic tasks such as extending a wiping motion from kinesthetic demonstrations \nor creating drawings on a whiteboard. Finally, in sequence improvement, the research \nrevealed that by utilizing reward-labeled trajectories as context and incorporating online \ninteraction, LLM-based agents could explore small grids and re ﬁne simple trajectories \nusing human-in-the-loop methods, such as optimizing a CartPole controller.  \n \nFigure 6. Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed \nin sequence transformation, completion, and improvement [148]. \n4.3. High-Level Planning (Including Decision-Making and Reasoning) \nThe abstraction and generalization capabilities of LLMs oﬀer eﬀective methodologies \nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various \nresearch outcomes have been realized in the ﬁelds of planning, decision-making,"
  },
  {
    "question": "What types of tasks are typically used for instruction tuning?",
    "chunk": "to attend only to past and present tokens, processing input and output tokens similarly\nthrough the decoder. This method underpins the development of the GPT series. Lastly, the\npreﬁx decoder, resembling the causal decoder’s masking mechanism, allows bidirectional\nattention on preﬁx tokens [107] and unidirectional attention on generated tokens. Similar\nto the encoder–decoder, the preﬁx decoder bidirectionally encodes the preﬁx sequence and\nsequentially predicts output tokens individually. Examples of preﬁx decoder-based LLMs\ninclude GLM-130B [108] and U-PaLM [109]. Additionally, various architectures have been\nproposed to address efﬁciency challenges during training or inference with long inputs,\ndue to the quadratic computational complexity of the traditional transformer architecture.\nFor instance, the Mixture-of-Experts (MoE) scaling method [34] sparsely activates a subset\nof the neural network for each input.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 8 of 39 \n \n[47] and BART [33], with Flan-T5 [106] be ing an encoder–decoder-based LLM. Secondly, \nthe causal decoder employs a unidirectional attention mask to restrict each input token to \nattend only to past and present tokens, proc essing input and output tokens similarly \nthrough the decoder. This method underpins the development of the GPT series. Lastly, \nthe preﬁx decoder, resembling the causal decoder’s masking mechanism, allows bidirec-\ntional attention on preﬁx tokens [107] and unidirectional attention on generated tokens. \nSimilar to the encoder–decoder, the pre ﬁx decoder bidirectionally encodes the pre ﬁx se-\nquence and sequentially predicts output tokens individually. Examples of preﬁx decoder-\nbased LLMs include GLM-130B [108] and U-PaLM [109]. Additionally, various architec-\ntures have been proposed to address e ﬃciency challenges during training or inference \nwith long inputs, due to the quadratic computational complexity of the traditional trans-\nformer architecture. For instance, the Mixture-of-Experts (MoE) scaling method [34] \nsparsely activates a subset of the neural network for each input. \n \nFigure 2. Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx De-\ncoder (middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectan-\ngles represent attention between preﬁx tokens, attention between preﬁx and target tokens, atten-\ntion between target tokens, and masked attention [5]. \nIn terms of the tuning of LLMs, these models are essentially pre-trained on massive \ndatasets and require ﬁne-tuning for diﬀerent application domains. However, the consid-\nerable model size and number of parameters pose challenges for ﬁne-tuning on standard \ncomputers and GPUs. The subsequent sections will discuss methods to address these chal-\nlenges. \nLLM tuning is broadly divided into two cate gories based on the training objective. \nInstruction tuning is a form of supervised learning where the training data typically in-\nclude descriptions of tasks, inputs, and corresponding outputs. This type of tuning is de-\nsigned (1) to enhance the functional capabilities of LLMs, (2) to specialize them by training \nwith discipline-speci ﬁc information, and (3) to improve task generalization and con-"
  },
  {
    "question": "How does the monitoring framework detect anomalies in robot perception?",
    "chunk": "Chen [168] explored methods to integrate commonsense into scene understanding\nusing LLMs and introduced three paradigms for classifying room types within indoor\nenvironments based on included objects. The zero-shot approach utilized a pre-trained\nlanguage model to identify the objects in a room and estimate their types. The feed-\nforward classiﬁer approach involved inputting sentences that listed a room’s objects into\nthe language model to generate embedding vectors, which were subsequently input into a\npre-trained shallow multilayer perceptron to predict each room type. Lastly, the classiﬁer\napproach embedded images of rooms alongside textual descriptions to identify the best-\nmatching description, thereby determining the room type. These paradigms demonstrated\nthe capacity to generalize to objects not presented in the training set and to make inferences\nwithin a space larger than that deﬁned by the trained object labels.\nYang [169] introduced the innovative zero-shot, open-vocabulary, LLM-based 3D\nvisual grounding pipeline called LLM-Grounder. This method breaks down complex natu-\nral language queries into semantic components and uses visual grounding tools such as\nOpenScene or LERF to locate objects within 3D scenes. Subsequently, the LLM evaluates\nspatial and commonsense relationships among these objects to achieve the ﬁnal grounding.\nRemarkably, LLM-Grounder operates without labeled training data and has proven its ca-\npacity to adapt to new 3D scenes and diverse text queries, enhancing grounding capabilities\nfor complex language queries and establishing itself as an effective solution.\nChen [170] developed NLMap, an open-vocabulary, queryable scene representation\nsystem. Designed to accumulate and incorporate contextual data within a scene repre-\nsentation for natural language queries, this system allows an LLM planner to visualize\nand query objects, thereby generating contextual plans. Initially, a VLM sets up a scene\nrepresentation for natural language queries; then, an LLM-based object suggestion module\nreviews instructions, suggests relevant objects, and queries the scene for object availability\nand location. Using this information, the LLM planner devises plans uniquely tailored to\nthe scene’s context. NLMap equips robots with the ability to function without a predeﬁned\ncatalog of objects or actions, overcoming the constraints of earlier methods and enabling\nmore adaptable operations in environments with novel or absent objects.\nElhafsi [171] introduced a monitoring framework that employed an LLM with superior\ncontextual understanding and reasoning capabilities to detect edge cases and anomalies\nwithin vision-based policies. This framework monitored the robot’s perception stream\nthrough an LLM-based module, designed to detect semantic anomalies that might occur\nduring operations. By converting the robot’s visual observations into textual descriptions\nat regular intervals and integrating these into LLM prompts, it could pinpoint factors\nleading to policy errors, unsafe behavior, or task confusion. The conversion of visual\ninformation into natural language descriptions used various techniques, without restriction\nto any speciﬁc method. This ﬂexibility enabled both fully end-to-end policies and classical\nautonomy stacks using learned perception to align more closely with human intuition.\nThe ﬁndings indicated that semantic anomalies did not always correspond to semantically\nexplainable failures, and end-to-end policies could sometimes behave unpredictably.\nHon [172] introduced a new model family named 3D-LLM, which incorporated 3D\nworld information into LLMs. The 3D-LLM model utilized 3D point clouds and their\nfeatures as input, enabling it to handle a variety of spatially aware 3D tasks. These tasks\nincluded 3D captioning, dense captioning, 3D question answering, task decomposition, 3D"
  },
  {
    "question": "How can models combine different types of information for better robot performance?",
    "chunk": "While foundation models offer considerable potential for advancing robotics intelli-\ngence, several limitations and future considerations remain. These include (1) the speed\nof inference required for real-time applications, (2) the computational efﬁciency necessary\nfor embedded systems, (3) the ability to handle multi-modality information, and (4) the\nnecessity of addressing safety and ethical considerations.\nFirst of all, LLMs and VLMs hold considerable potential for enhancing robot intelli-\ngence. Nonetheless, several critical issues remain to be addressed. Foundation models,\ncharacterized as large-scale models pre-trained on extensive datasets, face challenges re-\nlated to real-time requirements and limited computational resources in robotic applications.\nMoreover, concerns such as personal information protection, privacy, and security from\nexternal attacks need resolution to enable cloud-based LLMs for robotics.\nSecondly, to enhance the computational efﬁciency and usability of LMs, there is\nongoing research into small language models (SLMs). Despite having fewer parameters,\nSLMs can achieve performance comparable to LLMs in speciﬁc applications. Several\nSLMs have been introduced, including DistilBERT [224], which is a compact version of\nGoogle’s BERT; Phi-3 [61], another SLM; Florence-2 [84], a small VLM model from Microsoft;\nMobileBERT [225], which is optimized for mobile platforms; and compact open-source\nversions of OpenAI’s GPT models such as GPT-Neo [226] and GPT-J [227]. Generally, SLMs\nare streamlined models with fewer parameters compared to LLMs, which can number in the\nbillions. SLMs utilize smaller, domain-speciﬁc datasets and require shorter training periods,\ntypically just a few weeks, unlike LLMs, which demand vast datasets for broad learning\nand multiple months of training. Developing SLMs to excel within speciﬁc domains for\nrobotic systems and ensuring real-time performance with minimal computational resources\nare essential research directions for advancing robot intelligence with SLMs.\nThe third implication is that LLMs, based on text-centered natural language process-\ning, are limited as single-modality models when applied to real-world robotic systems\nwhere information often blends in diverse ways. Research on LLMs is transitioning from\nsingle-modality to multimodality models, as evidenced by VLMs and OpenAI Sora [228],\nwith increasing demand for such models. Currently, to address the limitations of LLMs’\nsingle-modality, robotic systems are being developed with multimodality models that\nintegrate vision, such as VLMs. However, relying solely on text and images falls short\nof the diverse information range required in the real world, including images, sounds,\nvideos, and proprioceptive sensory information (such as the position, orientation, balance,\nmovement degree, and direction of various parts of the robot). Proprioceptive sensory\ninformation related to actions and movements is particularly vital for enhancing dynamic\nhuman interaction, information processing, and manipulation and planning skills based on\ndynamic movements. For instance, the integrated VLA model, which facilitates low-level\ncontrol based on LLMs and VLMs as shown by Google’s RT-2 model, highlights the neces-\nsity for models capable of integrating information from a broader range of modalities to\nenhance robot intelligence.Appl. Sci.2024, 14, 8868 29 of 39\nFinally, the fourth area to consider is how to address safety and ethical issues when\nLLM is applied to robotic intelligence systems. Studies were conducted to address the\nissue of discriminatory and unsafe behaviors that may be generated by robot applications"
  },
  {
    "question": "What are the benefits of using external tools for automatic reasoning?",
    "chunk": "prompt techniques are summarized in Table4.\nTable 4.Prompt Techniques.\nName Explanation Ref.\nZero-Shot Prompting Enabling the model to perform new tasks without any examples [ 53]\nFew-Shot Prompting Providing a few examples to enable performing new tasks [ 49]\nChain-of-Thought Explicitly generating intermediate reasoning steps to perform\nstep-by-step inference [41]\nSelf-Consistency\nGenerating various reasoning paths independently through\nFew-Shot CoT, with each path going through a prompt generation\nprocess to select the most consistent answer\n[120]\nGenerated Knowledge Prompting\nIntegrating knowledge and information relevant to a question, and\nthen providing it along with the question to generate accurate\nanswers\n[126]\nPrompt Chaining Dividing a task into sub-tasks and connecting prompts for each\nsub-task as input–output pairs [125]\nTree of Thoughts\nDividing a problem into subproblems with intermediate steps that\nserve as “thoughts” towards solving the problem, where each\nthought undergoes an inference process and self-evaluates its\nprogress towards solving the problem\n[124]\nRetrieval Augmented Generation Combining external information retrieval with natural language\ngeneration [127]\nAutomatic Reasoning and Tool-use Using external tools to automatically generate intermediate\nreasoning steps [128]\nAutomatic Prompt Engineer Automatically generating and selecting commands [ 129]\nActive Prompt Addressing the issue that the effectiveness may be limited by\nhuman annotations [122]\nDirectional Stimulus Prompting Guiding the model to think and generate responses in a speciﬁc\ndirection [130]\nProgram-Aided Language Models Using models to understand natural language problems and\ngenerate programs as intermediate reasoning steps [123]\nReAct Combining reasoning and actions within a mode [ 131]\nReﬂexion Enhancing language-based agents through language feedback [ 132]\nMultimodal CoT A two-stage framework that integrates text and vision modalities [ 121]Appl. Sci.2024, 14, 8868 12 of 39\n4. Language Models for Robotic Intelligence\n4.1. Reward Design in Reinforcement Learning\nResearch in reinforcement learning, closely associated with the ﬁeld of robotics, has\nactively incorporated studies using LLM models. Speciﬁcally, Nvidia has developed a\nGPU-based multi-environment reinforcement learning platform. Utilizing its Omniverse\n3D virtual environment platform, Nvidia created Isaac Sim, which is dedicated to robot\nsimulation. Isaac Sim published research ﬁndings on Isaac Gym (Preview), which achieved\nsigniﬁcant reductions in reinforcement learning training times through GPU-based multi-\nenvironment approaches. Subsequently, Isaac Gym (Preview)’s features were integrated\ninto Isaac Sim and released as Omni Isaac Gym. Later, Nvidia introduced Orbit [133],\nfacilitating the simulation of PhysX 5.1-based cloth, soft-body, ﬂuid, and rigid-body dy-\nnamics, along with RGBD, LiDAR, and contact sensor simulation. Orbit also incorporates\nvarious robot platforms into the simulation environment. Recently, Orbit was updated\nto Isaac Lab and integrated into Isaac Sim 4.0. Nvidia has continuously advanced dy-\nnamic simulation environment technologies for reinforcement learning using GPU parallel\ncomputation. Leveraging this GPU reinforcement learning, they launched Eureka [11],\nwhich automates the design of reward functions for reinforcement learning using LLMs.\nFollowing this, Nvidia introduced DrEureka [134], an automated platform addressing the\nSim2Real problem [135] in reinforcement learning based on Eureka.\nEureka (Evolution-driven Universal REward Kit for Agent) [11], shown in Figure4,\nautomatically generates reward functions for various tasks using different robots, elimi-"
  },
  {
    "question": "What are robot behavior trees and why are they important?",
    "chunk": "Figure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable\nrobots to carry out instructions: (a) mobile manipulation and (b,c) tabletop manipulation, in both\nsimulated and real-world environments [153].Appl. Sci.2024, 14, 8868 20 of 39\nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn,\na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot\nbehavior trees (BTs) from textual descriptions. The developed model was compact enough\nto operate on a robot’s onboard microcomputer, while adept at constructing complex robot\nbehaviors. It provided structurally and logically correct BTs and demonstrated the ability\nto handle instructions that were not included in the training set.\nSong [155], as shown in Figure9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions\nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via\na low-level planner. It continuously updated environmental information as new objects\nwere detected during action implementation and revisited the LLM to adjust the plan if\nsubgoals failed or were delayed based on updated observations. This iterative process was\nrepeated until the subgoal was achieved, after which the system moved to the next goal.\nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated\ncompetitive performance with signiﬁcantly reduced training data and proved its ability to\ngeneralize in various tasks (e.g., ALFRED) with minimal examples.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39"
  },
  {
    "question": "What makes BERT different from earlier language models?",
    "chunk": "(RNNs) to model the probability of word sequences [24–26]. A key element of this stage\nis the development of word vectors, also known as word embeddings, which form word\nprediction models based on vectors that use a distributed representation of words [24,27].\nWord2vec, a simpliﬁed shallow neural network approach, was introduced to learn these\ndistributed word representations [28,29]. It proved highly effective across various NLP\ntasks by calculating meaningful similarities between word vectors. NLMs progressed\nfrom basic word sequence modeling to sophisticated techniques for representing language\nthrough word2vec.\nFollowing the NLM phase, the ﬁeld advanced to pre-trained language models (PLMs),\nwhich encompass models such as ELMO [30] and BERT [31]. PLMs, utilizing large-scale\ntext data, learn text patterns, structures, and meanings to develop pre-trained context-\nsensitive word representations. They have successfully executed a variety of language\nunderstanding and generation tasks using this acquired knowledge. ELMo [30] introduced\na pre-training method employing bidirectional LSTM (biLSTM) networks for modeling\ndeep contextualized word representations, optimizing performance through speciﬁc ﬁne-\ntuning of the trained biLSTM network for downstream tasks. ELMo is also characterized\nas a bidirectional language model for its dual-directional use of language models.\nAnother PLM model, BERT [31], leverages the transformer architecture [32], exhibiting\nremarkable effectiveness with self-attention mechanisms and parallel processing. BERT,\na pre-trained bidirectional language model, utilizes extensive unlabeled text data. The\nmethod of unsupervised learning-based pre-training in BERT comprises two primary tasks:\nmasked language models and next sentence prediction. PLMs that provide pre-trained\ncontext-aware word representations are profoundly effective in general-purpose semantic\nfeature extraction, facilitating enhancements in NLP task performance. Owing to these\ncharacteristics, numerous subsequent studies employing pre-training and ﬁne-tuning have\nbeen introduced, featuring varied structures [33,34] (e.g., BART [33] and GPT-2 [35]) and\nenhancing pre-training strategies [36–38].\nBased on subsequent studies, it has been found that increasing the model size or data\nsize of PLMs typically enhances the performance of LM models [39]. This has prompted\nresearch into training large-scale PLMs, such as GPT-3 with 175B parameters and PaLM\nwith 540B parameters. The focus of this research, grounded in scaling laws, primarily\ncenters on augmenting model sizes and exploring the capabilities of larger models. These\ncapabilities, known as the emergent abilities of LLMs, have sparked signiﬁcant interest. For\nexample, GPT-3 can address problems it has not been trained on with minimal examples\nthrough in-context learning, a feat GPT-2 ﬁnds challenging. Due to these characteristics,\nthe academic community commonly designates these large PLMs as LLMs [40–43]. Conse-\nquently, research in this area is highly active. Notably, since the introduction of OpenAI’s\nChatGPT, there has been a surge in the number of arXiv papers on LLMs. Following\nMicrosoft’s announcement [2] about integrating ChatGPT into robotics, a variety of studies\nhave explored the application of LLMs across different areas of robotics research. The\navailable LLM models are presented in chronological order in Table2. Additionally, Table3\nincludes the VLM models.Appl. Sci.2024, 14, 8868 7 of 39\nTable 2.Chronicle of LLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref."
  },
  {
    "question": "What findings support the use of LLMs in robotics?",
    "chunk": "Citation: Jeong, H.; Lee, H.; Kim, C.;\nShin, S. A Survey of Robot\nIntelligence with Large Language\nModels. Appl. Sci.2024, 14, 8868.\nhttps://doi.org/10.3390/app14198868\nAcademic Editors: Luis Gracia and J.\nErnesto Solanes\nReceived: 6 September 2024\nRevised: 24 September 2024\nAccepted: 25 September 2024\nPublished: 2 October 2024\nCopyright: © 2024 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\napplied  sciences\nReview\nA Survey of Robot Intelligence with Large Language Models\nHyeongyo Jeong1,† , Haechan Lee1,† , Changwon Kim2,* and Sungtae Shin1,*\n1 Department of Mechanical Engineering, Dong-A University, Busan 49315, Republic of Korea\n2 School of Mechanical Engineering, Pukyong National University, Busan 48513, Republic of Korea\n* Correspondence: ckim@pknu.ac.kr (C.K.); stshin@dau.ac.kr (S.S.)\n† These authors contributed equally to this work.\nAbstract: Since the emergence of ChatGPT, research on large language models (LLMs) has actively\nprogressed across various ﬁelds. LLMs, pre-trained on vast text datasets, have exhibited exceptional\nabilities in understanding natural language and planning tasks. These abilities of LLMs are promis-\ning in robotics. In general, traditional supervised learning-based robot intelligence systems have a\nsigniﬁcant lack of adaptability to dynamically changing environments. However, LLMs help a robot\nintelligence system to improve its generalization ability in dynamic and complex real-world environ-\nments. Indeed, ﬁndings from ongoing robotics studies indicate that LLMs can signiﬁcantly improve\nrobots’ behavior planning and execution capabilities. Additionally, vision-language models (VLMs),\ntrained on extensive visual and linguistic data for the vision question answering (VQA) problem,\nexcel at integrating computer vision with natural language processing. VLMs can comprehend visual\ncontexts and execute actions through natural language. They also provide descriptions of scenes\nin natural language. Several studies have explored the enhancement of robot intelligence using\nmultimodal data, including object recognition and description by VLMs, along with the execution\nof language-driven commands integrated with visual information. This review paper thoroughly\ninvestigates how foundation models such as LLMs and VLMs have been employed to boost robot\nintelligence. For clarity, the research areas are categorized into ﬁve topics: reward design in rein-\nforcement learning, low-level control, high-level planning, manipulation, and scene understanding.\nThis review also summarizes studies that show how foundation models, such as the Eureka model\nfor automating reward function design in reinforcement learning, RT-2 for integrating visual data,\nlanguage, and robot actions in vision-language-action models, and AutoRT for generating feasible\ntasks and executing robot behavior policies via LLMs, have improved robot intelligence.\nKeywords: embodied intelligence; foundation model; large language model (LLM); vision-language\nmodel (VLM); vision-language-action (VLA) model; robotics\n1. Introduction\nTo enhance the intelligence of robots in real-world environments that interact with\nhumans, developing robots capable of perceiving, acting, and interacting like humans is"
  },
  {
    "question": "How do LLMs assist in high-level planning for robots?",
    "chunk": "• It reviews and encapsulates how LLMs and VLMs have been employed to augment\nrobot intelligence across ﬁve topics as shown in Figure1: (1) reward design for\nreinforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation,\nand (5) scene understanding.\nAppl. Sci. 2024 , 14 , x FOR PEER REVIEW 3 of 39 \n \nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which \nenable low-level actuator control using LLM s and VLMs, Google has introduced AutoRT \n[10]. AutoRT is a system where robots interact  with real-world objects to collect motion \ndata. It begins by exploring th e surrounding space to identify feasible tasks, then uses a \nVLM to understand the situation and an LLM to propose possible tasks. By inpu tting the \nrobot’s operational guidelines and safety co nstraints into the LLM as prompts, AutoRT \nassesses the validity of the proposed tasks and the necessity for human intervention. \nThroughout this process, AutoRT safely selects and executes feasible tasks while collecting \nrelevant data. \nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for \nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical cau-\nsality in the real world, problem-solving through trial-and-error feedback, and code gen-\neration abilities. Eureka can autonomously generate reward functions for a variety of tasks \nand robots without needing speci ﬁc templates for each. This allows for the generation of \nhuman-level reward functions for diverse robo ts and tasks without human input. Further-\nmore, Eureka has demonstrated the ability to solve complex problems that were previ-\nously unsolved by expert-designed reward functions. \nGiven these research outcomes, integrating language models into robotic intelligence \npresents signi ﬁcant potential to enhance robot capabilities and applications dramatically, \nthereby rede ﬁning their roles in diverse industries and everyday life. Therefore, this sur-\nvey paper explores recent research trends in LLM- and VLM-based robot intelligence, \naiming to provide a comprehensive understanding of future development possibilities by \nexamining the application of language models in various robotic research ﬁelds. It also \nseeks to highlight research cases, identify cu rrent limitations, and suggest future research \ndirections. \nTo chronicle this advancement in robotics research ﬁelds, this review paper presents \nthe following contributions: \n• This paper summarizes and introduces the foundational elements and tuning meth-\nods of LLM architecture. \n• It explores and arranges prompt techniques  to enhance the problem-solving abilities \nof LLMs. \n• It  reviews and encapsulates how LLMs and VLMs have been employed to augment \nrobot intelligence across ﬁve topics as shown in Figure 1: (1) reward design for rein-\nforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation, \nand (5) scene understanding. \n \nFigure 1. Five categories for robot intelligence with large language models in this study. \nFigure 1.Five categories for robot intelligence with large language models in this study.Appl. Sci.2024, 14, 8868 4 of 39\nThe reward design in RLcategory represents a research ﬁeld in which an LLM devel-\nops and enhances reward functions employed in reinforcement learning via code-based"
  },
  {
    "question": "How can modifying the task library improve ART's performance?",
    "chunk": "into smaller units called thoughts, which it then assesses through a reasoning process to\ngauge its progress toward a solution. The ability of the model to generate and evaluate these\nthoughts is integrated with search algorithms such as breadth-ﬁrst and depth-ﬁrst search,\nfacilitating systematic thought exploration with lookahead and backtracking capabilities.\nIn contrast to the CoT method, which addresses problems sequentially, ToT concurrently\nexamines multiple pathways to ﬁnd a solution.Prompt chaining[125] is a strategy where\nthe model divides a task into sub-tasks, uses the outputs of each sub-task as subsequent\ninputs, and links prompts in input–output pairs. This approach improves the precision and\nconsistency of the outputs at each stage and simpliﬁes the handling of complex tasks by\nsubdividing them into manageable sub-tasks.\nGenerated knowledge prompting[126] is a technique in which the model incorpo-\nrates knowledge and information pertinent to the question and provides it alongside the\nquestion to generate more accurate answers. This method not only enhances the common-\nsense reasoning capabilities but also retains the ﬂexibility of existing models.Retrieval\naugmented generation(RAG) [127] merges external information retrieval with natural lan-\nguage generation. RAG can be ﬁne-tuned for knowledge-intensive downstream tasks and\nenables straightforward modiﬁcations or additions of knowledge within the framework.\nThis facilitates an increase in the model’s factual consistency, enhances the reliability of\ngenerated responses, and helps alleviate issues with hallucination.Automatic reasoning\nand tool-use(ART) [128] is a framework that utilizes external tools to autonomously gen-Appl. Sci.2024, 14, 8868 11 of 39\nerate intermediate reasoning steps. It chooses relevant tasks from a library that includes\ndemonstrations and calls on external tools as necessary to integrate their outputs into the\nreasoning process. The model generalizes from demonstrations using tools to decompose\nnew tasks and learns to use tools effectively. Enhancing ART’s performance is possible\nby modifying the task library or incorporating new tools.Automatic prompt engineer\n(APE) [129] is a framework designed for the automatic generation and selection of com-\nmands. The model generates command candidates for a problem and selects the most\nsuitable one based on a scoring function, such as execution accuracy or log probability.\nDirectional stimulus prompting[130] is a technique that directs the model to consider\nand generate responses in a particular direction. By deploying a tunable policy LM (e.g.,\nT5 [47]), it creates directional stimulus prompts for each input and uses these as cues to\nsteer the model toward producing the desired outcomes [131]. ReAct combines reasoning\nwith action within the model. It enables the model to perform reasoning in generating\nanswers, take actions based on external sources (e.g., documents, articles, and news), and\nreﬁne reasoning based on observations of these actions. This process facilitates the creation,\nmaintenance, and modiﬁcation of action plans while incorporating additional information\nfrom interactions with external sources.Reﬂexion [132] augments language-based agents\nwith language feedback. Reﬂexion involves three models: the actor, the evaluator, and self-\nreﬂection. The actor initiates actions within a speciﬁc environment to generate task steps,\nthe evaluator assesses these steps, and self-reﬂection provides linguistic feedback, which\nthe actor uses to formulate new steps and achieve the task’s objective. The introduced\nprompt techniques are summarized in Table4.\nTable 4.Prompt Techniques.\nName Explanation Ref."
  },
  {
    "question": "How do robots utilize object recognition in their planning processes?",
    "chunk": "paper details experiments in a simulated tabletop rearrangement, a mini-grid 2D maze,\nand real-world kitchen mobile manipulation settings to evaluate long-horizon reasoning\nperformance. Comparative experiments with SayCan revealed that while SayCan limits\nthe range of robot actions, GD can represent a wider array of actions. In contrast to\nCLIPort, which executes high-level language instructions directly, GD achieves enhanced\nperformance through detailed, step-by-step planning.\nHuang [153], as shown in Figure8, proposed the inner monologue method, which\nallowed LLMs to plan and adjust based on feedback from the environment. This approach\nenabled robots to formulate plans in dynamic environments, retry upon facing failure,\nor seek human feedback to reﬁne their strategies. The author clariﬁed that this method\nemerged from integrating the LLM’s high-level planning capabilities with perceptual feed-\nback and low-level control, thereby facilitating more adaptable and intelligent interactions.\nInner monologue integrated various feedback sources into the language model to assist the\nrobot in executing given instructions, including text-based indicators of the robot’s action\nsuccess or failure, object recognition and descriptions within the scene, the robot’s ability to\nask questions to gather additional information, breaking down instructions into multiple\nsteps to establish an execution plan, and enabling the robot to interact with humans to\nexecute and reﬁne the instructions. The inner monologue method was evaluated in both\nsimulated and real-world environments, such as tabletop rearrangement tasks and manip-\nulation tasks in a real kitchen. The results showed that inner monologue was an effective\nframework, enabling robots to act intelligently in complex interactive settings by effectively\nintegrating environmental feedback to plan and execute tasks.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 20 of 39 \n \n \nFigure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable"
  },
  {
    "question": "How does Text2Reward generate reward functions from natural language?",
    "chunk": "main randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 5.DrEureka leverages LLM to design reward functions and solves the sim-to-real problem\nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134].\nXie [136] introducedText2Reward, a framework that automatically generated dense\nreward functions for reinforcement learning using LLMs. Provided with a goal expressed\nin natural language, Text2Reward produced executable dense reward functions derived\nfrom a compact representation of the environment. This framework generated free-form\ndense reward codes and delivered performance comparable to or surpassing that of policies\ntrained with expert-designed codes across a variety of tasks, including 17 manipulator-\nrelated tasks and six novel locomotion behaviors. Additionally, Text2Reward incorporated\nuser feedback to iteratively enhance the generated reward functions, thereby increasing the\nsuccess rate of the learned policies.\nDi Palo [137] explored the use of LLMs and VLMs to improve reinforcement learning\nagents’ understanding of human intentions. They developed a framework that utilized\nlanguage as a primary inference tool, investigating how it could address key challengesAppl. Sci.2024, 14, 8868 14 of 39\nin reinforcement learning, such as efﬁcient exploration, data reuse in experience, skill\nscheduling, and observational learning. This framework employed LLMs and VLMs to\naddress these reinforcement learning challenges by (1) efﬁciently exploring environments\nwith sparse rewards, (2) reusing collected data to sequentially bootstrap the learning of\nnew tasks, (3) scheduling learned skills for novel tasks, and (4) acquiring knowledge from\nobserving expert agents.\nDu [138] developed success detectors that identiﬁed whether actions or tasks were\nsuccessfully completed, utilizing the large multimodal language model Flamingo and\nhuman reward annotations. The study on success detection spanned three distinct do-\nmains: (1) interactive language-conditioned agents in simulated households, (2) real-world\nrobotic manipulation tasks (inserting and removing small, medium, and large gears), and\n(3) “in-the-wild” human egocentric videos. These success detectors adapted to new lan-\nguage instructions and visual changes using VLMs such as Flamingo, which were trained\non a broad range of language and visual data. Furthermore, success detection was reframed\nas a VQA problem, enabling the tracking of task progress through multiple frames to ascer-\ntain whether tasks had been successfully completed. The proposed method proved to be\nmore accurate in detecting success compared to custom reward models in the ﬁrst two do-\nmains, even with new language instructions or visual changes. However, success detection\nin unseen real-world videos in the third domain posed a more challenging generalization\ntask, underscoring the need for additional research."
  },
  {
    "question": "Can you describe the role of success detectors in reinforcement learning?",
    "chunk": "main randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 5.DrEureka leverages LLM to design reward functions and solves the sim-to-real problem\nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134].\nXie [136] introducedText2Reward, a framework that automatically generated dense\nreward functions for reinforcement learning using LLMs. Provided with a goal expressed\nin natural language, Text2Reward produced executable dense reward functions derived\nfrom a compact representation of the environment. This framework generated free-form\ndense reward codes and delivered performance comparable to or surpassing that of policies\ntrained with expert-designed codes across a variety of tasks, including 17 manipulator-\nrelated tasks and six novel locomotion behaviors. Additionally, Text2Reward incorporated\nuser feedback to iteratively enhance the generated reward functions, thereby increasing the\nsuccess rate of the learned policies.\nDi Palo [137] explored the use of LLMs and VLMs to improve reinforcement learning\nagents’ understanding of human intentions. They developed a framework that utilized\nlanguage as a primary inference tool, investigating how it could address key challengesAppl. Sci.2024, 14, 8868 14 of 39\nin reinforcement learning, such as efﬁcient exploration, data reuse in experience, skill\nscheduling, and observational learning. This framework employed LLMs and VLMs to\naddress these reinforcement learning challenges by (1) efﬁciently exploring environments\nwith sparse rewards, (2) reusing collected data to sequentially bootstrap the learning of\nnew tasks, (3) scheduling learned skills for novel tasks, and (4) acquiring knowledge from\nobserving expert agents.\nDu [138] developed success detectors that identiﬁed whether actions or tasks were\nsuccessfully completed, utilizing the large multimodal language model Flamingo and\nhuman reward annotations. The study on success detection spanned three distinct do-\nmains: (1) interactive language-conditioned agents in simulated households, (2) real-world\nrobotic manipulation tasks (inserting and removing small, medium, and large gears), and\n(3) “in-the-wild” human egocentric videos. These success detectors adapted to new lan-\nguage instructions and visual changes using VLMs such as Flamingo, which were trained\non a broad range of language and visual data. Furthermore, success detection was reframed\nas a VQA problem, enabling the tracking of task progress through multiple frames to ascer-\ntain whether tasks had been successfully completed. The proposed method proved to be\nmore accurate in detecting success compared to custom reward models in the ﬁrst two do-\nmains, even with new language instructions or visual changes. However, success detection\nin unseen real-world videos in the third domain posed a more challenging generalization\ntask, underscoring the need for additional research."
  },
  {
    "question": "What was the success rate of robots using foot contact patterns in their tasks?",
    "chunk": "aim of this diversity evaluation is to conﬁrm that, unlike simulations, real-world data\ncollection by robots is labor-intensive, making it essential to gather data across a broad\nspectrum of tasks. Experimental outcomes illustrate that AutoRT achieves higher visual\nand linguistic diversity compared to RT-1 or BC-Z [143].\nOther researchers include Tang [144], who developed an approach that connects\nnatural language user commands with a locomotion controller using foot contact patterns as\nan interface for low-level commands. This innovative interface translates human commands\ninto the robot’s foot contact patterns, allowing the robot to move at a speciﬁed speed with\nprecise timing for each foot’s contact with the ground. To achieve this, the robot used a\ncyclic sliding window to extract foot contact ﬂags from a pattern template, thus generating\nthe required foot contact patterns. During training, a random pattern generator created\nfoot contact patterns, and during testing, an LLM translated human commands into these\npatterns. The robot then adjusted its movements based on the foot contact patterns it\nlearned through deep reinforcement learning, closely adhering to the intended foot contact\npatterns and speed commands. This approach demonstrated a 50% higher success rate in\ntask evaluation (across 30 tasks, including standing still) compared to two baselines (which\nemployed discrete gaits and sinusoidal functions as interfaces), successfully solving 10\nmore tasks than the baselines.\nMandi [145] introduced a novel method for multi-robot collaboration that utilizes\nLLMs for both high-level communication and low-level path planning. In this method, the\nrobots employ the LLM to discuss and reason about task strategies. They generate sub-taskAppl. Sci.2024, 14, 8868 16 of 39\nplans and task space waypoint paths, which a multi-arm motion planner then uses to expe-\ndite trajectory planning. Additionally, environmental feedback, such as collision detection,\nprompts the LLM agent to reﬁne plans and waypoints contextually. This method achieved\na high success rate across all tasks in the RoCoBench (including duties such as sweeping\nthe ﬂoor), effectively adapting to variations in task semantics. In real-world experiments,\nspeciﬁcally the block-sorting task, RoCo demonstrated its ability to communicate and\ncollaborate with other robot agents to successfully complete the tasks.\nWang [146] proposed a novel paradigm for utilizing few-shot prompts in physical\nenvironments. This method involved gathering observation and action pairs from existing\nmodel-based or learning-based controllers to form the initial text prompts. Data included\nsensor readings, such as IMU and joint encoders, coupled with target joint positions.\nThese data formed the starting input for LLM inference. As the robot interacted with its\nenvironment and collected new observational data, these initial data were updated with\noutputs from the LLM. In the subsequent prompt engineering phase, observation and action\npairs, along with explanatory prompts, were crafted to enable the LLM to function as a\nfeedback policy. The explanatory prompts provided clear descriptions of the robot walking\ntask and control design details, while the observation and action prompts delineated the\nformat and signiﬁcance of each observation and action. This method allowed the LLM\nto directly output low-level target joint positions for robot walking. The approach was\ntested using the ANYmal robot in MuJoCo and Isaac Gym simulators for robot walking,\nindicating that the LLM could act as a low-level feedback controller for dynamic motion\ncontrol within sophisticated robot systems.\nLiang [147] introduced a new framework namedCode as Policies(CaP) that directly\nconstructs robot policies from executable code generated by a code LLM. This framework"
  },
  {
    "question": "How effective is the inner monologue method in real-world tasks?",
    "chunk": "paper details experiments in a simulated tabletop rearrangement, a mini-grid 2D maze,\nand real-world kitchen mobile manipulation settings to evaluate long-horizon reasoning\nperformance. Comparative experiments with SayCan revealed that while SayCan limits\nthe range of robot actions, GD can represent a wider array of actions. In contrast to\nCLIPort, which executes high-level language instructions directly, GD achieves enhanced\nperformance through detailed, step-by-step planning.\nHuang [153], as shown in Figure8, proposed the inner monologue method, which\nallowed LLMs to plan and adjust based on feedback from the environment. This approach\nenabled robots to formulate plans in dynamic environments, retry upon facing failure,\nor seek human feedback to reﬁne their strategies. The author clariﬁed that this method\nemerged from integrating the LLM’s high-level planning capabilities with perceptual feed-\nback and low-level control, thereby facilitating more adaptable and intelligent interactions.\nInner monologue integrated various feedback sources into the language model to assist the\nrobot in executing given instructions, including text-based indicators of the robot’s action\nsuccess or failure, object recognition and descriptions within the scene, the robot’s ability to\nask questions to gather additional information, breaking down instructions into multiple\nsteps to establish an execution plan, and enabling the robot to interact with humans to\nexecute and reﬁne the instructions. The inner monologue method was evaluated in both\nsimulated and real-world environments, such as tabletop rearrangement tasks and manip-\nulation tasks in a real kitchen. The results showed that inner monologue was an effective\nframework, enabling robots to act intelligently in complex interactive settings by effectively\nintegrating environmental feedback to plan and execute tasks.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 20 of 39 \n \n \nFigure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable"
  },
  {
    "question": "How do VLMs enable robots to process visual and linguistic information?",
    "chunk": "of labeled data. This process is inherently resource-intensive. Moreover, these models\nare designed for a speciﬁc environment and require reconﬁguration whenever the task or\nenvironment changes. This renders the robots challenging to adapt and scale to disparate\nenvironments [3]. For practical robot systems, it is essential that they are able to ﬂexibly\nrespond to the ever-changing physical environment. From this perspective, the gener-\nalization of affordable tasks, environmental adaptability, and the accuracy of execution,\nplanning, and reasoning capabilities remain signiﬁcant challenges for traditional robotic\nintelligence systems [4].\nHowever, LLMs and VLMs help a robot intelligence system to enhance its general-\nization capability in dynamic and complex real-world environments. LLMs can leverage\npre-trained knowledge from extensive datasets to augment their ability to generalize to\neveryday tasks that are typically expected of robots. Unlike the conventional supervised\nmodels, LLMs can utilize zero-shot and few-shot learning to help robots quickly adapt\nto new environments without additional training [5]. This has the advantage of signiﬁ-\ncantly reducing the need for costly data collection and labeling. In addition, robot systems\nequipped with LLMs can process complex instructions based on their ability to understand\nand generate natural language, which can improve human–robot interactions. Furthermore,\nLLMs can be integrated with multimodal sensors such as LiDAR, depth, voice, tactile, pro-\nprioception, and visual information, which enables robots to comprehensively understand\nand adapt to their environment [6].\nLLMs have demonstrated exceptional capabilities in processing and understanding\ntext-based information, signiﬁcantly enhancing robotic communication abilities. For in-\nstance, robots can accurately comprehend and execute natural language commands via\nLLMs, providing scalability and ﬂexibility beyond traditional word-based robotic com-\nmand systems. Consequently, robots can respond more adaptably and intelligently in\ninteractions with human users, allowing them to engage in complex problem-solving and\ndecision-making processes beyond simple mechanical tasks.\nAdditionally, LLMs not only enhance a robot’s communication skills to improve HRI\nusability but also boost the robot’s planning abilities. Planning involves setting goals\nand devising a sequence of actions to achieve them, which are essential in determining a\nrobot’s autonomy and efﬁciency. LLMs interpret natural language from users and complex\ncommands, enabling robots to establish and execute suitable plans in various situations.\nMoreover, LLMs adapt ﬂexibly to new situations through a zero-shot approach and utilize\npast data for learning. These capabilities indicate that robots can play a vital role in\nautonomously navigating changing environments and resolving unexpected issues.\nMoreover, VLMs such as CLIP [7], which are trained to solve vision question answering\n(VQA) tasks, have the ability to process visual and linguistic information simultaneously.\nThis ability allows robots to visually perceive their surroundings and integrate this infor-\nmation into linguistic descriptions, enabling more sophisticated situational awareness. For\ninstance, using VLMs, a robot can recognize objects and provide descriptions, as well as\nunderstand and execute user commands based on visual cues. This integrated approach\nsigniﬁcantly enhances a robot’s autonomy and interaction capabilities.\nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which en-\nable low-level actuator control using LLMs and VLMs, Google has introduced AutoRT [10].\nAutoRT is a system where robots interact with real-world objects to collect motion data. ItAppl. Sci.2024, 14, 8868 3 of 39"
  },
  {
    "question": "What are the advantages of using LLMs in robotic systems?",
    "chunk": "4o [1] have signiﬁcantly altered the ﬁeld of robotic AI research. These LLMs, trained on\nvast amounts of textual data, have shown excellent performance in enabling robots to\ncommunicate with humans more naturally and efﬁciently. Moreover, beyond the impacts\non human–robot interaction (HRI), there is ongoing research aimed at surpassing the\nlimitations of traditional low-level robot control techniques and planning algorithms by\nutilizing the high-level situational awareness and knowledge-based planning capabilities\nof LLMs. Notably, the programming capabilities of ChatGPT in the research presented by\nMicrosoft’s ChatGPT for Robotics [2] have introduced a new paradigm for applying LLMs\nin the robotics ﬁeld.\nThe goal of robot intelligence is to enable robots to operate autonomously in complex\nenvironments, interact naturally with humans, and make high-level decisions. To promote\nAppl. Sci.2024, 14, 8868.https://doi.org/10.3390/app14198868 https://www.mdpi.com/journal/applsciAppl. Sci.2024, 14, 8868 2 of 39\nadvancements in robot intelligence, the adoption of foundation models, such as LLMs and\nvision-language models (VLMs), which boast large parameter scales and pre-training on\nmassive datasets, is accelerating. These foundation models can perform various tasks, such\nas complex language understanding and generation and visual perception, enabling robots\nto engage with their environment in a more human-like manner.\nWhile traditional robot intelligence systems are highly effective in structured and pre-\ndictable environments, they are signiﬁcantly limited in their ability to adapt to dynamically\nchanging and complex real-world scenarios. In general, the intelligence models used in\nthese robotic systems are based on supervised learning, which requires large amounts\nof labeled data. This process is inherently resource-intensive. Moreover, these models\nare designed for a speciﬁc environment and require reconﬁguration whenever the task or\nenvironment changes. This renders the robots challenging to adapt and scale to disparate\nenvironments [3]. For practical robot systems, it is essential that they are able to ﬂexibly\nrespond to the ever-changing physical environment. From this perspective, the gener-\nalization of affordable tasks, environmental adaptability, and the accuracy of execution,\nplanning, and reasoning capabilities remain signiﬁcant challenges for traditional robotic\nintelligence systems [4].\nHowever, LLMs and VLMs help a robot intelligence system to enhance its general-\nization capability in dynamic and complex real-world environments. LLMs can leverage\npre-trained knowledge from extensive datasets to augment their ability to generalize to\neveryday tasks that are typically expected of robots. Unlike the conventional supervised\nmodels, LLMs can utilize zero-shot and few-shot learning to help robots quickly adapt\nto new environments without additional training [5]. This has the advantage of signiﬁ-\ncantly reducing the need for costly data collection and labeling. In addition, robot systems\nequipped with LLMs can process complex instructions based on their ability to understand\nand generate natural language, which can improve human–robot interactions. Furthermore,\nLLMs can be integrated with multimodal sensors such as LiDAR, depth, voice, tactile, pro-\nprioception, and visual information, which enables robots to comprehensively understand\nand adapt to their environment [6].\nLLMs have demonstrated exceptional capabilities in processing and understanding\ntext-based information, signiﬁcantly enhancing robotic communication abilities. For in-\nstance, robots can accurately comprehend and execute natural language commands via\nLLMs, providing scalability and ﬂexibility beyond traditional word-based robotic com-"
  },
  {
    "question": "How does the integration of various modalities enhance robotic functionality?",
    "chunk": "While foundation models offer considerable potential for advancing robotics intelli-\ngence, several limitations and future considerations remain. These include (1) the speed\nof inference required for real-time applications, (2) the computational efﬁciency necessary\nfor embedded systems, (3) the ability to handle multi-modality information, and (4) the\nnecessity of addressing safety and ethical considerations.\nFirst of all, LLMs and VLMs hold considerable potential for enhancing robot intelli-\ngence. Nonetheless, several critical issues remain to be addressed. Foundation models,\ncharacterized as large-scale models pre-trained on extensive datasets, face challenges re-\nlated to real-time requirements and limited computational resources in robotic applications.\nMoreover, concerns such as personal information protection, privacy, and security from\nexternal attacks need resolution to enable cloud-based LLMs for robotics.\nSecondly, to enhance the computational efﬁciency and usability of LMs, there is\nongoing research into small language models (SLMs). Despite having fewer parameters,\nSLMs can achieve performance comparable to LLMs in speciﬁc applications. Several\nSLMs have been introduced, including DistilBERT [224], which is a compact version of\nGoogle’s BERT; Phi-3 [61], another SLM; Florence-2 [84], a small VLM model from Microsoft;\nMobileBERT [225], which is optimized for mobile platforms; and compact open-source\nversions of OpenAI’s GPT models such as GPT-Neo [226] and GPT-J [227]. Generally, SLMs\nare streamlined models with fewer parameters compared to LLMs, which can number in the\nbillions. SLMs utilize smaller, domain-speciﬁc datasets and require shorter training periods,\ntypically just a few weeks, unlike LLMs, which demand vast datasets for broad learning\nand multiple months of training. Developing SLMs to excel within speciﬁc domains for\nrobotic systems and ensuring real-time performance with minimal computational resources\nare essential research directions for advancing robot intelligence with SLMs.\nThe third implication is that LLMs, based on text-centered natural language process-\ning, are limited as single-modality models when applied to real-world robotic systems\nwhere information often blends in diverse ways. Research on LLMs is transitioning from\nsingle-modality to multimodality models, as evidenced by VLMs and OpenAI Sora [228],\nwith increasing demand for such models. Currently, to address the limitations of LLMs’\nsingle-modality, robotic systems are being developed with multimodality models that\nintegrate vision, such as VLMs. However, relying solely on text and images falls short\nof the diverse information range required in the real world, including images, sounds,\nvideos, and proprioceptive sensory information (such as the position, orientation, balance,\nmovement degree, and direction of various parts of the robot). Proprioceptive sensory\ninformation related to actions and movements is particularly vital for enhancing dynamic\nhuman interaction, information processing, and manipulation and planning skills based on\ndynamic movements. For instance, the integrated VLA model, which facilitates low-level\ncontrol based on LLMs and VLMs as shown by Google’s RT-2 model, highlights the neces-\nsity for models capable of integrating information from a broader range of modalities to\nenhance robot intelligence.Appl. Sci.2024, 14, 8868 29 of 39\nFinally, the fourth area to consider is how to address safety and ethical issues when\nLLM is applied to robotic intelligence systems. Studies were conducted to address the\nissue of discriminatory and unsafe behaviors that may be generated by robot applications"
  },
  {
    "question": "What kind of support did the project receive?",
    "chunk": "sualization, H.J. and H.L.; supervision, S.S. and C.K.; project administration, S.S.; funding acquisition,\nS.S. All authors have read and agreed to the published version of the manuscript.\nFunding: This work was supported by the Technology Innovation Program (RS-2024-00423702,\nA Meta-Humanoid with Hypermodal Cognitivity and Role Dexterity: Adroid4X) funded by the\nMinistry of Trade, Industry, and Energy (MOTIE, Korea) and Regional Innovation Strategy (RIS)\nthrough the National Research Foundation of Korea (NRF) funded by the Ministry of Education\n(MOE) (2023RIS-007).\nInstitutional Review Board Statement:Not applicable.\nInformed Consent Statement:Not applicable.\nData Availability Statement:No new data were created or analyzed in this study. Data sharing is\nnot applicable to this article.\nConﬂicts of Interest:The authors declare no conﬂicts of interest.\nReferences\n1. Hello GPT-4o. Available online:https://openai.com/index/hello-gpt-4o/ (accessed on 13 August 2024).\n2. Vemprala, S.H.; Bonatti, R.; Bucker, A.; Kapoor, A. ChatGPT for Robotics: Design Principles and Model Abilities.IEEE Access\n2024, 12, 55682–55696. [CrossRef]\n3. Hu, Y.; Xie, Q.; Jain, V.; Francis, J.; Patrikar, J.; Keetha, N.; Kim, S.; Xie, Y.; Zhang, T.; Zhao, S.; et al. Toward General-Purpose\nRobots via Foundation Models: A Survey and Meta-Analysis.arXiv 2023, arXiv:2312.08782.\n4. Xiao, X.; Liu, J.; Wang, Z.; Zhou, Y.; Qi, Y.; Cheng, Q.; He, B.; Jiang, S. Robot Learning in the Era of Foundation Models: A Survey.\narXiv 2023, arXiv:2311.14379.\n5. Mao, Y.; Ge, Y.; Fan, Y.; Xu, W.; Mi, Y.; Hu, Z.; Gao, Y. A Survey on LoRA of Large Language Models.arXiv 2024, arXiv:2407.11046.\n6. Hunt, W.; Ramchurn, S.D.; Soorati, M.D. A Survey of Language-Based Communication in Robotics.arXiv 2024, arXiv:2406.04086.\n7. Radford, A.; Kim, J.W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. Learning\nTransferable Visual Models From Natural Language Supervision.Proc. Mach. Learn. Res.2021, 139, 8748–8763.\n8. Brohan, A.; Brown, N.; Carbajal, J.; Chebotar, Y.; Dabis, J.; Finn, C.; Gopalakrishnan, K.; Hausman, K.; Herzog, A.; Hsu, J.; et al.\nRT-1: Robotics Transformer for Real-World Control at Scale. In Proceedings of the Robotics: Science and Systems 2023, Daegu,\nRepublic of Korea, 10–14 July 2023. [CrossRef]"
  },
  {
    "question": "In what ways can robotics benefit from commonsense reasoning?",
    "chunk": "Table 5.Summary of the reviewed papers in this study.\nName Explanation Ref.\nReward Design in RL\n• Eureka automatically generates and improves reward\nfunctions based on the virtual environment source\ncode.\n• DrEureka builds reward-aware physics priors using\nEureka and supports effective operation in the real\nworld through domain randomization.\n• LLMs design and reﬁne reward functions based on\nnatural language input.\n• LLMs and VLMs integrate multimodal data to\ngenerate reward functions.\n[11,134,136–139,176–180]\nLow-level\nControl\n• Generating commands to control actuators capable of\nlow-level control.\n• RT-1 and RT-2 enable robots to perform complex tasks\nbased on language-vision data.\n• AutoRT establishes a system where robots can\nautonomously collect and utilize data.\n[8–10,144–148,181–183]\nHigh-level\nPlanning\n• LLMs provide an effective methodology for tasks\nrelated to high-level planning within robotic systems.\n• By using natural language, LLMs can formulate plans\nto solve tasks that require long-horizon reasoning.\n• LLMs assess the feasibility of actions to determine and\nexecute the optimal robotic behavior.\n• LLMs generate behavior trees to structure complex\nrobotic actions accurately.\n[149–160,184–207]\nManipulation\n• Using LLMs and VLMs to integrate language and\nvision data allows various manipulations.\n• LLMs interpret high-level instructions to generate the\nnecessary robot actions and assess their feasibility.\n• VLMs extract object information from images to assist\nin performing manipulations.\n[161–167,208–215]\nScene\nUnderstanding\n• To solve VQA problems, use VLMs to extract\nhigh-level information from vision data.\n• For scene understanding, estimate and identify objects\nand evaluate relationships between objects.\n• For navigation, convert natural language instructions\nand combine them with vision data to identify the\nimage through probability distributions.\n[168–175,216–223]\n5. Discussion and Future Directions\nThe review revealed two potentials of foundation models: (1) commonsense reasoning\nfor planning and (2) the ability to generate code.\nThe ﬁrst ﬁnding from this review study is the potential to enhance robot intelligence\nthrough foundation models. Beyond the studies mentioned here, numerous recent stud-\nies have shown that pre-trained models such as LLMs and VLMs can enhance various\naspects of robot intelligence, such as situational awareness, high-level task planning, and\nhuman interaction. LLMs allow communication with humans in natural languages, objectAppl. Sci.2024, 14, 8868 28 of 39\nutilization based on extensive information, and high-level planning using that information.\nVLMs can describe tasks in text and understand visual information. Furthermore, the\ninformation from VLMs can be supplemented by connecting to knowledge databases via\nLLMs. These capabilities are crucial for enhancing robot intelligence, broadening the scope\nof robot applications, and maximizing robot utility.\nThe second ﬁnding is the code generation capability of LLMs, which has the potential\nto automate the robot development process traditionally performed by humans. Addi-\ntionally, robots that can autonomously update their own algorithms are no longer just\nscience ﬁction. Although limitations exist for robots to self-update, frameworks such as\nEureka and DrEureka, which automatically enhanced reinforcement learning performance\nfor robot motion control, demonstrate the potential for future advancements. This suggests\nthat LLMs may not only enhance human interactions but could also pave the way for\nself-improvement without human intervention."
  },
  {
    "question": "What is the process for generating a high-level plan in complex environments?",
    "chunk": "mental feedback during plan execution and revised the plan accordingly. The results\nindicated that the integration of programming language features substantially improved\ntask performance in contexts such as VirtualHome and real-world manipulation tasks in\nterms of success rate, goal conditions recall, and executability.Appl. Sci.2024, 14, 8868 21 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 10.ProgPrompt is a system that uses Python programming structures to provide environ-\nmental information and actions, enhancing the success rate of robot task planning through an error\nrecovery feedback mechanism and environmental state feedback [156].\nRana [157] introduced SayPlan, a scalable method for large-scale task planning using\nLLMs and based on a 3D Scene Graph (3DSG) representation. SayPlan involved the LLM\nsearching a collapsed 3D scene graph and task instructions to identify all relevant items\nand then locating the subgraph that contained the necessary items to complete the task.\nThe identiﬁed subgraph was subsequently used by the LLM to generate a high-level plan\nthat addressed the navigational aspect of the task. This plan was formatted as a JSON\n3D scene graph and subjected to a repetitive replanning process through feedback from\nthe scene graph simulator and a set of API calls for manipulation and operation until an\nexecutable plan was determined. SayPlan was tested in two large-scale environments,\nfeaturing up to three ﬂoors, 36 rooms, and 140 assets and objects, proving its capability\nto ground large-scale and long-horizon task plans from abstract and natural language\ninstructions, thereby enabling a mobile manipulator robot to execute these tasks.\nZeng [158], as shown in Figure11, proposed the Socratic model (SM), a modular\nframework that synergistically utilizes various forms of knowledge and employs multiple\npre-trained models to exchange information and leverage new multimodal capabilities. SM\noperates without ﬁne-tuning by integrating diverse pre-trained models and functions in\na zero-shot approach (e.g., using multimodal prompts), which enables it to harness new\nmultimodal capabilities. SM demonstrated state-of-the-art performance in zero-shot image\ncaptioning and video-to-text retrieval, and it effectively answered free-form questions about\negocentric video. Additionally, it supported interactions with external APIs and databases\n(e.g., web search) for multimodal assistive dialogue, robot perception, and planning, among"
  },
  {
    "question": "What challenges do foundation models face in real-time applications?",
    "chunk": "• For scene understanding, estimate and identify objects\nand evaluate relationships between objects.\n• For navigation, convert natural language instructions\nand combine them with vision data to identify the\nimage through probability distributions.\n[168–175,216–223]\n5. Discussion and Future Directions\nThe review revealed two potentials of foundation models: (1) commonsense reasoning\nfor planning and (2) the ability to generate code.\nThe ﬁrst ﬁnding from this review study is the potential to enhance robot intelligence\nthrough foundation models. Beyond the studies mentioned here, numerous recent stud-\nies have shown that pre-trained models such as LLMs and VLMs can enhance various\naspects of robot intelligence, such as situational awareness, high-level task planning, and\nhuman interaction. LLMs allow communication with humans in natural languages, objectAppl. Sci.2024, 14, 8868 28 of 39\nutilization based on extensive information, and high-level planning using that information.\nVLMs can describe tasks in text and understand visual information. Furthermore, the\ninformation from VLMs can be supplemented by connecting to knowledge databases via\nLLMs. These capabilities are crucial for enhancing robot intelligence, broadening the scope\nof robot applications, and maximizing robot utility.\nThe second ﬁnding is the code generation capability of LLMs, which has the potential\nto automate the robot development process traditionally performed by humans. Addi-\ntionally, robots that can autonomously update their own algorithms are no longer just\nscience ﬁction. Although limitations exist for robots to self-update, frameworks such as\nEureka and DrEureka, which automatically enhanced reinforcement learning performance\nfor robot motion control, demonstrate the potential for future advancements. This suggests\nthat LLMs may not only enhance human interactions but could also pave the way for\nself-improvement without human intervention.\nWhile foundation models offer considerable potential for advancing robotics intelli-\ngence, several limitations and future considerations remain. These include (1) the speed\nof inference required for real-time applications, (2) the computational efﬁciency necessary\nfor embedded systems, (3) the ability to handle multi-modality information, and (4) the\nnecessity of addressing safety and ethical considerations.\nFirst of all, LLMs and VLMs hold considerable potential for enhancing robot intelli-\ngence. Nonetheless, several critical issues remain to be addressed. Foundation models,\ncharacterized as large-scale models pre-trained on extensive datasets, face challenges re-\nlated to real-time requirements and limited computational resources in robotic applications.\nMoreover, concerns such as personal information protection, privacy, and security from\nexternal attacks need resolution to enable cloud-based LLMs for robotics.\nSecondly, to enhance the computational efﬁciency and usability of LMs, there is\nongoing research into small language models (SLMs). Despite having fewer parameters,\nSLMs can achieve performance comparable to LLMs in speciﬁc applications. Several\nSLMs have been introduced, including DistilBERT [224], which is a compact version of\nGoogle’s BERT; Phi-3 [61], another SLM; Florence-2 [84], a small VLM model from Microsoft;\nMobileBERT [225], which is optimized for mobile platforms; and compact open-source\nversions of OpenAI’s GPT models such as GPT-Neo [226] and GPT-J [227]. Generally, SLMs\nare streamlined models with fewer parameters compared to LLMs, which can number in the\nbillions. SLMs utilize smaller, domain-speciﬁc datasets and require shorter training periods,\ntypically just a few weeks, unlike LLMs, which demand vast datasets for broad learning"
  },
  {
    "question": "In what ways do automated reward functions compare to expert-designed ones?",
    "chunk": "Figure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses expert-\ndesigned functions through iterative improvements [11]. \nFollowing Eureka, DrEureka [134], shown in Figure 5, was developed to address the \nsim-to-real problem by automatically con ﬁguring appropriate reward functions and do-\nmain randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses\nexpert-designed functions through iterative improvements [11].\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 13 of 39 \n \n \nFigure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses expert-\ndesigned functions through iterative improvements [11]. \nFollowing Eureka, DrEureka [134], shown in Figure 5, was developed to address the \nsim-to-real problem by automatically con ﬁguring appropriate reward functions and do-\nmain randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 5.DrEureka leverages LLM to design reward functions and solves the sim-to-real problem\nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134].\nXie [136] introducedText2Reward, a framework that automatically generated dense\nreward functions for reinforcement learning using LLMs. Provided with a goal expressed\nin natural language, Text2Reward produced executable dense reward functions derived\nfrom a compact representation of the environment. This framework generated free-form\ndense reward codes and delivered performance comparable to or surpassing that of policies\ntrained with expert-designed codes across a variety of tasks, including 17 manipulator-\nrelated tasks and six novel locomotion behaviors. Additionally, Text2Reward incorporated\nuser feedback to iteratively enhance the generated reward functions, thereby increasing the\nsuccess rate of the learned policies.\nDi Palo [137] explored the use of LLMs and VLMs to improve reinforcement learning"
  },
  {
    "question": "Why is it necessary to consider social biases in LLMs for robotics?",
    "chunk": "and multiple months of training. Developing SLMs to excel within speciﬁc domains for\nrobotic systems and ensuring real-time performance with minimal computational resources\nare essential research directions for advancing robot intelligence with SLMs.\nThe third implication is that LLMs, based on text-centered natural language process-\ning, are limited as single-modality models when applied to real-world robotic systems\nwhere information often blends in diverse ways. Research on LLMs is transitioning from\nsingle-modality to multimodality models, as evidenced by VLMs and OpenAI Sora [228],\nwith increasing demand for such models. Currently, to address the limitations of LLMs’\nsingle-modality, robotic systems are being developed with multimodality models that\nintegrate vision, such as VLMs. However, relying solely on text and images falls short\nof the diverse information range required in the real world, including images, sounds,\nvideos, and proprioceptive sensory information (such as the position, orientation, balance,\nmovement degree, and direction of various parts of the robot). Proprioceptive sensory\ninformation related to actions and movements is particularly vital for enhancing dynamic\nhuman interaction, information processing, and manipulation and planning skills based on\ndynamic movements. For instance, the integrated VLA model, which facilitates low-level\ncontrol based on LLMs and VLMs as shown by Google’s RT-2 model, highlights the neces-\nsity for models capable of integrating information from a broader range of modalities to\nenhance robot intelligence.Appl. Sci.2024, 14, 8868 29 of 39\nFinally, the fourth area to consider is how to address safety and ethical issues when\nLLM is applied to robotic intelligence systems. Studies were conducted to address the\nissue of discriminatory and unsafe behaviors that may be generated by robot applications\npowered by LLMs [229]. The outputs of LLMs have the potential to generate content\nthat is biased based on personal characteristics (such as race, nationality, religion, gender,\ndisability, and so forth). In addition, they can also be used to instruct robotic systems to\nengage in violent or illegal behaviors such as misstatements, sexual predation, etc. Notable\nexamples include discriminatory behaviors such as inadequate recognition of children\nor individuals with speciﬁc skin tones in human detection systems, and the exclusion\nof individuals with disabilities from task assignments. It is imperative to consider the\npotential social biases of LLM when integrating with robotic systems. Although this kind of\nconsideration was secondary in traditional robotic systems because of the limitation of their\nlanguage capability, it is a necessary consideration for LLMs to be able to generate human-\nlike language. To address this issue, previous studies have attempted to resolve it in various\nways, such as AutoRT’s constitutional rules [10], DrEureka’s safety instructions [134], and\nNeMo’s guardrails [230]. The guideline-based output control of LLMs can represent an\naccessible method to ensure safety.\nAs an extension of this point, safety issues can be identiﬁed when integrating LLMs\nand VLMs into robotic intelligence systems [231]. Typically, in robotic intelligence systems,\nLLM models generate high-level action plans in various forms, such as programming\ncodes and behavior trees based on natural language or vector prompts. At this point, a\nprompt attack has the potential to disrupt the inference of the LLMs, thereby threatening\nthe reliability and safety of the robotic system. Prompt injection is one of the prompt attacks,\nwhereby the inference of LLMs is subtly altered through speciﬁc inputs. Jailbreak, another\nprompt attack, bypasses safety rules and causes LLMs to generate abnormal behaviors to"
  },
  {
    "question": "What challenges do researchers face in implementing foundation models in robotics?",
    "chunk": "Robots via Foundation Models: A Survey and Meta-Analysis.arXiv 2023, arXiv:2312.08782.\n4. Xiao, X.; Liu, J.; Wang, Z.; Zhou, Y.; Qi, Y.; Cheng, Q.; He, B.; Jiang, S. Robot Learning in the Era of Foundation Models: A Survey.\narXiv 2023, arXiv:2311.14379.\n5. Mao, Y.; Ge, Y.; Fan, Y.; Xu, W.; Mi, Y.; Hu, Z.; Gao, Y. A Survey on LoRA of Large Language Models.arXiv 2024, arXiv:2407.11046.\n6. Hunt, W.; Ramchurn, S.D.; Soorati, M.D. A Survey of Language-Based Communication in Robotics.arXiv 2024, arXiv:2406.04086.\n7. Radford, A.; Kim, J.W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. Learning\nTransferable Visual Models From Natural Language Supervision.Proc. Mach. Learn. Res.2021, 139, 8748–8763.\n8. Brohan, A.; Brown, N.; Carbajal, J.; Chebotar, Y.; Dabis, J.; Finn, C.; Gopalakrishnan, K.; Hausman, K.; Herzog, A.; Hsu, J.; et al.\nRT-1: Robotics Transformer for Real-World Control at Scale. In Proceedings of the Robotics: Science and Systems 2023, Daegu,\nRepublic of Korea, 10–14 July 2023. [CrossRef]\n9. Brohan, A.; Brown, N.; Carbajal, J.; Chebotar, Y.; Chen, X.; Choromanski, K.; Ding, T.; Driess, D.; Dubey, A.; Finn, C.; et al. RT-2:\nVision-Language-Action Models Transfer Web Knowledge to Robotic Control.arXiv 2023, arXiv:2307.15818.\n10. Ahn, M.; Dwibedi, D.; Finn, C.; Arenas, M.G.; Gopalakrishnan, K.; Hausman, K.; Ichter, B.; Irpan, A.; Joshi, N.; Julian, R.; et al.\nAutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents.arXiv 2024, arXiv:2401.12963.\n11. Ma, Y.J.; Liang, W.; Wang, G.; Huang, D.-A.; Bastani, O.; Jayaraman, D.; Zhu, Y.; Fan, L.; Anandkumar, A. Eureka: Human-Level\nReward Design via Coding Large Language Models.arXiv 2023, arXiv:2310.12931.\n12. Ma, Y.; Song, Z.; Zhuang, Y.; Hao, J.; King, I. A Survey on Vision-Language-Action Models for Embodied AI.arXiv 2024,\narXiv:2405.14093.\n13. Zhou, H.; Yao, X.; Meng, Y.; Sun, S.; Bing, Z.; Huang, K.; Knoll, A. Language-Conditioned Learning for Robotic Manipulation: A\nSurvey. arXiv 2023, arXiv:2312.10807."
  },
  {
    "question": "How does EmbodiedGPT improve object recognition capabilities?",
    "chunk": "state representation, while the world model writer updated the system’s state according to\nexecution outcomes. By facilitating access to the world state ‘memory’, Statler improved\nLLMs’ ability to reason about planning tasks with extended time horizons, overcoming\nlimitations imposed by context length.\nMu [150], shown in Figure7, introduced EmbodiedGPT, a model speciﬁcally designed\nfor Embodied AI, which leverages LLMs. This framework processes visual observations\nand natural language to establish long-term plans and execute tasks in real-time. Em-\nbodiedGPT utilizes pre-trained vision transformers and the LLaMA language model to\nencode visual features and map them to the language modality. The generated plan was\nsubsequently converted into speciﬁc task commands using general visual tokens, encoded\nby the vision model. The framework’s functionality comprises (1) encoding current visual\nfeatures, (2) mapping visual features to the language modality via attention-based interac-\ntions between visual tokens and text queries or learnable embedded queries, (3) generating\nplans with the LLaMA language model and translating them into speciﬁc task commands,\nand (4) querying the encoded visual tokens from the vision model and translating them\ninto low-level control commands through a downstream policy network for task execu-\ntion. Experimental results, utilizing the MS-COCO dataset, revealed that EmbodiedGPT\nexcels in object recognition and understanding spatial relationships. Notably, implement-Appl. Sci.2024, 14, 8868 18 of 39\ning a closed-loop design and a “chain-of-thought” training mode signiﬁcantly enhanced\nEmbodiedGPT’s performance. These results demonstrate that EmbodiedGPT effectively\nhandles various autonomous tasks, exhibiting superior capability in object recognition,\nunderstanding spatial relationships, and generating logical, executable plans.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 18 of 39 \n \ntime. EmbodiedGPT utilizes pre-trained visi on transformers and the LLaMA language \nmodel to encode visual features and map them  to the language modality. The generated \nplan was subsequently converted into speci ﬁc task commands using general visual \ntokens, encoded by the vision model. The framework’s functionality comprises (1) \nencoding current visual features, (2) mapping visual features to the language modality \nvia a ttention-based interactions between visual tokens and text queries or learnable \nembedded queries, (3) generating plans with the LLaMA language model and translating \nthem into speciﬁc task commands, and (4) querying the encoded visual tokens from the \nvision model and translating them into low-level control commands through a \ndownstream policy network for task execution. Experimental results, utilizing the MS-\nCOCO dataset, revealed that EmbodiedG PT excels in object recognition and \nunderstanding spatial relationships. Notabl y, implementing a closed-loop design and a \n“chain-of-thought” training mode signi ﬁcantly enhanced EmbodiedGPT’s performance. \nThese results demonstrate that EmbodiedGPT e ﬀectively handles various autonomous \ntasks, exhibiting superior capability in object recognition, understanding spatial \nrelationships, and generating logical, executable plans. \n \nFigure 7. After encoding visual features, they are mappe d using visual tokens and text queries. A \nplan is then created with the LLaMA model and turned into task commands. The visual tokens are \nqueried and converted into low-level control commands to perform the task [150]. \nChen [151] introduced the language-model-based commonsense reasoning (LMCR)"
  },
  {
    "question": "What techniques are used to enhance the performance of LLMs?",
    "chunk": "model focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM \nmodel inference. \n \nFigure 3. An overview of four strategies for parameter-eﬃcient ﬁne-tuning: (a) Adapter Tuning, \n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5]. \nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a \nlow-rank constraint on transformer layers to approximate the update matrices through \ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates \nthe parameter updates using low-rank dec omposition matrices. The primary bene ﬁt of \nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning, \nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory \nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning. \nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119]. \n3.3. Prompt Techniques for Increasing LLM Performance \nTo enhance the performance of LLMs, the most straightforward approach involves \ntraining with additional data via ﬁne-tuning techniques, which mirrors supervised learn-\ning in conventional machine learning. Another method for improving performance in-\nvolves the use of in-context learning, which capitalizes on prompts for zero-shot learning, \na capability ﬁrst observed in LLMs with the advent of GPT-3. The adaptation of these \nprompts for speciﬁc tasks is known as prompt engineering. Fundamentally, prompt engi-\nneering (or prompting) entails supplying inputs  to the model to perform a distinct task, \ndesigning the input format to encapsulate the task’s purpose and context, and generating \nthe desired output. The four components of pr ompt engineering can be analyzed as fol-\nlows: within the prompt, “ Instructions” delineate the speci ﬁc tasks or directives for the \nmodel and “Context” provides external or additional contextual information that can tune \nthe model. Furthermore, “Input data” refers to the type of input or questions seeking an-\nswers, and “ Output data” deﬁnes the output type or format within the prompt, thereby \noptimizing the LLM’s performance for particular tasks. Various methodologies for creat-\ning prompts have been introduced, as described below. \nFigure 3. An overview of four strategies for parameter-efﬁcient ﬁne-tuning: (a) Adapter Tuning,\n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5].\nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a\nlow-rank constraint on transformer layers to approximate the update matrices through\ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates\nthe parameter updates using low-rank decomposition matrices. The primary beneﬁt of\nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning,\nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory\nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning.\nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119].\n3.3. Prompt Techniques for Increasing LLM Performance"
  },
  {
    "question": "How successful was the new locomotion command interface developed by Tang?",
    "chunk": "of-thought prompting techniques to RT-2 has proven effective in solving more complex\nsemantic inference tasks, such as using a rock as an improvised hammer or offering an\nenergy drink instead of a carbonated beverage to a thirsty person. In comparison with\nthe earlier study on RT-1, RT-2 demonstrates enhanced performance in both familiar and\nnovel tasks.\nAutoRT [10] is a follow-up study based on the research results of RT-1 and RT-2,\nestablishing an orchestration of large-scale robotic agents for data collection in real-world\nscenarios. AutoRT employed 53 robots to gather 77,000 real robot episodes over seven\nmonths through both teleoperation and autonomous robot policies. At the heart of AutoRT\nis a robust foundation model that generates ‘task proposals’ based on given visual observa-\ntions. Notably, AutoRT introduces a ‘Robot Constitution’ using constitutional prompting to\nensure actions during the task proposal process do not compromise the safety of the robot\nor nearby individuals. This Robot Constitution, inspired by Asimov’s three laws [142],\ncomprises basic rules, safety rules that identify unsafe or unwanted tasks, and embodiment\nrules that clarify the robot’s operational boundaries.\nAutoRT enhances data collection by initially scanning the surroundings to identify\ninteresting scenes or tasks (exploration). It interprets the given context through a VLM\nand proposes potential tasks via an LLM (task generation). Subsequently, tasks suggested\nby the LLM are screened (affordance) to assess their feasibility and the need for human\nintervention, employing the Robot Constitution. During this procedure, viable tasks are\nchosen and performed, while pertinent data are gathered (data collection). The collected\ndata are then assessed for (diversity scoring) the visual diversity of the robot trajectories\nand the linguistic diversity of the language instructions generated by AutoRT (LLM). The\naim of this diversity evaluation is to conﬁrm that, unlike simulations, real-world data\ncollection by robots is labor-intensive, making it essential to gather data across a broad\nspectrum of tasks. Experimental outcomes illustrate that AutoRT achieves higher visual\nand linguistic diversity compared to RT-1 or BC-Z [143].\nOther researchers include Tang [144], who developed an approach that connects\nnatural language user commands with a locomotion controller using foot contact patterns as\nan interface for low-level commands. This innovative interface translates human commands\ninto the robot’s foot contact patterns, allowing the robot to move at a speciﬁed speed with\nprecise timing for each foot’s contact with the ground. To achieve this, the robot used a\ncyclic sliding window to extract foot contact ﬂags from a pattern template, thus generating\nthe required foot contact patterns. During training, a random pattern generator created\nfoot contact patterns, and during testing, an LLM translated human commands into these\npatterns. The robot then adjusted its movements based on the foot contact patterns it\nlearned through deep reinforcement learning, closely adhering to the intended foot contact\npatterns and speed commands. This approach demonstrated a 50% higher success rate in\ntask evaluation (across 30 tasks, including standing still) compared to two baselines (which\nemployed discrete gaits and sinusoidal functions as interfaces), successfully solving 10\nmore tasks than the baselines.\nMandi [145] introduced a novel method for multi-robot collaboration that utilizes\nLLMs for both high-level communication and low-level path planning. In this method, the\nrobots employ the LLM to discuss and reason about task strategies. They generate sub-taskAppl. Sci.2024, 14, 8868 16 of 39\nplans and task space waypoint paths, which a multi-arm motion planner then uses to expe-"
  },
  {
    "question": "How do large model parameters impact the fine-tuning process?",
    "chunk": "alignment tuning (or preference alignment) seeks to align the behavior of LLMs with hu-\nman values and preferences. Prominent methods include reinforcement learning from hu-\nman feedback (RLHF) [110], which involves ﬁne-tuning LLMs using human feedback to \nbetter reﬂect human values, and direct preference optimization (DPO) [111], focusing on \ntraining with pairs of human preferences th at usually include an input prompt and the \npreferred and non-preferred responses. \nFor both instruction tuning and alignment tuning, which involve training LLMs with \nextensively large model para meters, substantial GPU memo ry and computational re-\nsources are required, with high costs typi cally incurred when utilizing cloud-based \nFigure 2.Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx Decoder\n(middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectangles\nrepresent attention between preﬁx tokens, attention between preﬁx and target tokens, attention\nbetween target tokens, and masked attention [5].\nIn terms of the tuning of LLMs, these models are essentially pre-trained on mas-\nsive datasets and require ﬁne-tuning for different application domains. However, the\nconsiderable model size and number of parameters pose challenges for ﬁne-tuning on\nstandard computers and GPUs. The subsequent sections will discuss methods to address\nthese challenges.\nLLM tuning is broadly divided into two categories based on the training objective.\nInstruction tuning is a form of supervised learning where the training data typically include\ndescriptions of tasks, inputs, and corresponding outputs. This type of tuning is designed\n(1) to enhance the functional capabilities of LLMs, (2) to specialize them by training with\ndiscipline-speciﬁc information, and (3) to improve task generalization and consistency\nthrough a better understanding of natural language commands. Conversely, alignment\ntuning (or preference alignment) seeks to align the behavior of LLMs with human values\nand preferences. Prominent methods include reinforcement learning from human feedbackAppl. Sci.2024, 14, 8868 9 of 39\n(RLHF) [110], which involves ﬁne-tuning LLMs using human feedback to better reﬂect\nhuman values, and direct preference optimization (DPO) [111], focusing on training with\npairs of human preferences that usually include an input prompt and the preferred and\nnon-preferred responses.\nFor both instruction tuning and alignment tuning, which involve training LLMs\nwith extensively large model parameters, substantial GPU memory and computational\nresources are required, with high costs typically incurred when utilizing cloud-based\nresources. Under these conditions, parameter-efﬁcient ﬁne-tuning (PEFT) offers a method\ndesigned to efﬁciently conduct ﬁne-tuning of such LLMs [112].\nAmong the methods of PEFT, there are four major approaches as shown in Figure3:\nadapter tuning, prompt tuning, preﬁx tuning, and low-rank adaptation (LoRA). Adapter\ntuning [113,114] involves integrating small neural network modules, known as adapters,\ninto the core components of a transformer model, speciﬁcally into the attention and feed-\nforward layers. These adapters are inserted serially following these layers, allowing ﬁne-\ntuning of only the adapter modules according to speciﬁc task goals, while the parameters\nof the original language model remain unchanged. Consequently, adapter tuning effec-\ntively reduces the number of trainable parameters. Additionally, prompt tuning [115,116]"
  },
  {
    "question": "How many databases were reviewed in the survey and which ones?",
    "chunk": "Toward General-Purpose Robots via Foundation\nModels: A Survey and Meta-Analysis Foundation Models [ 3]\nA Survey of Large Language Models LLM [ 5]\nVision-Language Models for Vision Tasks: A Survey VLM [ 12]\nLanguage-conditioned Learning for Robotic\nManipulation: A Survey LLM, VLM, Manipulation [ 13]\nFoundation Models in Robotics: Applications,\nChallenges, and the Future Foundation Models [ 14]\nVision-and-Language Navigation: A Survey of Tasks,\nMethods, and Future Directions VLN [ 15]\n2. Review Protocol\nThis survey covered four databases: Web of Science, ScienceDirect, IEEE Xplore, and\narXiv. In fact, many of the articles surveyed had not been peer-reviewed and published at\nthe time of our search because the subject matter was relatively recent. Therefore, a consid-\nerable number of articles reviewed in this survey were sourced from the arXiv database.\nThe selection process of this study primarily relied on two iterations:\n• The titles and abstracts of the articles were reviewed to eliminate duplicates and\nirrelevant articles.\n• The full texts of the selected articles from the ﬁrst iteration were thoroughly examined\nand categorized.\n• Article searching began on 18 September 2023.\nRegarding the search queries,\n• the publication years were those after 2020,\n• the keywords of Robotics and LLM, which were ((“Robotic” OR “Robotics”) AND\n(“LLM” OR “LM” OR “Large Language Model” OR “Language Model”)), and relevant\njournal and conference articles written in English were considered.\nFrom these search criteria, recent studies utilizing language models in robotics research\nwere expected to be collected. Our aim is to provide a robust understanding of how\nlanguage models and their variants have been utilized to enhance robot intelligence in\nthe literature.\nAll articles that met the above criteria were included in this review. Following an\nintensive survey of the abstracts of the selected articles, we categorized the research topics\ninto ﬁve groups: reward design for reinforcement learning, low-level control, high-level\nplanning, manipulation, and scene understanding. Figure1 illustrates these ﬁve categories.\nOur categorization was based on a thorough review of the sources in the literature. Subse-\nquently, duplicate articles were removed, and those not meeting the speciﬁed eligibility\ncriteria were excluded. The exclusion criteria included: (1) articles in languages other than\nEnglish and (2) articles discussing general concepts that do not focus on deep reinforcement\nlearning-based manipulation.\n3. Related Works\n3.1. Language Model\nZhao’s LLM review paper [5] categorizes the evolution of Language Models (LMs)\ninto four phases. The initial stage, the statistical language model (SLM) [16–19], utilizes\nmethods based on statistical learning techniques and the Markov assumption to construct\nword prediction models. A notable method from this phase is the n-gram language model,\nwhich predicts words based on a ﬁxed context length of n. Although SLMs have enhanced\nperformance in various domains such as information retrieval (IR) [16,20] and naturalAppl. Sci.2024, 14, 8868 6 of 39\nlanguage processing (NLP) [21–23], higher-order language models have encountered lim-\nitations due to the curse of dimensionality, which necessitates estimating exponentially\nincreasing transition probabilities.\nThe subsequent phase of LMs, termed neural language models (NLMs), leveraged\nneural networks such as multi-layer perceptron (MLP) and recurrent neural networks"
  },
  {
    "question": "What type of data was analyzed in this study?",
    "chunk": "sualization, H.J. and H.L.; supervision, S.S. and C.K.; project administration, S.S.; funding acquisition,\nS.S. All authors have read and agreed to the published version of the manuscript.\nFunding: This work was supported by the Technology Innovation Program (RS-2024-00423702,\nA Meta-Humanoid with Hypermodal Cognitivity and Role Dexterity: Adroid4X) funded by the\nMinistry of Trade, Industry, and Energy (MOTIE, Korea) and Regional Innovation Strategy (RIS)\nthrough the National Research Foundation of Korea (NRF) funded by the Ministry of Education\n(MOE) (2023RIS-007).\nInstitutional Review Board Statement:Not applicable.\nInformed Consent Statement:Not applicable.\nData Availability Statement:No new data were created or analyzed in this study. Data sharing is\nnot applicable to this article.\nConﬂicts of Interest:The authors declare no conﬂicts of interest.\nReferences\n1. Hello GPT-4o. Available online:https://openai.com/index/hello-gpt-4o/ (accessed on 13 August 2024).\n2. Vemprala, S.H.; Bonatti, R.; Bucker, A.; Kapoor, A. ChatGPT for Robotics: Design Principles and Model Abilities.IEEE Access\n2024, 12, 55682–55696. [CrossRef]\n3. Hu, Y.; Xie, Q.; Jain, V.; Francis, J.; Patrikar, J.; Keetha, N.; Kim, S.; Xie, Y.; Zhang, T.; Zhao, S.; et al. Toward General-Purpose\nRobots via Foundation Models: A Survey and Meta-Analysis.arXiv 2023, arXiv:2312.08782.\n4. Xiao, X.; Liu, J.; Wang, Z.; Zhou, Y.; Qi, Y.; Cheng, Q.; He, B.; Jiang, S. Robot Learning in the Era of Foundation Models: A Survey.\narXiv 2023, arXiv:2311.14379.\n5. Mao, Y.; Ge, Y.; Fan, Y.; Xu, W.; Mi, Y.; Hu, Z.; Gao, Y. A Survey on LoRA of Large Language Models.arXiv 2024, arXiv:2407.11046.\n6. Hunt, W.; Ramchurn, S.D.; Soorati, M.D. A Survey of Language-Based Communication in Robotics.arXiv 2024, arXiv:2406.04086.\n7. Radford, A.; Kim, J.W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. Learning\nTransferable Visual Models From Natural Language Supervision.Proc. Mach. Learn. Res.2021, 139, 8748–8763.\n8. Brohan, A.; Brown, N.; Carbajal, J.; Chebotar, Y.; Dabis, J.; Finn, C.; Gopalakrishnan, K.; Hausman, K.; Herzog, A.; Hsu, J.; et al.\nRT-1: Robotics Transformer for Real-World Control at Scale. In Proceedings of the Robotics: Science and Systems 2023, Daegu,\nRepublic of Korea, 10–14 July 2023. [CrossRef]"
  },
  {
    "question": "What are the implications of using LLMs for high-level planning in robots?",
    "chunk": "supporting the creation of high-level policies for robots and accommodating a variety of\nrobotic tasks. Speciﬁcally, CaP interpreted natural language instructions through descrip-\ntions and formulated an action plan for the robot. Moreover, it utilized VLMs such as ViLD\nand MDETR to identify objects and ascertain their locations. Based on this information,\nthe framework controlled the robot’s movements to carry out speciﬁed tasks. The paper\ndemonstrated the CaP framework across diverse domains, including whiteboard drawing,\ntabletop manipulation, and mobile robot navigation and manipulation. Experimental\nresults showed that CaP achieved similar or better success rates than existing systems\nsuch as CLIPort, displaying notably strong generalization capabilities for new tasks. These\nﬁndings underscored the ﬂexibility and efﬁcacy of the CaP framework, establishing its\neffectiveness across various robotic systems.\nMirchandani [148], shown in Figure6, suggested that pre-trained LLMs could autore-\ngressively complete complex token sequences and function as general sequence modelers\nthrough in-context learning without needing additional training. Expanding on this con-\ncept, the study evaluated LLMs’ ability to operate as pattern machines in three domains:\nsequence transformation, sequence completion, and sequence improvement. In sequence\ntransformation, the research demonstrated that LLMs could generalize speciﬁc sequence\ntransformations using benchmarks such as ARC (abstraction and reasoning corpus) and\nPCFG (probabilistic context-free grammar), thereby proving their utility in spatial reasoning\ntasks for robotics. In sequence completion, the study examined whether LLMs could ﬁnish\npatterns in elementary functions (e.g., sinusoids), illustrating their utility in robotic tasks\nsuch as extending a wiping motion from kinesthetic demonstrations or creating drawings\non a whiteboard. Finally, in sequence improvement, the research revealed that by utilizing\nreward-labeled trajectories as context and incorporating online interaction, LLM-based\nagents could explore small grids and reﬁne simple trajectories using human-in-the-loop\nmethods, such as optimizing a CartPole controller.Appl. Sci.2024, 14, 8868 17 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 17 of 39 \n \nsequence transformation, the research demonstrated that LLMs could generalize speci ﬁc \nsequence transformations using benchmarks such as ARC (abstraction and reasoning \ncorpus) and PCFG (probabilistic context-free grammar), thereby proving their utility in \nspatial reasoning tasks for robotics. In sequence completion, the study examined whether \nLLMs could ﬁnish pa tterns in elementary functions (e.g., sinusoids), illustrating their \nutility in robotic tasks such as extending a wiping motion from kinesthetic demonstrations \nor creating drawings on a whiteboard. Finally, in sequence improvement, the research \nrevealed that by utilizing reward-labeled trajectories as context and incorporating online \ninteraction, LLM-based agents could explore small grids and re ﬁne simple trajectories \nusing human-in-the-loop methods, such as optimizing a CartPole controller.  \n \nFigure 6. Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed \nin sequence transformation, completion, and improvement [148]. \n4.3. High-Level Planning (Including Decision-Making and Reasoning) \nThe abstraction and generalization capabilities of LLMs oﬀer eﬀective methodologies \nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various \nresearch outcomes have been realized in the ﬁelds of planning, decision-making,"
  },
  {
    "question": "What is EmbodiedGPT and how does it relate to LLMs?",
    "chunk": "reward-labeled trajectories as context and incorporating online interaction, LLM-based\nagents could explore small grids and reﬁne simple trajectories using human-in-the-loop\nmethods, such as optimizing a CartPole controller.Appl. Sci.2024, 14, 8868 17 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 17 of 39 \n \nsequence transformation, the research demonstrated that LLMs could generalize speci ﬁc \nsequence transformations using benchmarks such as ARC (abstraction and reasoning \ncorpus) and PCFG (probabilistic context-free grammar), thereby proving their utility in \nspatial reasoning tasks for robotics. In sequence completion, the study examined whether \nLLMs could ﬁnish pa tterns in elementary functions (e.g., sinusoids), illustrating their \nutility in robotic tasks such as extending a wiping motion from kinesthetic demonstrations \nor creating drawings on a whiteboard. Finally, in sequence improvement, the research \nrevealed that by utilizing reward-labeled trajectories as context and incorporating online \ninteraction, LLM-based agents could explore small grids and re ﬁne simple trajectories \nusing human-in-the-loop methods, such as optimizing a CartPole controller.  \n \nFigure 6. Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed \nin sequence transformation, completion, and improvement [148]. \n4.3. High-Level Planning (Including Decision-Making and Reasoning) \nThe abstraction and generalization capabilities of LLMs oﬀer eﬀective methodologies \nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various \nresearch outcomes have been realized in the ﬁelds of planning, decision-making, \nreasoning, and behavior trees within robotics. \nYoneda [149] introduced Statler, a framew ork designed to provide LLMs with an \nexplicit world state representation through a continuously maintained ‘memory’. The core \nof Statler consisted of two components: the world model reader and the world model \nwriter. These components interacted with and sustained the world state. The world model \nreader interpreted user commands and generated executable code based on the current \nstate representation, while the world model wr iter updated the system’s state according \nto execution outcomes. By facilitating access to the world state ‘memory’, Statler improved \nLLMs’ ability to reason about planning task s with extended time horizons, overcoming \nlimitations imposed by context length. \nMu [150], shown in Figure 7, introduced EmbodiedGPT, a model speci ﬁcally \ndesigned for Embodied AI, which leverages LLMs. This framework processes visual \nobservations and natural language to establish long-term plans and execute tasks in real-\nFigure 6.Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed in\nsequence transformation, completion, and improvement [148].\n4.3. High-Level Planning (Including Decision-Making and Reasoning)\nThe abstraction and generalization capabilities of LLMs offer effective methodologies\nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various\nresearch outcomes have been realized in the ﬁelds of planning, decision-making, reasoning,\nand behavior trees within robotics.\nYoneda [149] introduced Statler, a framework designed to provide LLMs with an\nexplicit world state representation through a continuously maintained ‘memory’. The core\nof Statler consisted of two components: the world model reader and the world model\nwriter. These components interacted with and sustained the world state. The world model"
  },
  {
    "question": "What are the different domains included in the multi-task benchmark for robot manipulation?",
    "chunk": "learning objective. Experimental results demonstrated that the model effectively separated\naction and perception, achieving enhanced zero-shot and compositional generalization\nacross various manipulation tasks, speciﬁcally 16 tasks related to robot manipulation.\nHa [164] proposed a framework aimed at robot skill acquisition. This framework\nprovided a comprehensive solution by utilizing language guidance, without necessitating\nexpert demonstrations or reward speciﬁcation/engineering. It consisted of two main\ncomponents. The ﬁrst component, scaling up language-guided data generation, employed\nLLMs to break down tasks into subtasks and generate a hierarchical plan or task tree. This\nplan was materialized into various robot trajectories using 6-DoF exploration primitives.\nThese trajectories were subsequently veriﬁed and retries were performed as needed until\nsuccess was achieved. This approach enhanced the success rate of data collection and\nmore effectively mitigated the low-level understanding gap in LLMs by incorporating retry\nprocesses as part of the robot’s experiences. The second component, distilling down to\nlanguage-conditioned visuomotor policy, transformed robot experiences into a policy that\ndeduced control sequences from visual observations and natural language task descriptions.\nBy extending diffusion policies, this component handled language-based conditioning for\nmulti-task learning. To assess long-horizon behavior, commonsense reasoning, tool use,\nand intuitive physics, a new multi-task benchmark comprising 18 tasks related to robot\nmanipulation across ﬁve domains (mailbox, transport, drawer, catapult, and bus balance)\nwas developed. This benchmark effectively supported the learning of retry behaviors in\nthe data collection process and enhanced success rates.\nHuang [165], as shown in Figure12, aimed to synthesize dense robot trajectories,\nincluding 6-DoF end-effector waypoints, for various manipulation tasks using an open set\nof instructions and objects. Huang noted that LLMs were skilled at deriving affordances and\nconstraints from free-form language instructions. Further, by harnessing code generation\ncapabilities, Huang developed 3D value maps for the agent’s observation space through\ninteractions with VLMs. These 3D value maps were integrated into a model-based planning\nframework to generate closed-loop robot trajectories robust to dynamic perturbations in\na zero-shot approach. The proposed framework demonstrated efﬁcient learning of the\ndynamics model for scenes with contact-rich interactions and provided advantages in these\ncomplex scenarios.Appl. Sci.2024, 14, 8868 24 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 24 of 39 \n \nsuccess was achieved. This approach enhanced  t h e  s u c c e s s  r a t e  o f  d a t a  c o l l e c t i o n  a n d  \nmore e ﬀectively mitigated the low-level understanding gap in LLMs by incorporating \nretry processes as part of the robot’s experiences. The second component, distilling down \nto language-conditioned visuomotor policy, transformed robot experiences into a policy \nthat deduced control sequences from visual observations and natural language task \ndescriptions. By extending di ﬀusion policies, this component handled language-based \nconditioning for multi-task learning. To assess long-horizon behavior, commonsense \nreasoning, tool use, and intuitive physics, a new multi-task benchmark comprising 18 \ntasks related to robot manipulation across ﬁve domains (mailbox, transport, drawer, \ncatapult, and bus balance) was developed. This benchmark e ﬀectively supported the \nlearning of retry behaviors in the data collection process and enhanced success rates."
  },
  {
    "question": "How does Text2Motion compare to other planning methods?",
    "chunk": "SM operates without ﬁne-tuning by integrating diverse pre-trained models and functions \nin a zero-shot approach (e.g., using multimodal prompts), which enables it to harness new \nmultimodal capabilities. SM demonstrated state-of-the-art performance in zero-shot \nimage captioning and video-to-text retrieval, and it e ﬀectively answered free-form \nquestions about egocentric vi deo. Additionally, it supported interactions with external \nAPIs and databases (e.g., web search) for multimodal assistive dialogue, robot perception, \nand planning, among other novel applications. \n \nFigure 11. SM integrates various types of knowledge by using multiple pre-trained models and \nprovides meaningful results even in complex computer vision tasks such as image captioning, \ncontext inference, and activity prediction [158]. \nLin [159] introduced Text2Motion, a language-based framework designed to handle \nsequential manipulation tasks that requ ire long-horizon reasoning. Text2Motion \ninterpreted natural language instructions to formulate task plans and generated multiple \ncandidate skill sequences, evaluating the ge ometric feasibility of each sequence. By \nemploying a greedy search strategy, it selected  the optimal skill sequence to verify and \nexecute the ﬁnal plan. This method enabled Text2Motion to perform complex sequential \nmanipulation tasks with a higher success rate compared to existing language-based \nplanning methods, such as Saycan-gs an d Innermono-gs, and provided semantically \ngeneralized characteristics among skills with geometric relationships. \nFigure 11. SM integrates various types of knowledge by using multiple pre-trained models and\nprovides meaningful results even in complex computer vision tasks such as image captioning, context\ninference, and activity prediction [158].\nLin [159] introduced Text2Motion, a language-based framework designed to handle\nsequential manipulation tasks that require long-horizon reasoning. Text2Motion interpreted\nnatural language instructions to formulate task plans and generated multiple candidate\nskill sequences, evaluating the geometric feasibility of each sequence. By employing a\ngreedy search strategy, it selected the optimal skill sequence to verify and execute the ﬁnal\nplan. This method enabled Text2Motion to perform complex sequential manipulation tasks\nwith a higher success rate compared to existing language-based planning methods, such\nas Saycan-gs and Innermono-gs, and provided semantically generalized characteristics\namong skills with geometric relationships.\nWu [160] investigated personalization in-home cleaning robots that organize and tidy\nspaces, using an LLM to convert user-provided object placement locations into generalized\nrules. By using a camera to identify objects and CLIP to categorize them, TidyBot efﬁciently\nrelocated objects according to these rules. This method attained an impressive accuracy\nof 91.2% for unseen objects in a benchmark dataset, which encompassed a variety of\nobjects, receptacles, and example placements of both “seen” and “unseen” objects across\n96 scenarios. Additionally, it achieved an 85% success rate in removing objects during\nreal-world tests.\n4.4. Manipulation by LLMs\nIn robotics research, the manipulation domain, which includes robotic arms and\nend effectors, encompasses various areas that beneﬁt from foundation models such as\nLLMs for language-based interactions and VLMs for object handling. Among the studies\nintegrating manipulation with foundation models, Stone [161] introduced an approach\ncalled manipulation of open-world objects (MOO). This approach determined whether a\nrobot could follow instructions involving unseen object categories by linking pre-trained\nmodels to robotic policies. MOO utilized pre-trained vision-language models to derive\nobject information from language commands and images, guiding the robot’s actions based"
  },
  {
    "question": "What models does Instruct2Act use for object recognition and classification?",
    "chunk": "LLM. This process allows the LLM to assess the robot’s current state and capabilities, \nultimately generating an interpretable action plan. SayCan was evaluated across 101 robot \ntasks, achieving an 84% plan success rate and a 74% execution success rate in a simulated \nkitchen environment. In a real kitchen setting, the plan success rate decreased slightly to \n81% and the execution success rate fell to 60%, demonstrating that the policy and value \nfunctions generalize well to real-world settings. \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM\nto generate 3D affordance and constraint maps and design robot trajectories without additional\ntraining [165].\nAhn [166] introduced a framework named SayCan, which integrates LLMs with rein-\nforcement learning value functions, enabling robots to follow high-level text instructions.\nSayCan comprises two primary components: Say, which uses an LLM for task-based\ndecision-making, and Can, which evaluates the feasibility of these decisions via reinforce-\nment learning. Say leverages task-based knowledge from the LLM and reinforcement\nlearning functionality to assess the feasibility of task execution by robots in real-world\nscenarios. The LLM determines the actions necessary to achieve high-level goals and\nevaluates the effectiveness of each action in fulﬁlling the instructions. Learned through re-\ninforcement learning, the affordance function estimates each action’s success probability in\nthe current state, conﬁrming the executability of actions proposed by the LLM. This process\nallows the LLM to assess the robot’s current state and capabilities, ultimately generating\nan interpretable action plan. SayCan was evaluated across 101 robot tasks, achieving an\n84% plan success rate and a 74% execution success rate in a simulated kitchen environment.\nIn a real kitchen setting, the plan success rate decreased slightly to 81% and the execution\nsuccess rate fell to 60%, demonstrating that the policy and value functions generalize well\nto real-world settings.\nHuang [167] introduced the Instruct2Act framework, which employs LLMs to se-\nquentially map multi-modality instructions to robot actions. The previous method, CaP,\ngenerated robot policy program code directly from in-context examples based on language\ninstructions. However, this approach was constrained by the capabilities of the generated\ncode and encountered difﬁculties with longer, more complex commands due to the required\nhigh precision of code. To overcome these limitations, Instruct2Act introduced a novel\nstrategy that used multi-modality models and LLMs to simultaneously address recognition,\ntask planning, and low-level control modules. Instruct2Act utilized the segment anything\nmodel for identifying potential objects in input images for multi-modality recognition\nand the CLIP model for object classiﬁcation. As a result, Instruct2Act developed an inte-\ngrated search system capable of managing various input modalities and instruction types,\nincluding both pure language instructions and combined language-visual instructions,\nfacilitating the integration of diverse instruction types into a uniﬁed architecture. Moreover,\nfor pointer-language instructions, the framework supported task segmentation based on\nthe user’s clicks.\n4.5. Scene Understanding in LLMs and VLMs\nTo address the VQA problem, robotics research increasingly uses pre-trained VLMs\nto derive high-level information from visual data. This method is advantageous for scene\nunderstanding as it helps determine affordances that describe the relationship between the\ncurrent state and the next action based on images from cameras. Related studies focus on\naspects of scene understanding.Appl. Sci.2024, 14, 8868 25 of 39"
  },
  {
    "question": "What are low-level control commands in the context of task execution?",
    "chunk": "Appl. Sci. 2024, 14, x FOR PEER REVIEW 18 of 39 \n \ntime. EmbodiedGPT utilizes pre-trained visi on transformers and the LLaMA language \nmodel to encode visual features and map them  to the language modality. The generated \nplan was subsequently converted into speci ﬁc task commands using general visual \ntokens, encoded by the vision model. The framework’s functionality comprises (1) \nencoding current visual features, (2) mapping visual features to the language modality \nvia a ttention-based interactions between visual tokens and text queries or learnable \nembedded queries, (3) generating plans with the LLaMA language model and translating \nthem into speciﬁc task commands, and (4) querying the encoded visual tokens from the \nvision model and translating them into low-level control commands through a \ndownstream policy network for task execution. Experimental results, utilizing the MS-\nCOCO dataset, revealed that EmbodiedG PT excels in object recognition and \nunderstanding spatial relationships. Notabl y, implementing a closed-loop design and a \n“chain-of-thought” training mode signi ﬁcantly enhanced EmbodiedGPT’s performance. \nThese results demonstrate that EmbodiedGPT e ﬀectively handles various autonomous \ntasks, exhibiting superior capability in object recognition, understanding spatial \nrelationships, and generating logical, executable plans. \n \nFigure 7. After encoding visual features, they are mappe d using visual tokens and text queries. A \nplan is then created with the LLaMA model and turned into task commands. The visual tokens are \nqueried and converted into low-level control commands to perform the task [150]. \nChen [151] introduced the language-model-based commonsense reasoning (LMCR) \nframework to assist robots in comprehendin g incomplete natural language instructions. \nThis framework enabled robots to receive instructions in natural language from humans, \nobserve their surroundings, and employ a commonsense reasoning method to \nautonomously infer missing information. LMCR utilized a model of commonsense \nreasoning learned from web-based text materials, allowing robots to understand \nincomplete instructions and autonomously execute tasks. The framework comprised three \nmain functions: language understanding, commonsense reasoning, and action planning. \nIn language understanding, LMCR translated human natural language instructions into a \nform interpretable by robots, parsing them into verb frames to convert them into \nexecutable structures. During the common sense reasoning phase, the robot analyzed \nsurrounding objects and employed a language model trained on large-scale unstructured \ntext materials to ﬁll in the missing details from the instructions. This model identiﬁed the \nFigure 7.After encoding visual features, they are mapped using visual tokens and text queries. A\nplan is then created with the LLaMA model and turned into task commands. The visual tokens are\nqueried and converted into low-level control commands to perform the task [150].\nChen [151] introduced the language-model-based commonsense reasoning (LMCR)\nframework to assist robots in comprehending incomplete natural language instructions.\nThis framework enabled robots to receive instructions in natural language from humans, ob-\nserve their surroundings, and employ a commonsense reasoning method to autonomously\ninfer missing information. LMCR utilized a model of commonsense reasoning learned\nfrom web-based text materials, allowing robots to understand incomplete instructions and\nautonomously execute tasks. The framework comprised three main functions: language\nunderstanding, commonsense reasoning, and action planning. In language understanding,\nLMCR translated human natural language instructions into a form interpretable by robots,"
  },
  {
    "question": "What does the process of prefix tuning involve?",
    "chunk": "model focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM \nmodel inference. \n \nFigure 3. An overview of four strategies for parameter-eﬃcient ﬁne-tuning: (a) Adapter Tuning, \n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5]. \nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a \nlow-rank constraint on transformer layers to approximate the update matrices through \ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates \nthe parameter updates using low-rank dec omposition matrices. The primary bene ﬁt of \nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning, \nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory \nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning. \nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119]. \n3.3. Prompt Techniques for Increasing LLM Performance \nTo enhance the performance of LLMs, the most straightforward approach involves \ntraining with additional data via ﬁne-tuning techniques, which mirrors supervised learn-\ning in conventional machine learning. Another method for improving performance in-\nvolves the use of in-context learning, which capitalizes on prompts for zero-shot learning, \na capability ﬁrst observed in LLMs with the advent of GPT-3. The adaptation of these \nprompts for speciﬁc tasks is known as prompt engineering. Fundamentally, prompt engi-\nneering (or prompting) entails supplying inputs  to the model to perform a distinct task, \ndesigning the input format to encapsulate the task’s purpose and context, and generating \nthe desired output. The four components of pr ompt engineering can be analyzed as fol-\nlows: within the prompt, “ Instructions” delineate the speci ﬁc tasks or directives for the \nmodel and “Context” provides external or additional contextual information that can tune \nthe model. Furthermore, “Input data” refers to the type of input or questions seeking an-\nswers, and “ Output data” deﬁnes the output type or format within the prompt, thereby \noptimizing the LLM’s performance for particular tasks. Various methodologies for creat-\ning prompts have been introduced, as described below. \nFigure 3. An overview of four strategies for parameter-efﬁcient ﬁne-tuning: (a) Adapter Tuning,\n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5].\nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a\nlow-rank constraint on transformer layers to approximate the update matrices through\ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates\nthe parameter updates using low-rank decomposition matrices. The primary beneﬁt of\nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning,\nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory\nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning.\nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119].\n3.3. Prompt Techniques for Increasing LLM Performance"
  },
  {
    "question": "How do language models improve the autonomy of robots?",
    "chunk": "that is biased based on personal characteristics (such as race, nationality, religion, gender,\ndisability, and so forth). In addition, they can also be used to instruct robotic systems to\nengage in violent or illegal behaviors such as misstatements, sexual predation, etc. Notable\nexamples include discriminatory behaviors such as inadequate recognition of children\nor individuals with speciﬁc skin tones in human detection systems, and the exclusion\nof individuals with disabilities from task assignments. It is imperative to consider the\npotential social biases of LLM when integrating with robotic systems. Although this kind of\nconsideration was secondary in traditional robotic systems because of the limitation of their\nlanguage capability, it is a necessary consideration for LLMs to be able to generate human-\nlike language. To address this issue, previous studies have attempted to resolve it in various\nways, such as AutoRT’s constitutional rules [10], DrEureka’s safety instructions [134], and\nNeMo’s guardrails [230]. The guideline-based output control of LLMs can represent an\naccessible method to ensure safety.\nAs an extension of this point, safety issues can be identiﬁed when integrating LLMs\nand VLMs into robotic intelligence systems [231]. Typically, in robotic intelligence systems,\nLLM models generate high-level action plans in various forms, such as programming\ncodes and behavior trees based on natural language or vector prompts. At this point, a\nprompt attack has the potential to disrupt the inference of the LLMs, thereby threatening\nthe reliability and safety of the robotic system. Prompt injection is one of the prompt attacks,\nwhereby the inference of LLMs is subtly altered through speciﬁc inputs. Jailbreak, another\nprompt attack, bypasses safety rules and causes LLMs to generate abnormal behaviors to\nbe performed by the robotic system. Consequently, even minor disturbances in the input\nprompts have the potential to cause the entire robotic system to malfunction. To defend\nagainst this critical threat to the reliability and safety of robotic systems, various techniques\nhave been proposed, such as input validation, which ﬁlters the model’s input, and context\nlocking, which restricts access based on the history and content of the prompt. Furthermore,\nstrict guardrails that restrict harmful or unsafe outputs from models can be an alternative\nto improve the reliability of robotic systems. However, it is essential to recognize that the\nsecurity techniques may potentially lead to a decline in the performance of the robot system.\nConsequently, the trade-off between performance and safety must be carefully considered.\nSince the emergence of ChatGPT and Microsoft’s implementation of robot systems\nusing ChatGPT [2], artiﬁcial intelligence components have been applied more widely and\nintensively in robotics research. Despite existing challenges, it is expected that research\ninvolving foundation models to improve robot intelligence will persist across various\ndomains and methods, which will likely enhance the usability and market potential of\nrobot systems inﬂuenced by these advancements.\n6. Conclusions\nIn this paper, we have explored the potential impact and applicability of LLMs on\nrobotics research ﬁelds by summarizing studies that applied LLMs and VLMs to robots.\nFundamentally, LLMs can enhance the capability of robots in natural language processing\nto interact with humans and to improve the robots’ autonomy in various task scenarios.\nIn particular, the ability of LLMs to understand and generate natural language plays a\ncrucial role in enabling robots to comprehend and execute complex commands. This survey\nconﬁrmed that the scope of utilizing LLMs in robotics was not limited to simple natural\nlanguage processing but also extended to broader research areas. This study explored"
  },
  {
    "question": "How were the articles for the review selected from the databases?",
    "chunk": "on four key areas: pre-training, adaptation tuning, utilization, and capacity evaluation. Fur-\nthermore, it provided a summary of the resources currently available for the development\nof LLMs and discussed potential future directions for research in this ﬁeld. The survey [12]\nconducted a comprehensive and systematic review of VLMs for visual recognition tasks.\nIt addressed the evolution of the visual recognition paradigm, the principal architectures\nand datasets, and the fundamental principles of VLMs. Moreover, the paper provided\nan overview of the pre-training, transfer learning, and knowledge distillation methods\nemployed in the context of VLMs. The review [3] examined the potential for leveraging\nexisting natural language processing and computer vision foundation models in robotics.\nIn addition, it explored the possibility of developing a robot-speciﬁc foundation model. The\nreview [13] presented an analysis of recent studies on language-based approaches to robotic\nmanipulation. It comprised an analysis of learning paradigms integrated with foundation\nmodels related to manipulation tasks, including semantic information extraction, environ-\nment and evaluation, auxiliary tasks, task representation, safety issues, and other pertinent\nconsiderations. The survey paper [14] presented an analysis of recent research articles that\nemployed foundation models to address robotics challenges. It investigated the extent to\nwhich foundation models enhanced robot performance in perception, decision-making, and\ncontrol. In addition, it examined the obstacles impeding the implementation of foundation\nmodels in robot autonomy and proposed avenues for future advancements. The review\npaper [15] presented a comprehensive review of the research in the ﬁeld of vision and\nlanguage navigation (VLN), encompassing tasks, evaluation metrics, and methodologies\nrelated to autonomous navigation.Appl. Sci.2024, 14, 8868 5 of 39\nTable 1.Useful Review Papers.\nTitle Keywords Ref.\nToward General-Purpose Robots via Foundation\nModels: A Survey and Meta-Analysis Foundation Models [ 3]\nA Survey of Large Language Models LLM [ 5]\nVision-Language Models for Vision Tasks: A Survey VLM [ 12]\nLanguage-conditioned Learning for Robotic\nManipulation: A Survey LLM, VLM, Manipulation [ 13]\nFoundation Models in Robotics: Applications,\nChallenges, and the Future Foundation Models [ 14]\nVision-and-Language Navigation: A Survey of Tasks,\nMethods, and Future Directions VLN [ 15]\n2. Review Protocol\nThis survey covered four databases: Web of Science, ScienceDirect, IEEE Xplore, and\narXiv. In fact, many of the articles surveyed had not been peer-reviewed and published at\nthe time of our search because the subject matter was relatively recent. Therefore, a consid-\nerable number of articles reviewed in this survey were sourced from the arXiv database.\nThe selection process of this study primarily relied on two iterations:\n• The titles and abstracts of the articles were reviewed to eliminate duplicates and\nirrelevant articles.\n• The full texts of the selected articles from the ﬁrst iteration were thoroughly examined\nand categorized.\n• Article searching began on 18 September 2023.\nRegarding the search queries,\n• the publication years were those after 2020,\n• the keywords of Robotics and LLM, which were ((“Robotic” OR “Robotics”) AND\n(“LLM” OR “LM” OR “Large Language Model” OR “Language Model”)), and relevant\njournal and conference articles written in English were considered.\nFrom these search criteria, recent studies utilizing language models in robotics research\nwere expected to be collected. Our aim is to provide a robust understanding of how\nlanguage models and their variants have been utilized to enhance robot intelligence in\nthe literature."
  },
  {
    "question": "How might future research address ethical considerations in robotics?",
    "chunk": "prompts have the potential to cause the entire robotic system to malfunction. To defend\nagainst this critical threat to the reliability and safety of robotic systems, various techniques\nhave been proposed, such as input validation, which ﬁlters the model’s input, and context\nlocking, which restricts access based on the history and content of the prompt. Furthermore,\nstrict guardrails that restrict harmful or unsafe outputs from models can be an alternative\nto improve the reliability of robotic systems. However, it is essential to recognize that the\nsecurity techniques may potentially lead to a decline in the performance of the robot system.\nConsequently, the trade-off between performance and safety must be carefully considered.\nSince the emergence of ChatGPT and Microsoft’s implementation of robot systems\nusing ChatGPT [2], artiﬁcial intelligence components have been applied more widely and\nintensively in robotics research. Despite existing challenges, it is expected that research\ninvolving foundation models to improve robot intelligence will persist across various\ndomains and methods, which will likely enhance the usability and market potential of\nrobot systems inﬂuenced by these advancements.\n6. Conclusions\nIn this paper, we have explored the potential impact and applicability of LLMs on\nrobotics research ﬁelds by summarizing studies that applied LLMs and VLMs to robots.\nFundamentally, LLMs can enhance the capability of robots in natural language processing\nto interact with humans and to improve the robots’ autonomy in various task scenarios.\nIn particular, the ability of LLMs to understand and generate natural language plays a\ncrucial role in enabling robots to comprehend and execute complex commands. This survey\nconﬁrmed that the scope of utilizing LLMs in robotics was not limited to simple natural\nlanguage processing but also extended to broader research areas. This study explored\nextensive LLM applications in the robotics literature, such as planning, manipulation, and\nscene understanding, as well as reinforcement learning automation frameworks such as\nEureka, and included robot actions in language models such as AutoRT. Moreover, the\nresearch direction of current generative AI models is transitioning towards multimodalAppl. Sci.2024, 14, 8868 30 of 39\nlanguage models, moving beyond information acquisition and cognition aspects such as\ntext, images, and videos to include actuator actions within large models in therobotics ﬁeld.\nWhile the surveyed studies indicated that LLMs play a promising role in the future\nof robotics, certain limitations were also identiﬁed. First, the increased computational\nresources and energy consumption associated with embedding LLMs into robotic systems\nmust be addressed. Second, biases in language models and ethical considerations are\nsigniﬁcant issues that need to be tackled in robotics. Therefore, continual efforts will be\nnecessary in future research to resolve these challenges.\nOverall, LLMs are valuable tools that can signiﬁcantly advance robotics. This review\nhas revealed that innovative robot applications are possible through the integration of\nLLMs and VLMs. Moreover, these foundation models are expected to serve as critical\nelements for future robot research and practical applications in the real world.\nAuthor Contributions:Conceptualization, S.S. and C.K.; methodology, S.S.; formal analysis, H.J.,\nH.L. and S.S.; investigation, H.J., H.L. and S.S.; resources, H.J., H.L., S.S. and C.K.; writing—original\ndraft preparation, H.J., H.L., S.S. and C.K.; writing—review and editing, H.J., H.L., S.S. and C.K.; vi-"
  },
  {
    "question": "What are the components of the navigation system introduced by Shah?",
    "chunk": "grounding, 3D-assisted dialogue, and navigation. The model used a 3D feature extractor to\nalign 3D features from multi-view images with language features, facilitating more precise\ntext generation and question answering based on spatial understanding. To train 3D-LLM,\na pre-trained 2D VLM formed the backbone, enhanced by the addition of 3D positional\nembeddings to better capture 3D spatial information. The model generated location tokens\nthrough linguistic descriptions of speciﬁc objects and was trained using 3D features as input.\nExperimental results showed that 3D-LLM excelled in various 3D-related tasks, achieving\napproximately a 9% higher BLEU-1 score compared to previous models on the ScanQA\ndataset. It demonstrated superior performance in 3D captioning, task composition, andAppl. Sci.2024, 14, 8868 26 of 39\n3D-assisted dialogue, outperforming 2D VLMs and displaying an improved understanding\nof object locations, shapes, and interactions.\nIn the extension of scene understanding using VLMs, the keyword VLN (vision-\nand-language navigation) is widely used in navigation-related research, where language\nfoundation models are increasingly utilized.\nShah [173], as shown in Figure13, introduced a robotic navigation system named\nLM-Nav, which capitalized on the advantages of training with large, unlabeled trajectory\ndatasets while providing a high-level interface for users. LM-Nav utilized three large-scale\npre-trained models: ViNG, CLIP, and GPT-3. Initially, the LLM translated natural language\ninstructions into a sequence of textual landmarks. The VLM integrated these textual\nlandmarks with images to identify the relevant images through probabilistic distribution.\nSubsequently, the VNM utilized these landmarks to plan and execute robot trajectories\nwithin the environment. During this process, the robot utilized a graph search algorithm to\ndetermine optimal trajectories and to navigate along these paths in the real world. This\nmethod demonstrated LM-Nav’s ability to perform long-horizon navigation in complex\noutdoor environments using natural language instructions.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 27 of 39 \n \n \nFigure 13. LM-Nav uses three pre-trained models: ( a) VNM builds a topological graph from \nobservations, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, \n(d) A graph search algorithm then ﬁnds the best robot trajectory, and ( e) the robot executes the \nplanned path [173]. \nZhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow \ninstructions. NavGPT is a vision-language na vigation system that employs an LLM to \ntranslate visual inputs from a visual foundation model (VFM) into natural language. The \nLLM then interprets the current state and makes informed decisions to reach the intended \ngoal, based on these converted visuals, naviga tion history, and potential future routes. \nNavGPT conducts various functions, incl uding high-level planning, decomposing \ninstructions into sub-goals, identifying landmarks in observed scenes, monitoring \nnavigation progress, and modifying plans as necessary. Although NavGPT’s performance \non zero-shot tasks from the R2R dataset has not yet matched that of trained models, it \nunderscored the potential of utilizing multi-modality inputs with LLMs for visual \nnavigation and tapping into the explicit reasoning capabilities of LLMs to enhance learned \nmodels. \nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-"
  },
  {
    "question": "What are the limitations of the CaP method compared to Instruct2Act?",
    "chunk": "In a real kitchen setting, the plan success rate decreased slightly to 81% and the execution\nsuccess rate fell to 60%, demonstrating that the policy and value functions generalize well\nto real-world settings.\nHuang [167] introduced the Instruct2Act framework, which employs LLMs to se-\nquentially map multi-modality instructions to robot actions. The previous method, CaP,\ngenerated robot policy program code directly from in-context examples based on language\ninstructions. However, this approach was constrained by the capabilities of the generated\ncode and encountered difﬁculties with longer, more complex commands due to the required\nhigh precision of code. To overcome these limitations, Instruct2Act introduced a novel\nstrategy that used multi-modality models and LLMs to simultaneously address recognition,\ntask planning, and low-level control modules. Instruct2Act utilized the segment anything\nmodel for identifying potential objects in input images for multi-modality recognition\nand the CLIP model for object classiﬁcation. As a result, Instruct2Act developed an inte-\ngrated search system capable of managing various input modalities and instruction types,\nincluding both pure language instructions and combined language-visual instructions,\nfacilitating the integration of diverse instruction types into a uniﬁed architecture. Moreover,\nfor pointer-language instructions, the framework supported task segmentation based on\nthe user’s clicks.\n4.5. Scene Understanding in LLMs and VLMs\nTo address the VQA problem, robotics research increasingly uses pre-trained VLMs\nto derive high-level information from visual data. This method is advantageous for scene\nunderstanding as it helps determine affordances that describe the relationship between the\ncurrent state and the next action based on images from cameras. Related studies focus on\naspects of scene understanding.Appl. Sci.2024, 14, 8868 25 of 39\nChen [168] explored methods to integrate commonsense into scene understanding\nusing LLMs and introduced three paradigms for classifying room types within indoor\nenvironments based on included objects. The zero-shot approach utilized a pre-trained\nlanguage model to identify the objects in a room and estimate their types. The feed-\nforward classiﬁer approach involved inputting sentences that listed a room’s objects into\nthe language model to generate embedding vectors, which were subsequently input into a\npre-trained shallow multilayer perceptron to predict each room type. Lastly, the classiﬁer\napproach embedded images of rooms alongside textual descriptions to identify the best-\nmatching description, thereby determining the room type. These paradigms demonstrated\nthe capacity to generalize to objects not presented in the training set and to make inferences\nwithin a space larger than that deﬁned by the trained object labels.\nYang [169] introduced the innovative zero-shot, open-vocabulary, LLM-based 3D\nvisual grounding pipeline called LLM-Grounder. This method breaks down complex natu-\nral language queries into semantic components and uses visual grounding tools such as\nOpenScene or LERF to locate objects within 3D scenes. Subsequently, the LLM evaluates\nspatial and commonsense relationships among these objects to achieve the ﬁnal grounding.\nRemarkably, LLM-Grounder operates without labeled training data and has proven its ca-\npacity to adapt to new 3D scenes and diverse text queries, enhancing grounding capabilities\nfor complex language queries and establishing itself as an effective solution.\nChen [170] developed NLMap, an open-vocabulary, queryable scene representation\nsystem. Designed to accumulate and incorporate contextual data within a scene repre-\nsentation for natural language queries, this system allows an LLM planner to visualize"
  },
  {
    "question": "In what ways can LLMs enhance human-robot interactions?",
    "chunk": "of labeled data. This process is inherently resource-intensive. Moreover, these models\nare designed for a speciﬁc environment and require reconﬁguration whenever the task or\nenvironment changes. This renders the robots challenging to adapt and scale to disparate\nenvironments [3]. For practical robot systems, it is essential that they are able to ﬂexibly\nrespond to the ever-changing physical environment. From this perspective, the gener-\nalization of affordable tasks, environmental adaptability, and the accuracy of execution,\nplanning, and reasoning capabilities remain signiﬁcant challenges for traditional robotic\nintelligence systems [4].\nHowever, LLMs and VLMs help a robot intelligence system to enhance its general-\nization capability in dynamic and complex real-world environments. LLMs can leverage\npre-trained knowledge from extensive datasets to augment their ability to generalize to\neveryday tasks that are typically expected of robots. Unlike the conventional supervised\nmodels, LLMs can utilize zero-shot and few-shot learning to help robots quickly adapt\nto new environments without additional training [5]. This has the advantage of signiﬁ-\ncantly reducing the need for costly data collection and labeling. In addition, robot systems\nequipped with LLMs can process complex instructions based on their ability to understand\nand generate natural language, which can improve human–robot interactions. Furthermore,\nLLMs can be integrated with multimodal sensors such as LiDAR, depth, voice, tactile, pro-\nprioception, and visual information, which enables robots to comprehensively understand\nand adapt to their environment [6].\nLLMs have demonstrated exceptional capabilities in processing and understanding\ntext-based information, signiﬁcantly enhancing robotic communication abilities. For in-\nstance, robots can accurately comprehend and execute natural language commands via\nLLMs, providing scalability and ﬂexibility beyond traditional word-based robotic com-\nmand systems. Consequently, robots can respond more adaptably and intelligently in\ninteractions with human users, allowing them to engage in complex problem-solving and\ndecision-making processes beyond simple mechanical tasks.\nAdditionally, LLMs not only enhance a robot’s communication skills to improve HRI\nusability but also boost the robot’s planning abilities. Planning involves setting goals\nand devising a sequence of actions to achieve them, which are essential in determining a\nrobot’s autonomy and efﬁciency. LLMs interpret natural language from users and complex\ncommands, enabling robots to establish and execute suitable plans in various situations.\nMoreover, LLMs adapt ﬂexibly to new situations through a zero-shot approach and utilize\npast data for learning. These capabilities indicate that robots can play a vital role in\nautonomously navigating changing environments and resolving unexpected issues.\nMoreover, VLMs such as CLIP [7], which are trained to solve vision question answering\n(VQA) tasks, have the ability to process visual and linguistic information simultaneously.\nThis ability allows robots to visually perceive their surroundings and integrate this infor-\nmation into linguistic descriptions, enabling more sophisticated situational awareness. For\ninstance, using VLMs, a robot can recognize objects and provide descriptions, as well as\nunderstand and execute user commands based on visual cues. This integrated approach\nsigniﬁcantly enhances a robot’s autonomy and interaction capabilities.\nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which en-\nable low-level actuator control using LLMs and VLMs, Google has introduced AutoRT [10].\nAutoRT is a system where robots interact with real-world objects to collect motion data. ItAppl. Sci.2024, 14, 8868 3 of 39"
  },
  {
    "question": "How does the integration of programming language features benefit robot planning?",
    "chunk": "robots to carry out instructions: (a) mobile manipulation and (b,c) tabletop manipulation, in both\nsimulated and real-world environments [153].Appl. Sci.2024, 14, 8868 20 of 39\nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn,\na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot\nbehavior trees (BTs) from textual descriptions. The developed model was compact enough\nto operate on a robot’s onboard microcomputer, while adept at constructing complex robot\nbehaviors. It provided structurally and logically correct BTs and demonstrated the ability\nto handle instructions that were not included in the training set.\nSong [155], as shown in Figure9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions\nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via\na low-level planner. It continuously updated environmental information as new objects\nwere detected during action implementation and revisited the LLM to adjust the plan if\nsubgoals failed or were delayed based on updated observations. This iterative process was\nrepeated until the subgoal was achieved, after which the system moved to the next goal.\nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated\ncompetitive performance with signiﬁcantly reduced training data and proved its ability to\ngeneralize in various tasks (e.g., ALFRED) with minimal examples.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 9.LLM-Planner is a system that creates high-level plans based on natural language commands,\nsets subgoals to determine actions, and continuously updates the plan to reﬂect environmental\nchanges [155].\nSingh [156], as shown in Figure10, introduced ProgPrompt, a programmatic LLM\nprompt structure designed for generating plans across diverse situated environments,\nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that\nleveraged LLMs and included a Python programming structure to facilitate information\nabout the environment and executable actions. It featured a feedback mechanism, using\nexecutable program plan examples and assertion statements to mitigate errors, enhancing"
  },
  {
    "question": "What advantages does the attention-based interaction provide in EmbodiedGPT?",
    "chunk": "Yoneda [149] introduced Statler, a framew ork designed to provide LLMs with an \nexplicit world state representation through a continuously maintained ‘memory’. The core \nof Statler consisted of two components: the world model reader and the world model \nwriter. These components interacted with and sustained the world state. The world model \nreader interpreted user commands and generated executable code based on the current \nstate representation, while the world model wr iter updated the system’s state according \nto execution outcomes. By facilitating access to the world state ‘memory’, Statler improved \nLLMs’ ability to reason about planning task s with extended time horizons, overcoming \nlimitations imposed by context length. \nMu [150], shown in Figure 7, introduced EmbodiedGPT, a model speci ﬁcally \ndesigned for Embodied AI, which leverages LLMs. This framework processes visual \nobservations and natural language to establish long-term plans and execute tasks in real-\nFigure 6.Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed in\nsequence transformation, completion, and improvement [148].\n4.3. High-Level Planning (Including Decision-Making and Reasoning)\nThe abstraction and generalization capabilities of LLMs offer effective methodologies\nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various\nresearch outcomes have been realized in the ﬁelds of planning, decision-making, reasoning,\nand behavior trees within robotics.\nYoneda [149] introduced Statler, a framework designed to provide LLMs with an\nexplicit world state representation through a continuously maintained ‘memory’. The core\nof Statler consisted of two components: the world model reader and the world model\nwriter. These components interacted with and sustained the world state. The world model\nreader interpreted user commands and generated executable code based on the current\nstate representation, while the world model writer updated the system’s state according to\nexecution outcomes. By facilitating access to the world state ‘memory’, Statler improved\nLLMs’ ability to reason about planning tasks with extended time horizons, overcoming\nlimitations imposed by context length.\nMu [150], shown in Figure7, introduced EmbodiedGPT, a model speciﬁcally designed\nfor Embodied AI, which leverages LLMs. This framework processes visual observations\nand natural language to establish long-term plans and execute tasks in real-time. Em-\nbodiedGPT utilizes pre-trained vision transformers and the LLaMA language model to\nencode visual features and map them to the language modality. The generated plan was\nsubsequently converted into speciﬁc task commands using general visual tokens, encoded\nby the vision model. The framework’s functionality comprises (1) encoding current visual\nfeatures, (2) mapping visual features to the language modality via attention-based interac-\ntions between visual tokens and text queries or learnable embedded queries, (3) generating\nplans with the LLaMA language model and translating them into speciﬁc task commands,\nand (4) querying the encoded visual tokens from the vision model and translating them\ninto low-level control commands through a downstream policy network for task execu-\ntion. Experimental results, utilizing the MS-COCO dataset, revealed that EmbodiedGPT\nexcels in object recognition and understanding spatial relationships. Notably, implement-Appl. Sci.2024, 14, 8868 18 of 39\ning a closed-loop design and a “chain-of-thought” training mode signiﬁcantly enhanced\nEmbodiedGPT’s performance. These results demonstrate that EmbodiedGPT effectively\nhandles various autonomous tasks, exhibiting superior capability in object recognition,"
  },
  {
    "question": "What improvements does 3D-LLM show over traditional models?",
    "chunk": "grounding, 3D-assisted dialogue, and navigation. The model used a 3D feature extractor to\nalign 3D features from multi-view images with language features, facilitating more precise\ntext generation and question answering based on spatial understanding. To train 3D-LLM,\na pre-trained 2D VLM formed the backbone, enhanced by the addition of 3D positional\nembeddings to better capture 3D spatial information. The model generated location tokens\nthrough linguistic descriptions of speciﬁc objects and was trained using 3D features as input.\nExperimental results showed that 3D-LLM excelled in various 3D-related tasks, achieving\napproximately a 9% higher BLEU-1 score compared to previous models on the ScanQA\ndataset. It demonstrated superior performance in 3D captioning, task composition, andAppl. Sci.2024, 14, 8868 26 of 39\n3D-assisted dialogue, outperforming 2D VLMs and displaying an improved understanding\nof object locations, shapes, and interactions.\nIn the extension of scene understanding using VLMs, the keyword VLN (vision-\nand-language navigation) is widely used in navigation-related research, where language\nfoundation models are increasingly utilized.\nShah [173], as shown in Figure13, introduced a robotic navigation system named\nLM-Nav, which capitalized on the advantages of training with large, unlabeled trajectory\ndatasets while providing a high-level interface for users. LM-Nav utilized three large-scale\npre-trained models: ViNG, CLIP, and GPT-3. Initially, the LLM translated natural language\ninstructions into a sequence of textual landmarks. The VLM integrated these textual\nlandmarks with images to identify the relevant images through probabilistic distribution.\nSubsequently, the VNM utilized these landmarks to plan and execute robot trajectories\nwithin the environment. During this process, the robot utilized a graph search algorithm to\ndetermine optimal trajectories and to navigate along these paths in the real world. This\nmethod demonstrated LM-Nav’s ability to perform long-horizon navigation in complex\noutdoor environments using natural language instructions.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 27 of 39 \n \n \nFigure 13. LM-Nav uses three pre-trained models: ( a) VNM builds a topological graph from \nobservations, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, \n(d) A graph search algorithm then ﬁnds the best robot trajectory, and ( e) the robot executes the \nplanned path [173]. \nZhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow \ninstructions. NavGPT is a vision-language na vigation system that employs an LLM to \ntranslate visual inputs from a visual foundation model (VFM) into natural language. The \nLLM then interprets the current state and makes informed decisions to reach the intended \ngoal, based on these converted visuals, naviga tion history, and potential future routes. \nNavGPT conducts various functions, incl uding high-level planning, decomposing \ninstructions into sub-goals, identifying landmarks in observed scenes, monitoring \nnavigation progress, and modifying plans as necessary. Although NavGPT’s performance \non zero-shot tasks from the R2R dataset has not yet matched that of trained models, it \nunderscored the potential of utilizing multi-modality inputs with LLMs for visual \nnavigation and tapping into the explicit reasoning capabilities of LLMs to enhance learned \nmodels. \nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-"
  },
  {
    "question": "What is the importance of high-level planning in robot intelligence?",
    "chunk": "signiﬁcant lack of adaptability to dynamically changing environments. However, LLMs help a robot\nintelligence system to improve its generalization ability in dynamic and complex real-world environ-\nments. Indeed, ﬁndings from ongoing robotics studies indicate that LLMs can signiﬁcantly improve\nrobots’ behavior planning and execution capabilities. Additionally, vision-language models (VLMs),\ntrained on extensive visual and linguistic data for the vision question answering (VQA) problem,\nexcel at integrating computer vision with natural language processing. VLMs can comprehend visual\ncontexts and execute actions through natural language. They also provide descriptions of scenes\nin natural language. Several studies have explored the enhancement of robot intelligence using\nmultimodal data, including object recognition and description by VLMs, along with the execution\nof language-driven commands integrated with visual information. This review paper thoroughly\ninvestigates how foundation models such as LLMs and VLMs have been employed to boost robot\nintelligence. For clarity, the research areas are categorized into ﬁve topics: reward design in rein-\nforcement learning, low-level control, high-level planning, manipulation, and scene understanding.\nThis review also summarizes studies that show how foundation models, such as the Eureka model\nfor automating reward function design in reinforcement learning, RT-2 for integrating visual data,\nlanguage, and robot actions in vision-language-action models, and AutoRT for generating feasible\ntasks and executing robot behavior policies via LLMs, have improved robot intelligence.\nKeywords: embodied intelligence; foundation model; large language model (LLM); vision-language\nmodel (VLM); vision-language-action (VLA) model; robotics\n1. Introduction\nTo enhance the intelligence of robots in real-world environments that interact with\nhumans, developing robots capable of perceiving, acting, and interacting like humans is\na crucial goal. The recent advancements in large language models (LLMs) such as GPT-\n4o [1] have signiﬁcantly altered the ﬁeld of robotic AI research. These LLMs, trained on\nvast amounts of textual data, have shown excellent performance in enabling robots to\ncommunicate with humans more naturally and efﬁciently. Moreover, beyond the impacts\non human–robot interaction (HRI), there is ongoing research aimed at surpassing the\nlimitations of traditional low-level robot control techniques and planning algorithms by\nutilizing the high-level situational awareness and knowledge-based planning capabilities\nof LLMs. Notably, the programming capabilities of ChatGPT in the research presented by\nMicrosoft’s ChatGPT for Robotics [2] have introduced a new paradigm for applying LLMs\nin the robotics ﬁeld.\nThe goal of robot intelligence is to enable robots to operate autonomously in complex\nenvironments, interact naturally with humans, and make high-level decisions. To promote\nAppl. Sci.2024, 14, 8868.https://doi.org/10.3390/app14198868 https://www.mdpi.com/journal/applsciAppl. Sci.2024, 14, 8868 2 of 39\nadvancements in robot intelligence, the adoption of foundation models, such as LLMs and\nvision-language models (VLMs), which boast large parameter scales and pre-training on\nmassive datasets, is accelerating. These foundation models can perform various tasks, such\nas complex language understanding and generation and visual perception, enabling robots\nto engage with their environment in a more human-like manner.\nWhile traditional robot intelligence systems are highly effective in structured and pre-\ndictable environments, they are signiﬁcantly limited in their ability to adapt to dynamically\nchanging and complex real-world scenarios. In general, the intelligence models used in"
  },
  {
    "question": "What current limitations exist in the application of language models to robotics?",
    "chunk": "understand the situation and an LLM to propose possible tasks. By inputting the robot’s\noperational guidelines and safety constraints into the LLM as prompts, AutoRT assesses\nthe validity of the proposed tasks and the necessity for human intervention. Throughout\nthis process, AutoRT safely selects and executes feasible tasks while collecting relevant\ndata.\nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for\nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical causal-\nity in the real world, problem-solving through trial-and-error feedback, and code generation\nabilities. Eureka can autonomously generate reward functions for a variety of tasks and\nrobots without needing speciﬁc templates for each. This allows for the generation of\nhuman-level reward functions for diverse robots and tasks without human input. Further-\nmore, Eureka has demonstrated the ability to solve complex problems that were previously\nunsolved by expert-designed reward functions.\nGiven these research outcomes, integrating language models into robotic intelligence\npresents signiﬁcant potential to enhance robot capabilities and applications dramatically,\nthereby redeﬁning their roles in diverse industries and everyday life. Therefore, this survey\npaper explores recent research trends in LLM- and VLM-based robot intelligence, aiming to\nprovide a comprehensive understanding of future development possibilities by examining\nthe application of language models in various robotic research ﬁelds. It also seeks to\nhighlight research cases, identify current limitations, and suggest future research directions.\nTo chronicle this advancement in robotics research ﬁelds, this review paper presents\nthe following contributions:\n• This paper summarizes and introduces the foundational elements and tuning methods\nof LLM architecture.\n• It explores and arranges prompt techniques to enhance the problem-solving abilities\nof LLMs.\n• It reviews and encapsulates how LLMs and VLMs have been employed to augment\nrobot intelligence across ﬁve topics as shown in Figure1: (1) reward design for\nreinforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation,\nand (5) scene understanding.\nAppl. Sci. 2024 , 14 , x FOR PEER REVIEW 3 of 39 \n \nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which \nenable low-level actuator control using LLM s and VLMs, Google has introduced AutoRT \n[10]. AutoRT is a system where robots interact  with real-world objects to collect motion \ndata. It begins by exploring th e surrounding space to identify feasible tasks, then uses a \nVLM to understand the situation and an LLM to propose possible tasks. By inpu tting the \nrobot’s operational guidelines and safety co nstraints into the LLM as prompts, AutoRT \nassesses the validity of the proposed tasks and the necessity for human intervention. \nThroughout this process, AutoRT safely selects and executes feasible tasks while collecting \nrelevant data. \nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for \nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical cau-\nsality in the real world, problem-solving through trial-and-error feedback, and code gen-\neration abilities. Eureka can autonomously generate reward functions for a variety of tasks \nand robots without needing speci ﬁc templates for each. This allows for the generation of \nhuman-level reward functions for diverse robo ts and tasks without human input. Further-"
  },
  {
    "question": "What role does the segment anything model play in Instruct2Act?",
    "chunk": "In a real kitchen setting, the plan success rate decreased slightly to 81% and the execution\nsuccess rate fell to 60%, demonstrating that the policy and value functions generalize well\nto real-world settings.\nHuang [167] introduced the Instruct2Act framework, which employs LLMs to se-\nquentially map multi-modality instructions to robot actions. The previous method, CaP,\ngenerated robot policy program code directly from in-context examples based on language\ninstructions. However, this approach was constrained by the capabilities of the generated\ncode and encountered difﬁculties with longer, more complex commands due to the required\nhigh precision of code. To overcome these limitations, Instruct2Act introduced a novel\nstrategy that used multi-modality models and LLMs to simultaneously address recognition,\ntask planning, and low-level control modules. Instruct2Act utilized the segment anything\nmodel for identifying potential objects in input images for multi-modality recognition\nand the CLIP model for object classiﬁcation. As a result, Instruct2Act developed an inte-\ngrated search system capable of managing various input modalities and instruction types,\nincluding both pure language instructions and combined language-visual instructions,\nfacilitating the integration of diverse instruction types into a uniﬁed architecture. Moreover,\nfor pointer-language instructions, the framework supported task segmentation based on\nthe user’s clicks.\n4.5. Scene Understanding in LLMs and VLMs\nTo address the VQA problem, robotics research increasingly uses pre-trained VLMs\nto derive high-level information from visual data. This method is advantageous for scene\nunderstanding as it helps determine affordances that describe the relationship between the\ncurrent state and the next action based on images from cameras. Related studies focus on\naspects of scene understanding.Appl. Sci.2024, 14, 8868 25 of 39\nChen [168] explored methods to integrate commonsense into scene understanding\nusing LLMs and introduced three paradigms for classifying room types within indoor\nenvironments based on included objects. The zero-shot approach utilized a pre-trained\nlanguage model to identify the objects in a room and estimate their types. The feed-\nforward classiﬁer approach involved inputting sentences that listed a room’s objects into\nthe language model to generate embedding vectors, which were subsequently input into a\npre-trained shallow multilayer perceptron to predict each room type. Lastly, the classiﬁer\napproach embedded images of rooms alongside textual descriptions to identify the best-\nmatching description, thereby determining the room type. These paradigms demonstrated\nthe capacity to generalize to objects not presented in the training set and to make inferences\nwithin a space larger than that deﬁned by the trained object labels.\nYang [169] introduced the innovative zero-shot, open-vocabulary, LLM-based 3D\nvisual grounding pipeline called LLM-Grounder. This method breaks down complex natu-\nral language queries into semantic components and uses visual grounding tools such as\nOpenScene or LERF to locate objects within 3D scenes. Subsequently, the LLM evaluates\nspatial and commonsense relationships among these objects to achieve the ﬁnal grounding.\nRemarkably, LLM-Grounder operates without labeled training data and has proven its ca-\npacity to adapt to new 3D scenes and diverse text queries, enhancing grounding capabilities\nfor complex language queries and establishing itself as an effective solution.\nChen [170] developed NLMap, an open-vocabulary, queryable scene representation\nsystem. Designed to accumulate and incorporate contextual data within a scene repre-\nsentation for natural language queries, this system allows an LLM planner to visualize"
  },
  {
    "question": "How does LLM-Planner process natural language for robotics?",
    "chunk": "Figure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable\nrobots to carry out instructions: (a) mobile manipulation and (b,c) tabletop manipulation, in both\nsimulated and real-world environments [153].Appl. Sci.2024, 14, 8868 20 of 39\nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn,\na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot\nbehavior trees (BTs) from textual descriptions. The developed model was compact enough\nto operate on a robot’s onboard microcomputer, while adept at constructing complex robot\nbehaviors. It provided structurally and logically correct BTs and demonstrated the ability\nto handle instructions that were not included in the training set.\nSong [155], as shown in Figure9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions\nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via\na low-level planner. It continuously updated environmental information as new objects\nwere detected during action implementation and revisited the LLM to adjust the plan if\nsubgoals failed or were delayed based on updated observations. This iterative process was\nrepeated until the subgoal was achieved, after which the system moved to the next goal.\nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated\ncompetitive performance with signiﬁcantly reduced training data and proved its ability to\ngeneralize in various tasks (e.g., ALFRED) with minimal examples.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39"
  },
  {
    "question": "Why might smaller models be preferred for specific robotic applications?",
    "chunk": "While foundation models offer considerable potential for advancing robotics intelli-\ngence, several limitations and future considerations remain. These include (1) the speed\nof inference required for real-time applications, (2) the computational efﬁciency necessary\nfor embedded systems, (3) the ability to handle multi-modality information, and (4) the\nnecessity of addressing safety and ethical considerations.\nFirst of all, LLMs and VLMs hold considerable potential for enhancing robot intelli-\ngence. Nonetheless, several critical issues remain to be addressed. Foundation models,\ncharacterized as large-scale models pre-trained on extensive datasets, face challenges re-\nlated to real-time requirements and limited computational resources in robotic applications.\nMoreover, concerns such as personal information protection, privacy, and security from\nexternal attacks need resolution to enable cloud-based LLMs for robotics.\nSecondly, to enhance the computational efﬁciency and usability of LMs, there is\nongoing research into small language models (SLMs). Despite having fewer parameters,\nSLMs can achieve performance comparable to LLMs in speciﬁc applications. Several\nSLMs have been introduced, including DistilBERT [224], which is a compact version of\nGoogle’s BERT; Phi-3 [61], another SLM; Florence-2 [84], a small VLM model from Microsoft;\nMobileBERT [225], which is optimized for mobile platforms; and compact open-source\nversions of OpenAI’s GPT models such as GPT-Neo [226] and GPT-J [227]. Generally, SLMs\nare streamlined models with fewer parameters compared to LLMs, which can number in the\nbillions. SLMs utilize smaller, domain-speciﬁc datasets and require shorter training periods,\ntypically just a few weeks, unlike LLMs, which demand vast datasets for broad learning\nand multiple months of training. Developing SLMs to excel within speciﬁc domains for\nrobotic systems and ensuring real-time performance with minimal computational resources\nare essential research directions for advancing robot intelligence with SLMs.\nThe third implication is that LLMs, based on text-centered natural language process-\ning, are limited as single-modality models when applied to real-world robotic systems\nwhere information often blends in diverse ways. Research on LLMs is transitioning from\nsingle-modality to multimodality models, as evidenced by VLMs and OpenAI Sora [228],\nwith increasing demand for such models. Currently, to address the limitations of LLMs’\nsingle-modality, robotic systems are being developed with multimodality models that\nintegrate vision, such as VLMs. However, relying solely on text and images falls short\nof the diverse information range required in the real world, including images, sounds,\nvideos, and proprioceptive sensory information (such as the position, orientation, balance,\nmovement degree, and direction of various parts of the robot). Proprioceptive sensory\ninformation related to actions and movements is particularly vital for enhancing dynamic\nhuman interaction, information processing, and manipulation and planning skills based on\ndynamic movements. For instance, the integrated VLA model, which facilitates low-level\ncontrol based on LLMs and VLMs as shown by Google’s RT-2 model, highlights the neces-\nsity for models capable of integrating information from a broader range of modalities to\nenhance robot intelligence.Appl. Sci.2024, 14, 8868 29 of 39\nFinally, the fourth area to consider is how to address safety and ethical issues when\nLLM is applied to robotic intelligence systems. Studies were conducted to address the\nissue of discriminatory and unsafe behaviors that may be generated by robot applications"
  },
  {
    "question": "How do robots use LLMs to interpret complex user commands?",
    "chunk": "mand systems. Consequently, robots can respond more adaptably and intelligently in\ninteractions with human users, allowing them to engage in complex problem-solving and\ndecision-making processes beyond simple mechanical tasks.\nAdditionally, LLMs not only enhance a robot’s communication skills to improve HRI\nusability but also boost the robot’s planning abilities. Planning involves setting goals\nand devising a sequence of actions to achieve them, which are essential in determining a\nrobot’s autonomy and efﬁciency. LLMs interpret natural language from users and complex\ncommands, enabling robots to establish and execute suitable plans in various situations.\nMoreover, LLMs adapt ﬂexibly to new situations through a zero-shot approach and utilize\npast data for learning. These capabilities indicate that robots can play a vital role in\nautonomously navigating changing environments and resolving unexpected issues.\nMoreover, VLMs such as CLIP [7], which are trained to solve vision question answering\n(VQA) tasks, have the ability to process visual and linguistic information simultaneously.\nThis ability allows robots to visually perceive their surroundings and integrate this infor-\nmation into linguistic descriptions, enabling more sophisticated situational awareness. For\ninstance, using VLMs, a robot can recognize objects and provide descriptions, as well as\nunderstand and execute user commands based on visual cues. This integrated approach\nsigniﬁcantly enhances a robot’s autonomy and interaction capabilities.\nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which en-\nable low-level actuator control using LLMs and VLMs, Google has introduced AutoRT [10].\nAutoRT is a system where robots interact with real-world objects to collect motion data. ItAppl. Sci.2024, 14, 8868 3 of 39\nbegins by exploring the surrounding space to identify feasible tasks, then uses a VLM to\nunderstand the situation and an LLM to propose possible tasks. By inputting the robot’s\noperational guidelines and safety constraints into the LLM as prompts, AutoRT assesses\nthe validity of the proposed tasks and the necessity for human intervention. Throughout\nthis process, AutoRT safely selects and executes feasible tasks while collecting relevant\ndata.\nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for\nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical causal-\nity in the real world, problem-solving through trial-and-error feedback, and code generation\nabilities. Eureka can autonomously generate reward functions for a variety of tasks and\nrobots without needing speciﬁc templates for each. This allows for the generation of\nhuman-level reward functions for diverse robots and tasks without human input. Further-\nmore, Eureka has demonstrated the ability to solve complex problems that were previously\nunsolved by expert-designed reward functions.\nGiven these research outcomes, integrating language models into robotic intelligence\npresents signiﬁcant potential to enhance robot capabilities and applications dramatically,\nthereby redeﬁning their roles in diverse industries and everyday life. Therefore, this survey\npaper explores recent research trends in LLM- and VLM-based robot intelligence, aiming to\nprovide a comprehensive understanding of future development possibilities by examining\nthe application of language models in various robotic research ﬁelds. It also seeks to\nhighlight research cases, identify current limitations, and suggest future research directions.\nTo chronicle this advancement in robotics research ﬁelds, this review paper presents\nthe following contributions:\n• This paper summarizes and introduces the foundational elements and tuning methods\nof LLM architecture.\n• It explores and arranges prompt techniques to enhance the problem-solving abilities\nof LLMs."
  },
  {
    "question": "Can you explain the role of natural language commands in LLM-Planner?",
    "chunk": "Figure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 9.LLM-Planner is a system that creates high-level plans based on natural language commands,\nsets subgoals to determine actions, and continuously updates the plan to reﬂect environmental\nchanges [155].\nSingh [156], as shown in Figure10, introduced ProgPrompt, a programmatic LLM\nprompt structure designed for generating plans across diverse situated environments,\nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that\nleveraged LLMs and included a Python programming structure to facilitate information\nabout the environment and executable actions. It featured a feedback mechanism, using\nexecutable program plan examples and assertion statements to mitigate errors, enhancing\ntask success rates. Additionally, ProgPrompt veriﬁed the current state through environ-\nmental feedback during plan execution and revised the plan accordingly. The results\nindicated that the integration of programming language features substantially improved\ntask performance in contexts such as VirtualHome and real-world manipulation tasks in\nterms of success rate, goal conditions recall, and executability.Appl. Sci.2024, 14, 8868 21 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 10.ProgPrompt is a system that uses Python programming structures to provide environ-\nmental information and actions, enhancing the success rate of robot task planning through an error"
  },
  {
    "question": "Can you explain the concept of zero-shot learning in LLMs?",
    "chunk": "model focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM \nmodel inference. \n \nFigure 3. An overview of four strategies for parameter-eﬃcient ﬁne-tuning: (a) Adapter Tuning, \n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5]. \nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a \nlow-rank constraint on transformer layers to approximate the update matrices through \ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates \nthe parameter updates using low-rank dec omposition matrices. The primary bene ﬁt of \nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning, \nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory \nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning. \nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119]. \n3.3. Prompt Techniques for Increasing LLM Performance \nTo enhance the performance of LLMs, the most straightforward approach involves \ntraining with additional data via ﬁne-tuning techniques, which mirrors supervised learn-\ning in conventional machine learning. Another method for improving performance in-\nvolves the use of in-context learning, which capitalizes on prompts for zero-shot learning, \na capability ﬁrst observed in LLMs with the advent of GPT-3. The adaptation of these \nprompts for speciﬁc tasks is known as prompt engineering. Fundamentally, prompt engi-\nneering (or prompting) entails supplying inputs  to the model to perform a distinct task, \ndesigning the input format to encapsulate the task’s purpose and context, and generating \nthe desired output. The four components of pr ompt engineering can be analyzed as fol-\nlows: within the prompt, “ Instructions” delineate the speci ﬁc tasks or directives for the \nmodel and “Context” provides external or additional contextual information that can tune \nthe model. Furthermore, “Input data” refers to the type of input or questions seeking an-\nswers, and “ Output data” deﬁnes the output type or format within the prompt, thereby \noptimizing the LLM’s performance for particular tasks. Various methodologies for creat-\ning prompts have been introduced, as described below. \nFigure 3. An overview of four strategies for parameter-efﬁcient ﬁne-tuning: (a) Adapter Tuning,\n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5].\nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a\nlow-rank constraint on transformer layers to approximate the update matrices through\ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates\nthe parameter updates using low-rank decomposition matrices. The primary beneﬁt of\nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning,\nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory\nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning.\nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119].\n3.3. Prompt Techniques for Increasing LLM Performance"
  },
  {
    "question": "What is the Chain of Thought approach in AI?",
    "chunk": "reasoning steps, enabling the model to perform step-by-step reasoning. This approach\nallows the model to incrementally solve complex tasks. For instance, when asked, “If\nsomeone’s age will be 30 in 5 years, how old are they now?”, the model uses the information\n“age in 5 years is 30” to perform the intermediate reasoning step of “30→ 5 = 25” to derive\nthe ﬁnal answer.Self-consistency [120] involves the model generating various independent\nreasoning paths through few-Shot CoT, ultimately selecting the most consistent answer\namong the outputs. This method enhances the performance of CoT prompts in both\narithmetic and commonsense reasoning tasks.Multimodal CoT [121] is a two-stage\nframework that integrates text and visual modalities. Initially, intermediate reasoning steps\nare generated through rationale generation based on multimodal data. Subsequently, the\nanswer inferences are intertwined, and the informative rationales are utilized to derive the\nﬁnal answer.\nGenerally, CoT relies on human-generated annotations, which may not always provide\nthe optimal solution for problem-solving. To overcome this limitation,active prompt[122]\nhas been proposed. Active prompt enhances model performance by intensively training\nthe model on questions with higher uncertainty levels. It evaluates the uncertainty of\nanswers by posing questions to the model, with or without CoT examples. Questions with\nhigh uncertainty are selected for human annotation, and newly annotated examples are\nused to reason through each question.Program-aided language models(PAL) [123] is a\ntechnique that employs the model to understand natural language problems and generate\nprograms as intermediate reasoning steps. Unlike CoT, PAL solves problems stepwise\nusing a program runtime such as Python rather than free-form text.\nTree of thoughts(ToT) [124] is a method whereby the model breaks down a problem\ninto smaller units called thoughts, which it then assesses through a reasoning process to\ngauge its progress toward a solution. The ability of the model to generate and evaluate these\nthoughts is integrated with search algorithms such as breadth-ﬁrst and depth-ﬁrst search,\nfacilitating systematic thought exploration with lookahead and backtracking capabilities.\nIn contrast to the CoT method, which addresses problems sequentially, ToT concurrently\nexamines multiple pathways to ﬁnd a solution.Prompt chaining[125] is a strategy where\nthe model divides a task into sub-tasks, uses the outputs of each sub-task as subsequent\ninputs, and links prompts in input–output pairs. This approach improves the precision and\nconsistency of the outputs at each stage and simpliﬁes the handling of complex tasks by\nsubdividing them into manageable sub-tasks.\nGenerated knowledge prompting[126] is a technique in which the model incorpo-\nrates knowledge and information pertinent to the question and provides it alongside the\nquestion to generate more accurate answers. This method not only enhances the common-\nsense reasoning capabilities but also retains the ﬂexibility of existing models.Retrieval\naugmented generation(RAG) [127] merges external information retrieval with natural lan-\nguage generation. RAG can be ﬁne-tuned for knowledge-intensive downstream tasks and\nenables straightforward modiﬁcations or additions of knowledge within the framework.\nThis facilitates an increase in the model’s factual consistency, enhances the reliability of\ngenerated responses, and helps alleviate issues with hallucination.Automatic reasoning\nand tool-use(ART) [128] is a framework that utilizes external tools to autonomously gen-Appl. Sci.2024, 14, 8868 11 of 39\nerate intermediate reasoning steps. It chooses relevant tasks from a library that includes"
  },
  {
    "question": "Can you describe the role of self-attention in BERT?",
    "chunk": "(RNNs) to model the probability of word sequences [24–26]. A key element of this stage\nis the development of word vectors, also known as word embeddings, which form word\nprediction models based on vectors that use a distributed representation of words [24,27].\nWord2vec, a simpliﬁed shallow neural network approach, was introduced to learn these\ndistributed word representations [28,29]. It proved highly effective across various NLP\ntasks by calculating meaningful similarities between word vectors. NLMs progressed\nfrom basic word sequence modeling to sophisticated techniques for representing language\nthrough word2vec.\nFollowing the NLM phase, the ﬁeld advanced to pre-trained language models (PLMs),\nwhich encompass models such as ELMO [30] and BERT [31]. PLMs, utilizing large-scale\ntext data, learn text patterns, structures, and meanings to develop pre-trained context-\nsensitive word representations. They have successfully executed a variety of language\nunderstanding and generation tasks using this acquired knowledge. ELMo [30] introduced\na pre-training method employing bidirectional LSTM (biLSTM) networks for modeling\ndeep contextualized word representations, optimizing performance through speciﬁc ﬁne-\ntuning of the trained biLSTM network for downstream tasks. ELMo is also characterized\nas a bidirectional language model for its dual-directional use of language models.\nAnother PLM model, BERT [31], leverages the transformer architecture [32], exhibiting\nremarkable effectiveness with self-attention mechanisms and parallel processing. BERT,\na pre-trained bidirectional language model, utilizes extensive unlabeled text data. The\nmethod of unsupervised learning-based pre-training in BERT comprises two primary tasks:\nmasked language models and next sentence prediction. PLMs that provide pre-trained\ncontext-aware word representations are profoundly effective in general-purpose semantic\nfeature extraction, facilitating enhancements in NLP task performance. Owing to these\ncharacteristics, numerous subsequent studies employing pre-training and ﬁne-tuning have\nbeen introduced, featuring varied structures [33,34] (e.g., BART [33] and GPT-2 [35]) and\nenhancing pre-training strategies [36–38].\nBased on subsequent studies, it has been found that increasing the model size or data\nsize of PLMs typically enhances the performance of LM models [39]. This has prompted\nresearch into training large-scale PLMs, such as GPT-3 with 175B parameters and PaLM\nwith 540B parameters. The focus of this research, grounded in scaling laws, primarily\ncenters on augmenting model sizes and exploring the capabilities of larger models. These\ncapabilities, known as the emergent abilities of LLMs, have sparked signiﬁcant interest. For\nexample, GPT-3 can address problems it has not been trained on with minimal examples\nthrough in-context learning, a feat GPT-2 ﬁnds challenging. Due to these characteristics,\nthe academic community commonly designates these large PLMs as LLMs [40–43]. Conse-\nquently, research in this area is highly active. Notably, since the introduction of OpenAI’s\nChatGPT, there has been a surge in the number of arXiv papers on LLMs. Following\nMicrosoft’s announcement [2] about integrating ChatGPT into robotics, a variety of studies\nhave explored the application of LLMs across different areas of robotics research. The\navailable LLM models are presented in chronological order in Table2. Additionally, Table3\nincludes the VLM models.Appl. Sci.2024, 14, 8868 7 of 39\nTable 2.Chronicle of LLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref."
  },
  {
    "question": "What is instruction tuning in LLMs?",
    "chunk": "quence and sequentially predicts output tokens individually. Examples of preﬁx decoder-\nbased LLMs include GLM-130B [108] and U-PaLM [109]. Additionally, various architec-\ntures have been proposed to address e ﬃciency challenges during training or inference \nwith long inputs, due to the quadratic computational complexity of the traditional trans-\nformer architecture. For instance, the Mixture-of-Experts (MoE) scaling method [34] \nsparsely activates a subset of the neural network for each input. \n \nFigure 2. Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx De-\ncoder (middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectan-\ngles represent attention between preﬁx tokens, attention between preﬁx and target tokens, atten-\ntion between target tokens, and masked attention [5]. \nIn terms of the tuning of LLMs, these models are essentially pre-trained on massive \ndatasets and require ﬁne-tuning for diﬀerent application domains. However, the consid-\nerable model size and number of parameters pose challenges for ﬁne-tuning on standard \ncomputers and GPUs. The subsequent sections will discuss methods to address these chal-\nlenges. \nLLM tuning is broadly divided into two cate gories based on the training objective. \nInstruction tuning is a form of supervised learning where the training data typically in-\nclude descriptions of tasks, inputs, and corresponding outputs. This type of tuning is de-\nsigned (1) to enhance the functional capabilities of LLMs, (2) to specialize them by training \nwith discipline-speci ﬁc information, and (3) to improve task generalization and con-\nsistency through a be tter understanding of natural language commands. Conversely, \nalignment tuning (or preference alignment) seeks to align the behavior of LLMs with hu-\nman values and preferences. Prominent methods include reinforcement learning from hu-\nman feedback (RLHF) [110], which involves ﬁne-tuning LLMs using human feedback to \nbetter reﬂect human values, and direct preference optimization (DPO) [111], focusing on \ntraining with pairs of human preferences th at usually include an input prompt and the \npreferred and non-preferred responses. \nFor both instruction tuning and alignment tuning, which involve training LLMs with \nextensively large model para meters, substantial GPU memo ry and computational re-\nsources are required, with high costs typi cally incurred when utilizing cloud-based \nFigure 2.Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx Decoder\n(middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectangles\nrepresent attention between preﬁx tokens, attention between preﬁx and target tokens, attention\nbetween target tokens, and masked attention [5].\nIn terms of the tuning of LLMs, these models are essentially pre-trained on mas-\nsive datasets and require ﬁne-tuning for different application domains. However, the\nconsiderable model size and number of parameters pose challenges for ﬁne-tuning on\nstandard computers and GPUs. The subsequent sections will discuss methods to address\nthese challenges.\nLLM tuning is broadly divided into two categories based on the training objective.\nInstruction tuning is a form of supervised learning where the training data typically include\ndescriptions of tasks, inputs, and corresponding outputs. This type of tuning is designed\n(1) to enhance the functional capabilities of LLMs, (2) to specialize them by training with"
  },
  {
    "question": "How do VLMs assist in object manipulation tasks?",
    "chunk": "Table 5.Summary of the reviewed papers in this study.\nName Explanation Ref.\nReward Design in RL\n• Eureka automatically generates and improves reward\nfunctions based on the virtual environment source\ncode.\n• DrEureka builds reward-aware physics priors using\nEureka and supports effective operation in the real\nworld through domain randomization.\n• LLMs design and reﬁne reward functions based on\nnatural language input.\n• LLMs and VLMs integrate multimodal data to\ngenerate reward functions.\n[11,134,136–139,176–180]\nLow-level\nControl\n• Generating commands to control actuators capable of\nlow-level control.\n• RT-1 and RT-2 enable robots to perform complex tasks\nbased on language-vision data.\n• AutoRT establishes a system where robots can\nautonomously collect and utilize data.\n[8–10,144–148,181–183]\nHigh-level\nPlanning\n• LLMs provide an effective methodology for tasks\nrelated to high-level planning within robotic systems.\n• By using natural language, LLMs can formulate plans\nto solve tasks that require long-horizon reasoning.\n• LLMs assess the feasibility of actions to determine and\nexecute the optimal robotic behavior.\n• LLMs generate behavior trees to structure complex\nrobotic actions accurately.\n[149–160,184–207]\nManipulation\n• Using LLMs and VLMs to integrate language and\nvision data allows various manipulations.\n• LLMs interpret high-level instructions to generate the\nnecessary robot actions and assess their feasibility.\n• VLMs extract object information from images to assist\nin performing manipulations.\n[161–167,208–215]\nScene\nUnderstanding\n• To solve VQA problems, use VLMs to extract\nhigh-level information from vision data.\n• For scene understanding, estimate and identify objects\nand evaluate relationships between objects.\n• For navigation, convert natural language instructions\nand combine them with vision data to identify the\nimage through probability distributions.\n[168–175,216–223]\n5. Discussion and Future Directions\nThe review revealed two potentials of foundation models: (1) commonsense reasoning\nfor planning and (2) the ability to generate code.\nThe ﬁrst ﬁnding from this review study is the potential to enhance robot intelligence\nthrough foundation models. Beyond the studies mentioned here, numerous recent stud-\nies have shown that pre-trained models such as LLMs and VLMs can enhance various\naspects of robot intelligence, such as situational awareness, high-level task planning, and\nhuman interaction. LLMs allow communication with humans in natural languages, objectAppl. Sci.2024, 14, 8868 28 of 39\nutilization based on extensive information, and high-level planning using that information.\nVLMs can describe tasks in text and understand visual information. Furthermore, the\ninformation from VLMs can be supplemented by connecting to knowledge databases via\nLLMs. These capabilities are crucial for enhancing robot intelligence, broadening the scope\nof robot applications, and maximizing robot utility.\nThe second ﬁnding is the code generation capability of LLMs, which has the potential\nto automate the robot development process traditionally performed by humans. Addi-\ntionally, robots that can autonomously update their own algorithms are no longer just\nscience ﬁction. Although limitations exist for robots to self-update, frameworks such as\nEureka and DrEureka, which automatically enhanced reinforcement learning performance\nfor robot motion control, demonstrate the potential for future advancements. This suggests\nthat LLMs may not only enhance human interactions but could also pave the way for\nself-improvement without human intervention."
  },
  {
    "question": "What safety and ethical considerations need to be addressed in robotic intelligence?",
    "chunk": "While foundation models offer considerable potential for advancing robotics intelli-\ngence, several limitations and future considerations remain. These include (1) the speed\nof inference required for real-time applications, (2) the computational efﬁciency necessary\nfor embedded systems, (3) the ability to handle multi-modality information, and (4) the\nnecessity of addressing safety and ethical considerations.\nFirst of all, LLMs and VLMs hold considerable potential for enhancing robot intelli-\ngence. Nonetheless, several critical issues remain to be addressed. Foundation models,\ncharacterized as large-scale models pre-trained on extensive datasets, face challenges re-\nlated to real-time requirements and limited computational resources in robotic applications.\nMoreover, concerns such as personal information protection, privacy, and security from\nexternal attacks need resolution to enable cloud-based LLMs for robotics.\nSecondly, to enhance the computational efﬁciency and usability of LMs, there is\nongoing research into small language models (SLMs). Despite having fewer parameters,\nSLMs can achieve performance comparable to LLMs in speciﬁc applications. Several\nSLMs have been introduced, including DistilBERT [224], which is a compact version of\nGoogle’s BERT; Phi-3 [61], another SLM; Florence-2 [84], a small VLM model from Microsoft;\nMobileBERT [225], which is optimized for mobile platforms; and compact open-source\nversions of OpenAI’s GPT models such as GPT-Neo [226] and GPT-J [227]. Generally, SLMs\nare streamlined models with fewer parameters compared to LLMs, which can number in the\nbillions. SLMs utilize smaller, domain-speciﬁc datasets and require shorter training periods,\ntypically just a few weeks, unlike LLMs, which demand vast datasets for broad learning\nand multiple months of training. Developing SLMs to excel within speciﬁc domains for\nrobotic systems and ensuring real-time performance with minimal computational resources\nare essential research directions for advancing robot intelligence with SLMs.\nThe third implication is that LLMs, based on text-centered natural language process-\ning, are limited as single-modality models when applied to real-world robotic systems\nwhere information often blends in diverse ways. Research on LLMs is transitioning from\nsingle-modality to multimodality models, as evidenced by VLMs and OpenAI Sora [228],\nwith increasing demand for such models. Currently, to address the limitations of LLMs’\nsingle-modality, robotic systems are being developed with multimodality models that\nintegrate vision, such as VLMs. However, relying solely on text and images falls short\nof the diverse information range required in the real world, including images, sounds,\nvideos, and proprioceptive sensory information (such as the position, orientation, balance,\nmovement degree, and direction of various parts of the robot). Proprioceptive sensory\ninformation related to actions and movements is particularly vital for enhancing dynamic\nhuman interaction, information processing, and manipulation and planning skills based on\ndynamic movements. For instance, the integrated VLA model, which facilitates low-level\ncontrol based on LLMs and VLMs as shown by Google’s RT-2 model, highlights the neces-\nsity for models capable of integrating information from a broader range of modalities to\nenhance robot intelligence.Appl. Sci.2024, 14, 8868 29 of 39\nFinally, the fourth area to consider is how to address safety and ethical issues when\nLLM is applied to robotic intelligence systems. Studies were conducted to address the\nissue of discriminatory and unsafe behaviors that may be generated by robot applications"
  },
  {
    "question": "How does the integration of visual data change the way LLMs process information?",
    "chunk": "3.3. Prompt Techniques for Increasing LLM Performance\nTo enhance the performance of LLMs, the most straightforward approach involves\ntraining with additional data via ﬁne-tuning techniques, which mirrors supervised learning\nin conventional machine learning. Another method for improving performance involves the\nuse of in-context learning, which capitalizes on prompts for zero-shot learning, a capability\nﬁrst observed in LLMs with the advent of GPT-3. The adaptation of these prompts for\nspeciﬁc tasks is known as prompt engineering. Fundamentally, prompt engineering (or\nprompting) entails supplying inputs to the model to perform a distinct task, designing the\ninput format to encapsulate the task’s purpose and context, and generating the desired\noutput. The four components of prompt engineering can be analyzed as follows: within\nthe prompt, “Instructions” delineate the speciﬁc tasks or directives for the model andAppl. Sci.2024, 14, 8868 10 of 39\n“Context” provides external or additional contextual information that can tune the model.\nFurthermore, “Input data” refers to the type of input or questions seeking answers, and\n“Output data” deﬁnes the output type or format within the prompt, thereby optimizing the\nLLM’s performance for particular tasks. Various methodologies for creating prompts have\nbeen introduced, as described below.\nZero-shot prompting[53] is a technique that allows the model to take on new tasks\nwith no prior examples. The model relies solely on the task description or instructions with-\nout additional training. Likewise,few-shot prompting[49] introduces a small number of\nexamples to aid the model in learning new tasks. This approach does not require extensive\ndatasets and can improve the model’s performance through a limited set of examples.\nChain-of-thought (CoT) [41] is a technique that explicitly describes intermediate\nreasoning steps, enabling the model to perform step-by-step reasoning. This approach\nallows the model to incrementally solve complex tasks. For instance, when asked, “If\nsomeone’s age will be 30 in 5 years, how old are they now?”, the model uses the information\n“age in 5 years is 30” to perform the intermediate reasoning step of “30→ 5 = 25” to derive\nthe ﬁnal answer.Self-consistency [120] involves the model generating various independent\nreasoning paths through few-Shot CoT, ultimately selecting the most consistent answer\namong the outputs. This method enhances the performance of CoT prompts in both\narithmetic and commonsense reasoning tasks.Multimodal CoT [121] is a two-stage\nframework that integrates text and visual modalities. Initially, intermediate reasoning steps\nare generated through rationale generation based on multimodal data. Subsequently, the\nanswer inferences are intertwined, and the informative rationales are utilized to derive the\nﬁnal answer.\nGenerally, CoT relies on human-generated annotations, which may not always provide\nthe optimal solution for problem-solving. To overcome this limitation,active prompt[122]\nhas been proposed. Active prompt enhances model performance by intensively training\nthe model on questions with higher uncertainty levels. It evaluates the uncertainty of\nanswers by posing questions to the model, with or without CoT examples. Questions with\nhigh uncertainty are selected for human annotation, and newly annotated examples are\nused to reason through each question.Program-aided language models(PAL) [123] is a\ntechnique that employs the model to understand natural language problems and generate\nprograms as intermediate reasoning steps. Unlike CoT, PAL solves problems stepwise\nusing a program runtime such as Python rather than free-form text.\nTree of thoughts(ToT) [124] is a method whereby the model breaks down a problem"
  },
  {
    "question": "What roles do LLMs and VLMs play in robotic applications?",
    "chunk": "more, Eureka has demonstrated the ability to solve complex problems that were previ-\nously unsolved by expert-designed reward functions. \nGiven these research outcomes, integrating language models into robotic intelligence \npresents signi ﬁcant potential to enhance robot capabilities and applications dramatically, \nthereby rede ﬁning their roles in diverse industries and everyday life. Therefore, this sur-\nvey paper explores recent research trends in LLM- and VLM-based robot intelligence, \naiming to provide a comprehensive understanding of future development possibilities by \nexamining the application of language models in various robotic research ﬁelds. It also \nseeks to highlight research cases, identify cu rrent limitations, and suggest future research \ndirections. \nTo chronicle this advancement in robotics research ﬁelds, this review paper presents \nthe following contributions: \n• This paper summarizes and introduces the foundational elements and tuning meth-\nods of LLM architecture. \n• It explores and arranges prompt techniques  to enhance the problem-solving abilities \nof LLMs. \n• It  reviews and encapsulates how LLMs and VLMs have been employed to augment \nrobot intelligence across ﬁve topics as shown in Figure 1: (1) reward design for rein-\nforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation, \nand (5) scene understanding. \n \nFigure 1. Five categories for robot intelligence with large language models in this study. \nFigure 1.Five categories for robot intelligence with large language models in this study.Appl. Sci.2024, 14, 8868 4 of 39\nThe reward design in RLcategory represents a research ﬁeld in which an LLM devel-\nops and enhances reward functions employed in reinforcement learning via code-based\ndescriptions and natural language input. This enables robots to learn optimal policies\nfor speciﬁc tasks through reinforcement learning, even in complex environments. The\nlow-level controlcategory includes a research area in which LLMs and VLMs generate\ncommand sequences that directly control the robot’s actuators through natural language\nand visual input. Thehigh-level planningcategory is a research area where the LLM\nidentiﬁes the present circumstances and objective of the tasks, subsequently developing an\nexplainable plan based on the reasoning required for problem-solving. In this research area,\nthe LLM is also tasked with developing the optimal robot behavior plan, which entails\nevaluating the feasibility of the established plan. In themanipulation category, the LLM in-\nterprets high-level instructions and the VLM (and LLM) analyzes various conditions based\non their understanding of the surroundings to assist robot arms in performing the speciﬁc\ntasks. While this category can be broadly included in the high-level planning category,\nthere are numerous studies that are speciﬁcally related to manipulation with a robot arm,\nwhich is why the manipulation category was separated. Thescene understandingcategory\nrepresents a research area that seeks to combine LLMs and VLMs with the objective of\nassisting robots in comprehending their surrounding environment. This is accomplished\nby identifying objects based on natural language instructions and visual information, as\nwell as by evaluating the relationships between them. This research area is also closely\nrelated to the ﬁeld of autonomous visual navigation. From a boarder perspective, there is an\noverlap between the scene understanding category and the perception-related components\nof the high-level planning category. However, in this review, the scene understanding\ncategory was considered a distinct category due to its prevalence as an application of\nVLM models.\nTable 1 lists resources that aid in understanding robot intelligence based on language"
  },
  {
    "question": "How does high-level planning affect the performance of robotic systems?",
    "chunk": "more, Eureka has demonstrated the ability to solve complex problems that were previ-\nously unsolved by expert-designed reward functions. \nGiven these research outcomes, integrating language models into robotic intelligence \npresents signi ﬁcant potential to enhance robot capabilities and applications dramatically, \nthereby rede ﬁning their roles in diverse industries and everyday life. Therefore, this sur-\nvey paper explores recent research trends in LLM- and VLM-based robot intelligence, \naiming to provide a comprehensive understanding of future development possibilities by \nexamining the application of language models in various robotic research ﬁelds. It also \nseeks to highlight research cases, identify cu rrent limitations, and suggest future research \ndirections. \nTo chronicle this advancement in robotics research ﬁelds, this review paper presents \nthe following contributions: \n• This paper summarizes and introduces the foundational elements and tuning meth-\nods of LLM architecture. \n• It explores and arranges prompt techniques  to enhance the problem-solving abilities \nof LLMs. \n• It  reviews and encapsulates how LLMs and VLMs have been employed to augment \nrobot intelligence across ﬁve topics as shown in Figure 1: (1) reward design for rein-\nforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation, \nand (5) scene understanding. \n \nFigure 1. Five categories for robot intelligence with large language models in this study. \nFigure 1.Five categories for robot intelligence with large language models in this study.Appl. Sci.2024, 14, 8868 4 of 39\nThe reward design in RLcategory represents a research ﬁeld in which an LLM devel-\nops and enhances reward functions employed in reinforcement learning via code-based\ndescriptions and natural language input. This enables robots to learn optimal policies\nfor speciﬁc tasks through reinforcement learning, even in complex environments. The\nlow-level controlcategory includes a research area in which LLMs and VLMs generate\ncommand sequences that directly control the robot’s actuators through natural language\nand visual input. Thehigh-level planningcategory is a research area where the LLM\nidentiﬁes the present circumstances and objective of the tasks, subsequently developing an\nexplainable plan based on the reasoning required for problem-solving. In this research area,\nthe LLM is also tasked with developing the optimal robot behavior plan, which entails\nevaluating the feasibility of the established plan. In themanipulation category, the LLM in-\nterprets high-level instructions and the VLM (and LLM) analyzes various conditions based\non their understanding of the surroundings to assist robot arms in performing the speciﬁc\ntasks. While this category can be broadly included in the high-level planning category,\nthere are numerous studies that are speciﬁcally related to manipulation with a robot arm,\nwhich is why the manipulation category was separated. Thescene understandingcategory\nrepresents a research area that seeks to combine LLMs and VLMs with the objective of\nassisting robots in comprehending their surrounding environment. This is accomplished\nby identifying objects based on natural language instructions and visual information, as\nwell as by evaluating the relationships between them. This research area is also closely\nrelated to the ﬁeld of autonomous visual navigation. From a boarder perspective, there is an\noverlap between the scene understanding category and the perception-related components\nof the high-level planning category. However, in this review, the scene understanding\ncategory was considered a distinct category due to its prevalence as an application of\nVLM models.\nTable 1 lists resources that aid in understanding robot intelligence based on language"
  },
  {
    "question": "How are reward functions generated for robotic tasks?",
    "chunk": "Figure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses expert-\ndesigned functions through iterative improvements [11]. \nFollowing Eureka, DrEureka [134], shown in Figure 5, was developed to address the \nsim-to-real problem by automatically con ﬁguring appropriate reward functions and do-\nmain randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses\nexpert-designed functions through iterative improvements [11].\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 13 of 39 \n \n \nFigure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses expert-\ndesigned functions through iterative improvements [11]. \nFollowing Eureka, DrEureka [134], shown in Figure 5, was developed to address the \nsim-to-real problem by automatically con ﬁguring appropriate reward functions and do-\nmain randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 5.DrEureka leverages LLM to design reward functions and solves the sim-to-real problem\nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134].\nXie [136] introducedText2Reward, a framework that automatically generated dense\nreward functions for reinforcement learning using LLMs. Provided with a goal expressed\nin natural language, Text2Reward produced executable dense reward functions derived\nfrom a compact representation of the environment. This framework generated free-form\ndense reward codes and delivered performance comparable to or surpassing that of policies\ntrained with expert-designed codes across a variety of tasks, including 17 manipulator-\nrelated tasks and six novel locomotion behaviors. Additionally, Text2Reward incorporated\nuser feedback to iteratively enhance the generated reward functions, thereby increasing the\nsuccess rate of the learned policies.\nDi Palo [137] explored the use of LLMs and VLMs to improve reinforcement learning"
  },
  {
    "question": "What role does planning play in a robot's autonomy and efficiency?",
    "chunk": "of labeled data. This process is inherently resource-intensive. Moreover, these models\nare designed for a speciﬁc environment and require reconﬁguration whenever the task or\nenvironment changes. This renders the robots challenging to adapt and scale to disparate\nenvironments [3]. For practical robot systems, it is essential that they are able to ﬂexibly\nrespond to the ever-changing physical environment. From this perspective, the gener-\nalization of affordable tasks, environmental adaptability, and the accuracy of execution,\nplanning, and reasoning capabilities remain signiﬁcant challenges for traditional robotic\nintelligence systems [4].\nHowever, LLMs and VLMs help a robot intelligence system to enhance its general-\nization capability in dynamic and complex real-world environments. LLMs can leverage\npre-trained knowledge from extensive datasets to augment their ability to generalize to\neveryday tasks that are typically expected of robots. Unlike the conventional supervised\nmodels, LLMs can utilize zero-shot and few-shot learning to help robots quickly adapt\nto new environments without additional training [5]. This has the advantage of signiﬁ-\ncantly reducing the need for costly data collection and labeling. In addition, robot systems\nequipped with LLMs can process complex instructions based on their ability to understand\nand generate natural language, which can improve human–robot interactions. Furthermore,\nLLMs can be integrated with multimodal sensors such as LiDAR, depth, voice, tactile, pro-\nprioception, and visual information, which enables robots to comprehensively understand\nand adapt to their environment [6].\nLLMs have demonstrated exceptional capabilities in processing and understanding\ntext-based information, signiﬁcantly enhancing robotic communication abilities. For in-\nstance, robots can accurately comprehend and execute natural language commands via\nLLMs, providing scalability and ﬂexibility beyond traditional word-based robotic com-\nmand systems. Consequently, robots can respond more adaptably and intelligently in\ninteractions with human users, allowing them to engage in complex problem-solving and\ndecision-making processes beyond simple mechanical tasks.\nAdditionally, LLMs not only enhance a robot’s communication skills to improve HRI\nusability but also boost the robot’s planning abilities. Planning involves setting goals\nand devising a sequence of actions to achieve them, which are essential in determining a\nrobot’s autonomy and efﬁciency. LLMs interpret natural language from users and complex\ncommands, enabling robots to establish and execute suitable plans in various situations.\nMoreover, LLMs adapt ﬂexibly to new situations through a zero-shot approach and utilize\npast data for learning. These capabilities indicate that robots can play a vital role in\nautonomously navigating changing environments and resolving unexpected issues.\nMoreover, VLMs such as CLIP [7], which are trained to solve vision question answering\n(VQA) tasks, have the ability to process visual and linguistic information simultaneously.\nThis ability allows robots to visually perceive their surroundings and integrate this infor-\nmation into linguistic descriptions, enabling more sophisticated situational awareness. For\ninstance, using VLMs, a robot can recognize objects and provide descriptions, as well as\nunderstand and execute user commands based on visual cues. This integrated approach\nsigniﬁcantly enhances a robot’s autonomy and interaction capabilities.\nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which en-\nable low-level actuator control using LLMs and VLMs, Google has introduced AutoRT [10].\nAutoRT is a system where robots interact with real-world objects to collect motion data. ItAppl. Sci.2024, 14, 8868 3 of 39"
  },
  {
    "question": "How do strict guardrails contribute to the reliability of robots?",
    "chunk": "prompts have the potential to cause the entire robotic system to malfunction. To defend\nagainst this critical threat to the reliability and safety of robotic systems, various techniques\nhave been proposed, such as input validation, which ﬁlters the model’s input, and context\nlocking, which restricts access based on the history and content of the prompt. Furthermore,\nstrict guardrails that restrict harmful or unsafe outputs from models can be an alternative\nto improve the reliability of robotic systems. However, it is essential to recognize that the\nsecurity techniques may potentially lead to a decline in the performance of the robot system.\nConsequently, the trade-off between performance and safety must be carefully considered.\nSince the emergence of ChatGPT and Microsoft’s implementation of robot systems\nusing ChatGPT [2], artiﬁcial intelligence components have been applied more widely and\nintensively in robotics research. Despite existing challenges, it is expected that research\ninvolving foundation models to improve robot intelligence will persist across various\ndomains and methods, which will likely enhance the usability and market potential of\nrobot systems inﬂuenced by these advancements.\n6. Conclusions\nIn this paper, we have explored the potential impact and applicability of LLMs on\nrobotics research ﬁelds by summarizing studies that applied LLMs and VLMs to robots.\nFundamentally, LLMs can enhance the capability of robots in natural language processing\nto interact with humans and to improve the robots’ autonomy in various task scenarios.\nIn particular, the ability of LLMs to understand and generate natural language plays a\ncrucial role in enabling robots to comprehend and execute complex commands. This survey\nconﬁrmed that the scope of utilizing LLMs in robotics was not limited to simple natural\nlanguage processing but also extended to broader research areas. This study explored\nextensive LLM applications in the robotics literature, such as planning, manipulation, and\nscene understanding, as well as reinforcement learning automation frameworks such as\nEureka, and included robot actions in language models such as AutoRT. Moreover, the\nresearch direction of current generative AI models is transitioning towards multimodalAppl. Sci.2024, 14, 8868 30 of 39\nlanguage models, moving beyond information acquisition and cognition aspects such as\ntext, images, and videos to include actuator actions within large models in therobotics ﬁeld.\nWhile the surveyed studies indicated that LLMs play a promising role in the future\nof robotics, certain limitations were also identiﬁed. First, the increased computational\nresources and energy consumption associated with embedding LLMs into robotic systems\nmust be addressed. Second, biases in language models and ethical considerations are\nsigniﬁcant issues that need to be tackled in robotics. Therefore, continual efforts will be\nnecessary in future research to resolve these challenges.\nOverall, LLMs are valuable tools that can signiﬁcantly advance robotics. This review\nhas revealed that innovative robot applications are possible through the integration of\nLLMs and VLMs. Moreover, these foundation models are expected to serve as critical\nelements for future robot research and practical applications in the real world.\nAuthor Contributions:Conceptualization, S.S. and C.K.; methodology, S.S.; formal analysis, H.J.,\nH.L. and S.S.; investigation, H.J., H.L. and S.S.; resources, H.J., H.L., S.S. and C.K.; writing—original\ndraft preparation, H.J., H.L., S.S. and C.K.; writing—review and editing, H.J., H.L., S.S. and C.K.; vi-"
  },
  {
    "question": "What were the success rates of the SayCan framework in different environments?",
    "chunk": "including 6-DoF end-eﬀector waypoints, for various manipulation tasks using an open set \nof instructions and objects. Huang note d that LLMs were skilled at deriving a ﬀordances \nand constraints from free-form language in structions. Further, by harnessing code \ngeneration capabilities, Huang developed 3D value maps for the agent’s observation \nspace through interactions with VLMs. Thes e 3D value maps were integrated into a \nmodel-based planning framework to generate closed-loop robot trajectories robust to \ndynamic perturbations in a zero-shot approach. The proposed framework demonstrated \neﬃcient learning of the dynamics model for sc enes with contact-rich interactions and \nprovided advantages in these complex scenarios. \n \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM to \ngenerate 3D a ﬀordance and constraint maps and design robot trajectories without additional \ntraining [165]. \nAhn [166] introduced a framework named SayCan, which integrates LLMs with \nreinforcement learning value functions, enabling robots to follow high-level text \ninstructions. SayCan comprises two primary components: Say, which uses an LLM for \ntask-based decision-making, and Can, which evaluates the feasibility of these decisions \nvia reinforcement learning. Say leverages task-based knowledge from the LLM and \nreinforcement learning functionality to assess the feasibility of task execution by robots in \nreal-world scenarios. The LLM determines th e actions necessary to achieve high-level \ngoals and evaluates the eﬀectiveness of each action in fulﬁlling the instructions. Learned \nthrough reinforcement learning, the a ﬀordance function estimate s each action’s success \nprobability in the current state, con ﬁrming the executability of actions proposed by the \nLLM. This process allows the LLM to assess the robot’s current state and capabilities, \nultimately generating an interpretable action plan. SayCan was evaluated across 101 robot \ntasks, achieving an 84% plan success rate and a 74% execution success rate in a simulated \nkitchen environment. In a real kitchen setting, the plan success rate decreased slightly to \n81% and the execution success rate fell to 60%, demonstrating that the policy and value \nfunctions generalize well to real-world settings. \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM\nto generate 3D affordance and constraint maps and design robot trajectories without additional\ntraining [165].\nAhn [166] introduced a framework named SayCan, which integrates LLMs with rein-\nforcement learning value functions, enabling robots to follow high-level text instructions.\nSayCan comprises two primary components: Say, which uses an LLM for task-based\ndecision-making, and Can, which evaluates the feasibility of these decisions via reinforce-\nment learning. Say leverages task-based knowledge from the LLM and reinforcement\nlearning functionality to assess the feasibility of task execution by robots in real-world\nscenarios. The LLM determines the actions necessary to achieve high-level goals and\nevaluates the effectiveness of each action in fulﬁlling the instructions. Learned through re-\ninforcement learning, the affordance function estimates each action’s success probability in\nthe current state, conﬁrming the executability of actions proposed by the LLM. This process\nallows the LLM to assess the robot’s current state and capabilities, ultimately generating\nan interpretable action plan. SayCan was evaluated across 101 robot tasks, achieving an\n84% plan success rate and a 74% execution success rate in a simulated kitchen environment."
  },
  {
    "question": "How do LLMs handle long-term planning in robotics?",
    "chunk": "reward-labeled trajectories as context and incorporating online interaction, LLM-based\nagents could explore small grids and reﬁne simple trajectories using human-in-the-loop\nmethods, such as optimizing a CartPole controller.Appl. Sci.2024, 14, 8868 17 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 17 of 39 \n \nsequence transformation, the research demonstrated that LLMs could generalize speci ﬁc \nsequence transformations using benchmarks such as ARC (abstraction and reasoning \ncorpus) and PCFG (probabilistic context-free grammar), thereby proving their utility in \nspatial reasoning tasks for robotics. In sequence completion, the study examined whether \nLLMs could ﬁnish pa tterns in elementary functions (e.g., sinusoids), illustrating their \nutility in robotic tasks such as extending a wiping motion from kinesthetic demonstrations \nor creating drawings on a whiteboard. Finally, in sequence improvement, the research \nrevealed that by utilizing reward-labeled trajectories as context and incorporating online \ninteraction, LLM-based agents could explore small grids and re ﬁne simple trajectories \nusing human-in-the-loop methods, such as optimizing a CartPole controller.  \n \nFigure 6. Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed \nin sequence transformation, completion, and improvement [148]. \n4.3. High-Level Planning (Including Decision-Making and Reasoning) \nThe abstraction and generalization capabilities of LLMs oﬀer eﬀective methodologies \nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various \nresearch outcomes have been realized in the ﬁelds of planning, decision-making, \nreasoning, and behavior trees within robotics. \nYoneda [149] introduced Statler, a framew ork designed to provide LLMs with an \nexplicit world state representation through a continuously maintained ‘memory’. The core \nof Statler consisted of two components: the world model reader and the world model \nwriter. These components interacted with and sustained the world state. The world model \nreader interpreted user commands and generated executable code based on the current \nstate representation, while the world model wr iter updated the system’s state according \nto execution outcomes. By facilitating access to the world state ‘memory’, Statler improved \nLLMs’ ability to reason about planning task s with extended time horizons, overcoming \nlimitations imposed by context length. \nMu [150], shown in Figure 7, introduced EmbodiedGPT, a model speci ﬁcally \ndesigned for Embodied AI, which leverages LLMs. This framework processes visual \nobservations and natural language to establish long-term plans and execute tasks in real-\nFigure 6.Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed in\nsequence transformation, completion, and improvement [148].\n4.3. High-Level Planning (Including Decision-Making and Reasoning)\nThe abstraction and generalization capabilities of LLMs offer effective methodologies\nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various\nresearch outcomes have been realized in the ﬁelds of planning, decision-making, reasoning,\nand behavior trees within robotics.\nYoneda [149] introduced Statler, a framework designed to provide LLMs with an\nexplicit world state representation through a continuously maintained ‘memory’. The core\nof Statler consisted of two components: the world model reader and the world model\nwriter. These components interacted with and sustained the world state. The world model"
  },
  {
    "question": "What role does the LLaMA model play in generating plans?",
    "chunk": "Appl. Sci. 2024, 14, x FOR PEER REVIEW 18 of 39 \n \ntime. EmbodiedGPT utilizes pre-trained visi on transformers and the LLaMA language \nmodel to encode visual features and map them  to the language modality. The generated \nplan was subsequently converted into speci ﬁc task commands using general visual \ntokens, encoded by the vision model. The framework’s functionality comprises (1) \nencoding current visual features, (2) mapping visual features to the language modality \nvia a ttention-based interactions between visual tokens and text queries or learnable \nembedded queries, (3) generating plans with the LLaMA language model and translating \nthem into speciﬁc task commands, and (4) querying the encoded visual tokens from the \nvision model and translating them into low-level control commands through a \ndownstream policy network for task execution. Experimental results, utilizing the MS-\nCOCO dataset, revealed that EmbodiedG PT excels in object recognition and \nunderstanding spatial relationships. Notabl y, implementing a closed-loop design and a \n“chain-of-thought” training mode signi ﬁcantly enhanced EmbodiedGPT’s performance. \nThese results demonstrate that EmbodiedGPT e ﬀectively handles various autonomous \ntasks, exhibiting superior capability in object recognition, understanding spatial \nrelationships, and generating logical, executable plans. \n \nFigure 7. After encoding visual features, they are mappe d using visual tokens and text queries. A \nplan is then created with the LLaMA model and turned into task commands. The visual tokens are \nqueried and converted into low-level control commands to perform the task [150]. \nChen [151] introduced the language-model-based commonsense reasoning (LMCR) \nframework to assist robots in comprehendin g incomplete natural language instructions. \nThis framework enabled robots to receive instructions in natural language from humans, \nobserve their surroundings, and employ a commonsense reasoning method to \nautonomously infer missing information. LMCR utilized a model of commonsense \nreasoning learned from web-based text materials, allowing robots to understand \nincomplete instructions and autonomously execute tasks. The framework comprised three \nmain functions: language understanding, commonsense reasoning, and action planning. \nIn language understanding, LMCR translated human natural language instructions into a \nform interpretable by robots, parsing them into verb frames to convert them into \nexecutable structures. During the common sense reasoning phase, the robot analyzed \nsurrounding objects and employed a language model trained on large-scale unstructured \ntext materials to ﬁll in the missing details from the instructions. This model identiﬁed the \nFigure 7.After encoding visual features, they are mapped using visual tokens and text queries. A\nplan is then created with the LLaMA model and turned into task commands. The visual tokens are\nqueried and converted into low-level control commands to perform the task [150].\nChen [151] introduced the language-model-based commonsense reasoning (LMCR)\nframework to assist robots in comprehending incomplete natural language instructions.\nThis framework enabled robots to receive instructions in natural language from humans, ob-\nserve their surroundings, and employ a commonsense reasoning method to autonomously\ninfer missing information. LMCR utilized a model of commonsense reasoning learned\nfrom web-based text materials, allowing robots to understand incomplete instructions and\nautonomously execute tasks. The framework comprised three main functions: language\nunderstanding, commonsense reasoning, and action planning. In language understanding,\nLMCR translated human natural language instructions into a form interpretable by robots,"
  },
  {
    "question": "How can researchers ensure the reliability of robotic systems using language models?",
    "chunk": "that is biased based on personal characteristics (such as race, nationality, religion, gender,\ndisability, and so forth). In addition, they can also be used to instruct robotic systems to\nengage in violent or illegal behaviors such as misstatements, sexual predation, etc. Notable\nexamples include discriminatory behaviors such as inadequate recognition of children\nor individuals with speciﬁc skin tones in human detection systems, and the exclusion\nof individuals with disabilities from task assignments. It is imperative to consider the\npotential social biases of LLM when integrating with robotic systems. Although this kind of\nconsideration was secondary in traditional robotic systems because of the limitation of their\nlanguage capability, it is a necessary consideration for LLMs to be able to generate human-\nlike language. To address this issue, previous studies have attempted to resolve it in various\nways, such as AutoRT’s constitutional rules [10], DrEureka’s safety instructions [134], and\nNeMo’s guardrails [230]. The guideline-based output control of LLMs can represent an\naccessible method to ensure safety.\nAs an extension of this point, safety issues can be identiﬁed when integrating LLMs\nand VLMs into robotic intelligence systems [231]. Typically, in robotic intelligence systems,\nLLM models generate high-level action plans in various forms, such as programming\ncodes and behavior trees based on natural language or vector prompts. At this point, a\nprompt attack has the potential to disrupt the inference of the LLMs, thereby threatening\nthe reliability and safety of the robotic system. Prompt injection is one of the prompt attacks,\nwhereby the inference of LLMs is subtly altered through speciﬁc inputs. Jailbreak, another\nprompt attack, bypasses safety rules and causes LLMs to generate abnormal behaviors to\nbe performed by the robotic system. Consequently, even minor disturbances in the input\nprompts have the potential to cause the entire robotic system to malfunction. To defend\nagainst this critical threat to the reliability and safety of robotic systems, various techniques\nhave been proposed, such as input validation, which ﬁlters the model’s input, and context\nlocking, which restricts access based on the history and content of the prompt. Furthermore,\nstrict guardrails that restrict harmful or unsafe outputs from models can be an alternative\nto improve the reliability of robotic systems. However, it is essential to recognize that the\nsecurity techniques may potentially lead to a decline in the performance of the robot system.\nConsequently, the trade-off between performance and safety must be carefully considered.\nSince the emergence of ChatGPT and Microsoft’s implementation of robot systems\nusing ChatGPT [2], artiﬁcial intelligence components have been applied more widely and\nintensively in robotics research. Despite existing challenges, it is expected that research\ninvolving foundation models to improve robot intelligence will persist across various\ndomains and methods, which will likely enhance the usability and market potential of\nrobot systems inﬂuenced by these advancements.\n6. Conclusions\nIn this paper, we have explored the potential impact and applicability of LLMs on\nrobotics research ﬁelds by summarizing studies that applied LLMs and VLMs to robots.\nFundamentally, LLMs can enhance the capability of robots in natural language processing\nto interact with humans and to improve the robots’ autonomy in various task scenarios.\nIn particular, the ability of LLMs to understand and generate natural language plays a\ncrucial role in enabling robots to comprehend and execute complex commands. This survey\nconﬁrmed that the scope of utilizing LLMs in robotics was not limited to simple natural\nlanguage processing but also extended to broader research areas. This study explored"
  },
  {
    "question": "How can robots recognize objects and respond to visual cues?",
    "chunk": "mand systems. Consequently, robots can respond more adaptably and intelligently in\ninteractions with human users, allowing them to engage in complex problem-solving and\ndecision-making processes beyond simple mechanical tasks.\nAdditionally, LLMs not only enhance a robot’s communication skills to improve HRI\nusability but also boost the robot’s planning abilities. Planning involves setting goals\nand devising a sequence of actions to achieve them, which are essential in determining a\nrobot’s autonomy and efﬁciency. LLMs interpret natural language from users and complex\ncommands, enabling robots to establish and execute suitable plans in various situations.\nMoreover, LLMs adapt ﬂexibly to new situations through a zero-shot approach and utilize\npast data for learning. These capabilities indicate that robots can play a vital role in\nautonomously navigating changing environments and resolving unexpected issues.\nMoreover, VLMs such as CLIP [7], which are trained to solve vision question answering\n(VQA) tasks, have the ability to process visual and linguistic information simultaneously.\nThis ability allows robots to visually perceive their surroundings and integrate this infor-\nmation into linguistic descriptions, enabling more sophisticated situational awareness. For\ninstance, using VLMs, a robot can recognize objects and provide descriptions, as well as\nunderstand and execute user commands based on visual cues. This integrated approach\nsigniﬁcantly enhances a robot’s autonomy and interaction capabilities.\nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which en-\nable low-level actuator control using LLMs and VLMs, Google has introduced AutoRT [10].\nAutoRT is a system where robots interact with real-world objects to collect motion data. ItAppl. Sci.2024, 14, 8868 3 of 39\nbegins by exploring the surrounding space to identify feasible tasks, then uses a VLM to\nunderstand the situation and an LLM to propose possible tasks. By inputting the robot’s\noperational guidelines and safety constraints into the LLM as prompts, AutoRT assesses\nthe validity of the proposed tasks and the necessity for human intervention. Throughout\nthis process, AutoRT safely selects and executes feasible tasks while collecting relevant\ndata.\nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for\nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical causal-\nity in the real world, problem-solving through trial-and-error feedback, and code generation\nabilities. Eureka can autonomously generate reward functions for a variety of tasks and\nrobots without needing speciﬁc templates for each. This allows for the generation of\nhuman-level reward functions for diverse robots and tasks without human input. Further-\nmore, Eureka has demonstrated the ability to solve complex problems that were previously\nunsolved by expert-designed reward functions.\nGiven these research outcomes, integrating language models into robotic intelligence\npresents signiﬁcant potential to enhance robot capabilities and applications dramatically,\nthereby redeﬁning their roles in diverse industries and everyday life. Therefore, this survey\npaper explores recent research trends in LLM- and VLM-based robot intelligence, aiming to\nprovide a comprehensive understanding of future development possibilities by examining\nthe application of language models in various robotic research ﬁelds. It also seeks to\nhighlight research cases, identify current limitations, and suggest future research directions.\nTo chronicle this advancement in robotics research ﬁelds, this review paper presents\nthe following contributions:\n• This paper summarizes and introduces the foundational elements and tuning methods\nof LLM architecture.\n• It explores and arranges prompt techniques to enhance the problem-solving abilities\nof LLMs."
  },
  {
    "question": "What are the two main components of Ha's framework for robot skill acquisition?",
    "chunk": "on the current image, command, and identiﬁed object data. Experimental use of real mobile\nmanipulation robots showed that MOO could adapt to new object types and environments\nin a zero-shot fashion. Moreover, MOO responded to non-verbal cues such as pointing at\nspeciﬁc objects, extending its scope to open-world exploration and manipulation.Appl. Sci.2024, 14, 8868 23 of 39\nExisting VLMs often lack a comprehensive understanding of physical concepts such\nas material and fragility, which limits their effectiveness in robotic manipulation tasks. To\naddress this issue, Gao [162] introduced PhysObjects, an object-centric dataset featuring\n39.6K crowd-sourced annotations and 417K automated annotations of physical concepts.\nThe automated annotations involved assigning speciﬁc concept values to predeﬁned object\ncategories or continuous concepts such as material and fragility. Fine-tuning a VLM on\nPhysObjects enhanced comprehension of physical concepts by capturing human biases\nrelated to the visual appearance of objects. Integrating this physically grounded VLM\nwith an LLM-based robotic planner framework improved performance in tasks requiring\nreasoning about physical concepts.\nThe traditional pre-training and ﬁne-tuning pipeline often suffers from decreased\nlearning efﬁciency and challenges in generalizing to unseen objects and tasks due to its\nreliance on domain-speciﬁc action information and domain-general visual information. To\naddress these limitations, Wang [163] proposed a modular approach named ProgramPort,\nwhich utilizes the syntactic and semantic structure of language instructions. Wang’s\nframework incorporated a semantic parser to reconstruct executable programs, composed of\nfunctional modules based on vision and action across multiple modalities. Each functional\nmodule combined deterministic computation with learnable neural networks. Program\nexecution involved generating parameters for general manipulation primitives used by the\nrobot’s end effector. The entire module network was trainable with an end-to-end imitation\nlearning objective. Experimental results demonstrated that the model effectively separated\naction and perception, achieving enhanced zero-shot and compositional generalization\nacross various manipulation tasks, speciﬁcally 16 tasks related to robot manipulation.\nHa [164] proposed a framework aimed at robot skill acquisition. This framework\nprovided a comprehensive solution by utilizing language guidance, without necessitating\nexpert demonstrations or reward speciﬁcation/engineering. It consisted of two main\ncomponents. The ﬁrst component, scaling up language-guided data generation, employed\nLLMs to break down tasks into subtasks and generate a hierarchical plan or task tree. This\nplan was materialized into various robot trajectories using 6-DoF exploration primitives.\nThese trajectories were subsequently veriﬁed and retries were performed as needed until\nsuccess was achieved. This approach enhanced the success rate of data collection and\nmore effectively mitigated the low-level understanding gap in LLMs by incorporating retry\nprocesses as part of the robot’s experiences. The second component, distilling down to\nlanguage-conditioned visuomotor policy, transformed robot experiences into a policy that\ndeduced control sequences from visual observations and natural language task descriptions.\nBy extending diffusion policies, this component handled language-based conditioning for\nmulti-task learning. To assess long-horizon behavior, commonsense reasoning, tool use,\nand intuitive physics, a new multi-task benchmark comprising 18 tasks related to robot\nmanipulation across ﬁve domains (mailbox, transport, drawer, catapult, and bus balance)\nwas developed. This benchmark effectively supported the learning of retry behaviors in\nthe data collection process and enhanced success rates.\nHuang [165], as shown in Figure12, aimed to synthesize dense robot trajectories,\nincluding 6-DoF end-effector waypoints, for various manipulation tasks using an open set"
  },
  {
    "question": "In what ways can robots adapt to new environments using language models?",
    "chunk": "mand systems. Consequently, robots can respond more adaptably and intelligently in\ninteractions with human users, allowing them to engage in complex problem-solving and\ndecision-making processes beyond simple mechanical tasks.\nAdditionally, LLMs not only enhance a robot’s communication skills to improve HRI\nusability but also boost the robot’s planning abilities. Planning involves setting goals\nand devising a sequence of actions to achieve them, which are essential in determining a\nrobot’s autonomy and efﬁciency. LLMs interpret natural language from users and complex\ncommands, enabling robots to establish and execute suitable plans in various situations.\nMoreover, LLMs adapt ﬂexibly to new situations through a zero-shot approach and utilize\npast data for learning. These capabilities indicate that robots can play a vital role in\nautonomously navigating changing environments and resolving unexpected issues.\nMoreover, VLMs such as CLIP [7], which are trained to solve vision question answering\n(VQA) tasks, have the ability to process visual and linguistic information simultaneously.\nThis ability allows robots to visually perceive their surroundings and integrate this infor-\nmation into linguistic descriptions, enabling more sophisticated situational awareness. For\ninstance, using VLMs, a robot can recognize objects and provide descriptions, as well as\nunderstand and execute user commands based on visual cues. This integrated approach\nsigniﬁcantly enhances a robot’s autonomy and interaction capabilities.\nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which en-\nable low-level actuator control using LLMs and VLMs, Google has introduced AutoRT [10].\nAutoRT is a system where robots interact with real-world objects to collect motion data. ItAppl. Sci.2024, 14, 8868 3 of 39\nbegins by exploring the surrounding space to identify feasible tasks, then uses a VLM to\nunderstand the situation and an LLM to propose possible tasks. By inputting the robot’s\noperational guidelines and safety constraints into the LLM as prompts, AutoRT assesses\nthe validity of the proposed tasks and the necessity for human intervention. Throughout\nthis process, AutoRT safely selects and executes feasible tasks while collecting relevant\ndata.\nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for\nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical causal-\nity in the real world, problem-solving through trial-and-error feedback, and code generation\nabilities. Eureka can autonomously generate reward functions for a variety of tasks and\nrobots without needing speciﬁc templates for each. This allows for the generation of\nhuman-level reward functions for diverse robots and tasks without human input. Further-\nmore, Eureka has demonstrated the ability to solve complex problems that were previously\nunsolved by expert-designed reward functions.\nGiven these research outcomes, integrating language models into robotic intelligence\npresents signiﬁcant potential to enhance robot capabilities and applications dramatically,\nthereby redeﬁning their roles in diverse industries and everyday life. Therefore, this survey\npaper explores recent research trends in LLM- and VLM-based robot intelligence, aiming to\nprovide a comprehensive understanding of future development possibilities by examining\nthe application of language models in various robotic research ﬁelds. It also seeks to\nhighlight research cases, identify current limitations, and suggest future research directions.\nTo chronicle this advancement in robotics research ﬁelds, this review paper presents\nthe following contributions:\n• This paper summarizes and introduces the foundational elements and tuning methods\nof LLM architecture.\n• It explores and arranges prompt techniques to enhance the problem-solving abilities\nof LLMs."
  },
  {
    "question": "Can robots autonomously navigate changing environments using LLMs?",
    "chunk": "of labeled data. This process is inherently resource-intensive. Moreover, these models\nare designed for a speciﬁc environment and require reconﬁguration whenever the task or\nenvironment changes. This renders the robots challenging to adapt and scale to disparate\nenvironments [3]. For practical robot systems, it is essential that they are able to ﬂexibly\nrespond to the ever-changing physical environment. From this perspective, the gener-\nalization of affordable tasks, environmental adaptability, and the accuracy of execution,\nplanning, and reasoning capabilities remain signiﬁcant challenges for traditional robotic\nintelligence systems [4].\nHowever, LLMs and VLMs help a robot intelligence system to enhance its general-\nization capability in dynamic and complex real-world environments. LLMs can leverage\npre-trained knowledge from extensive datasets to augment their ability to generalize to\neveryday tasks that are typically expected of robots. Unlike the conventional supervised\nmodels, LLMs can utilize zero-shot and few-shot learning to help robots quickly adapt\nto new environments without additional training [5]. This has the advantage of signiﬁ-\ncantly reducing the need for costly data collection and labeling. In addition, robot systems\nequipped with LLMs can process complex instructions based on their ability to understand\nand generate natural language, which can improve human–robot interactions. Furthermore,\nLLMs can be integrated with multimodal sensors such as LiDAR, depth, voice, tactile, pro-\nprioception, and visual information, which enables robots to comprehensively understand\nand adapt to their environment [6].\nLLMs have demonstrated exceptional capabilities in processing and understanding\ntext-based information, signiﬁcantly enhancing robotic communication abilities. For in-\nstance, robots can accurately comprehend and execute natural language commands via\nLLMs, providing scalability and ﬂexibility beyond traditional word-based robotic com-\nmand systems. Consequently, robots can respond more adaptably and intelligently in\ninteractions with human users, allowing them to engage in complex problem-solving and\ndecision-making processes beyond simple mechanical tasks.\nAdditionally, LLMs not only enhance a robot’s communication skills to improve HRI\nusability but also boost the robot’s planning abilities. Planning involves setting goals\nand devising a sequence of actions to achieve them, which are essential in determining a\nrobot’s autonomy and efﬁciency. LLMs interpret natural language from users and complex\ncommands, enabling robots to establish and execute suitable plans in various situations.\nMoreover, LLMs adapt ﬂexibly to new situations through a zero-shot approach and utilize\npast data for learning. These capabilities indicate that robots can play a vital role in\nautonomously navigating changing environments and resolving unexpected issues.\nMoreover, VLMs such as CLIP [7], which are trained to solve vision question answering\n(VQA) tasks, have the ability to process visual and linguistic information simultaneously.\nThis ability allows robots to visually perceive their surroundings and integrate this infor-\nmation into linguistic descriptions, enabling more sophisticated situational awareness. For\ninstance, using VLMs, a robot can recognize objects and provide descriptions, as well as\nunderstand and execute user commands based on visual cues. This integrated approach\nsigniﬁcantly enhances a robot’s autonomy and interaction capabilities.\nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which en-\nable low-level actuator control using LLMs and VLMs, Google has introduced AutoRT [10].\nAutoRT is a system where robots interact with real-world objects to collect motion data. ItAppl. Sci.2024, 14, 8868 3 of 39"
  },
  {
    "question": "What are the four strategies for parameter-efficient fine-tuning mentioned?",
    "chunk": "model focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM \nmodel inference. \n \nFigure 3. An overview of four strategies for parameter-eﬃcient ﬁne-tuning: (a) Adapter Tuning, \n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5]. \nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a \nlow-rank constraint on transformer layers to approximate the update matrices through \ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates \nthe parameter updates using low-rank dec omposition matrices. The primary bene ﬁt of \nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning, \nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory \nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning. \nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119]. \n3.3. Prompt Techniques for Increasing LLM Performance \nTo enhance the performance of LLMs, the most straightforward approach involves \ntraining with additional data via ﬁne-tuning techniques, which mirrors supervised learn-\ning in conventional machine learning. Another method for improving performance in-\nvolves the use of in-context learning, which capitalizes on prompts for zero-shot learning, \na capability ﬁrst observed in LLMs with the advent of GPT-3. The adaptation of these \nprompts for speciﬁc tasks is known as prompt engineering. Fundamentally, prompt engi-\nneering (or prompting) entails supplying inputs  to the model to perform a distinct task, \ndesigning the input format to encapsulate the task’s purpose and context, and generating \nthe desired output. The four components of pr ompt engineering can be analyzed as fol-\nlows: within the prompt, “ Instructions” delineate the speci ﬁc tasks or directives for the \nmodel and “Context” provides external or additional contextual information that can tune \nthe model. Furthermore, “Input data” refers to the type of input or questions seeking an-\nswers, and “ Output data” deﬁnes the output type or format within the prompt, thereby \noptimizing the LLM’s performance for particular tasks. Various methodologies for creat-\ning prompts have been introduced, as described below. \nFigure 3. An overview of four strategies for parameter-efﬁcient ﬁne-tuning: (a) Adapter Tuning,\n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5].\nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a\nlow-rank constraint on transformer layers to approximate the update matrices through\ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates\nthe parameter updates using low-rank decomposition matrices. The primary beneﬁt of\nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning,\nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory\nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning.\nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119].\n3.3. Prompt Techniques for Increasing LLM Performance"
  },
  {
    "question": "What are the main features of the Socratic model proposed by Zeng?",
    "chunk": "recovery feedback mechanism and environmental state feedback [156].\nRana [157] introduced SayPlan, a scalable method for large-scale task planning using\nLLMs and based on a 3D Scene Graph (3DSG) representation. SayPlan involved the LLM\nsearching a collapsed 3D scene graph and task instructions to identify all relevant items\nand then locating the subgraph that contained the necessary items to complete the task.\nThe identiﬁed subgraph was subsequently used by the LLM to generate a high-level plan\nthat addressed the navigational aspect of the task. This plan was formatted as a JSON\n3D scene graph and subjected to a repetitive replanning process through feedback from\nthe scene graph simulator and a set of API calls for manipulation and operation until an\nexecutable plan was determined. SayPlan was tested in two large-scale environments,\nfeaturing up to three ﬂoors, 36 rooms, and 140 assets and objects, proving its capability\nto ground large-scale and long-horizon task plans from abstract and natural language\ninstructions, thereby enabling a mobile manipulator robot to execute these tasks.\nZeng [158], as shown in Figure11, proposed the Socratic model (SM), a modular\nframework that synergistically utilizes various forms of knowledge and employs multiple\npre-trained models to exchange information and leverage new multimodal capabilities. SM\noperates without ﬁne-tuning by integrating diverse pre-trained models and functions in\na zero-shot approach (e.g., using multimodal prompts), which enables it to harness new\nmultimodal capabilities. SM demonstrated state-of-the-art performance in zero-shot image\ncaptioning and video-to-text retrieval, and it effectively answered free-form questions about\negocentric video. Additionally, it supported interactions with external APIs and databases\n(e.g., web search) for multimodal assistive dialogue, robot perception, and planning, among\nother novel applications.Appl. Sci.2024, 14, 8868 22 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 22 of 39 \n \nFigure 10. ProgPrompt is a system that uses Pyth on programming structures to provide \nenvironmental information and actions, enhancing the success rate of robot task planning through \nan error recovery feedback mechanism and environmental state feedback [156]. \nRana [157] introduced SayPlan, a scalable method for large-scale task planning using \nLLMs and based on a 3D Scene Graph (3DSG) representation. SayPlan involved the LLM \nsearching a collapsed 3D scene graph and task instructions to identify all relevant items \nand then locating the subgraph that contained the necessary items to complete the task. \nThe identiﬁed subgraph was subsequently used by the LLM to generate a high-level plan \nthat addressed the navigational aspect of the task. This plan was formatted as a JSON 3D \nscene graph and subjected to a repetitive replanning process through feedback from the \nscene graph simulator and a set of API calls for manipulation and operation until an \nexecutable plan was determined. SayPlan wa s tested in two large-scale environments, \nfeaturing up to three ﬂoors, 36 rooms, and 140 assets and objects, proving its capability to \nground large-scale and long-horizon task plans from abstract and natural language \ninstructions, thereby enabling a mobile manipulator robot to execute these tasks. \nZeng [158], as shown in Figure 11, proposed the Socratic model (SM), a modular \nframework that synergistically utilizes various forms of knowledge and employs multiple \npre-trained models to exchange information and leverage new multimodal capabilities."
  },
  {
    "question": "What are the five groups of research topics identified in the survey?",
    "chunk": "Toward General-Purpose Robots via Foundation\nModels: A Survey and Meta-Analysis Foundation Models [ 3]\nA Survey of Large Language Models LLM [ 5]\nVision-Language Models for Vision Tasks: A Survey VLM [ 12]\nLanguage-conditioned Learning for Robotic\nManipulation: A Survey LLM, VLM, Manipulation [ 13]\nFoundation Models in Robotics: Applications,\nChallenges, and the Future Foundation Models [ 14]\nVision-and-Language Navigation: A Survey of Tasks,\nMethods, and Future Directions VLN [ 15]\n2. Review Protocol\nThis survey covered four databases: Web of Science, ScienceDirect, IEEE Xplore, and\narXiv. In fact, many of the articles surveyed had not been peer-reviewed and published at\nthe time of our search because the subject matter was relatively recent. Therefore, a consid-\nerable number of articles reviewed in this survey were sourced from the arXiv database.\nThe selection process of this study primarily relied on two iterations:\n• The titles and abstracts of the articles were reviewed to eliminate duplicates and\nirrelevant articles.\n• The full texts of the selected articles from the ﬁrst iteration were thoroughly examined\nand categorized.\n• Article searching began on 18 September 2023.\nRegarding the search queries,\n• the publication years were those after 2020,\n• the keywords of Robotics and LLM, which were ((“Robotic” OR “Robotics”) AND\n(“LLM” OR “LM” OR “Large Language Model” OR “Language Model”)), and relevant\njournal and conference articles written in English were considered.\nFrom these search criteria, recent studies utilizing language models in robotics research\nwere expected to be collected. Our aim is to provide a robust understanding of how\nlanguage models and their variants have been utilized to enhance robot intelligence in\nthe literature.\nAll articles that met the above criteria were included in this review. Following an\nintensive survey of the abstracts of the selected articles, we categorized the research topics\ninto ﬁve groups: reward design for reinforcement learning, low-level control, high-level\nplanning, manipulation, and scene understanding. Figure1 illustrates these ﬁve categories.\nOur categorization was based on a thorough review of the sources in the literature. Subse-\nquently, duplicate articles were removed, and those not meeting the speciﬁed eligibility\ncriteria were excluded. The exclusion criteria included: (1) articles in languages other than\nEnglish and (2) articles discussing general concepts that do not focus on deep reinforcement\nlearning-based manipulation.\n3. Related Works\n3.1. Language Model\nZhao’s LLM review paper [5] categorizes the evolution of Language Models (LMs)\ninto four phases. The initial stage, the statistical language model (SLM) [16–19], utilizes\nmethods based on statistical learning techniques and the Markov assumption to construct\nword prediction models. A notable method from this phase is the n-gram language model,\nwhich predicts words based on a ﬁxed context length of n. Although SLMs have enhanced\nperformance in various domains such as information retrieval (IR) [16,20] and naturalAppl. Sci.2024, 14, 8868 6 of 39\nlanguage processing (NLP) [21–23], higher-order language models have encountered lim-\nitations due to the curse of dimensionality, which necessitates estimating exponentially\nincreasing transition probabilities.\nThe subsequent phase of LMs, termed neural language models (NLMs), leveraged\nneural networks such as multi-layer perceptron (MLP) and recurrent neural networks"
  },
  {
    "question": "What improvements were observed in VirtualHome tasks using these planning systems?",
    "chunk": "Figure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 9.LLM-Planner is a system that creates high-level plans based on natural language commands,\nsets subgoals to determine actions, and continuously updates the plan to reﬂect environmental\nchanges [155].\nSingh [156], as shown in Figure10, introduced ProgPrompt, a programmatic LLM\nprompt structure designed for generating plans across diverse situated environments,\nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that\nleveraged LLMs and included a Python programming structure to facilitate information\nabout the environment and executable actions. It featured a feedback mechanism, using\nexecutable program plan examples and assertion statements to mitigate errors, enhancing\ntask success rates. Additionally, ProgPrompt veriﬁed the current state through environ-\nmental feedback during plan execution and revised the plan accordingly. The results\nindicated that the integration of programming language features substantially improved\ntask performance in contexts such as VirtualHome and real-world manipulation tasks in\nterms of success rate, goal conditions recall, and executability.Appl. Sci.2024, 14, 8868 21 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 10.ProgPrompt is a system that uses Python programming structures to provide environ-\nmental information and actions, enhancing the success rate of robot task planning through an error"
  },
  {
    "question": "How does LLM-Grounder handle natural language queries in 3D environments?",
    "chunk": "Chen [168] explored methods to integrate commonsense into scene understanding\nusing LLMs and introduced three paradigms for classifying room types within indoor\nenvironments based on included objects. The zero-shot approach utilized a pre-trained\nlanguage model to identify the objects in a room and estimate their types. The feed-\nforward classiﬁer approach involved inputting sentences that listed a room’s objects into\nthe language model to generate embedding vectors, which were subsequently input into a\npre-trained shallow multilayer perceptron to predict each room type. Lastly, the classiﬁer\napproach embedded images of rooms alongside textual descriptions to identify the best-\nmatching description, thereby determining the room type. These paradigms demonstrated\nthe capacity to generalize to objects not presented in the training set and to make inferences\nwithin a space larger than that deﬁned by the trained object labels.\nYang [169] introduced the innovative zero-shot, open-vocabulary, LLM-based 3D\nvisual grounding pipeline called LLM-Grounder. This method breaks down complex natu-\nral language queries into semantic components and uses visual grounding tools such as\nOpenScene or LERF to locate objects within 3D scenes. Subsequently, the LLM evaluates\nspatial and commonsense relationships among these objects to achieve the ﬁnal grounding.\nRemarkably, LLM-Grounder operates without labeled training data and has proven its ca-\npacity to adapt to new 3D scenes and diverse text queries, enhancing grounding capabilities\nfor complex language queries and establishing itself as an effective solution.\nChen [170] developed NLMap, an open-vocabulary, queryable scene representation\nsystem. Designed to accumulate and incorporate contextual data within a scene repre-\nsentation for natural language queries, this system allows an LLM planner to visualize\nand query objects, thereby generating contextual plans. Initially, a VLM sets up a scene\nrepresentation for natural language queries; then, an LLM-based object suggestion module\nreviews instructions, suggests relevant objects, and queries the scene for object availability\nand location. Using this information, the LLM planner devises plans uniquely tailored to\nthe scene’s context. NLMap equips robots with the ability to function without a predeﬁned\ncatalog of objects or actions, overcoming the constraints of earlier methods and enabling\nmore adaptable operations in environments with novel or absent objects.\nElhafsi [171] introduced a monitoring framework that employed an LLM with superior\ncontextual understanding and reasoning capabilities to detect edge cases and anomalies\nwithin vision-based policies. This framework monitored the robot’s perception stream\nthrough an LLM-based module, designed to detect semantic anomalies that might occur\nduring operations. By converting the robot’s visual observations into textual descriptions\nat regular intervals and integrating these into LLM prompts, it could pinpoint factors\nleading to policy errors, unsafe behavior, or task confusion. The conversion of visual\ninformation into natural language descriptions used various techniques, without restriction\nto any speciﬁc method. This ﬂexibility enabled both fully end-to-end policies and classical\nautonomy stacks using learned perception to align more closely with human intuition.\nThe ﬁndings indicated that semantic anomalies did not always correspond to semantically\nexplainable failures, and end-to-end policies could sometimes behave unpredictably.\nHon [172] introduced a new model family named 3D-LLM, which incorporated 3D\nworld information into LLMs. The 3D-LLM model utilized 3D point clouds and their\nfeatures as input, enabling it to handle a variety of spatially aware 3D tasks. These tasks\nincluded 3D captioning, dense captioning, 3D question answering, task decomposition, 3D"
  },
  {
    "question": "How do robots use natural language and visual input for control commands?",
    "chunk": "more, Eureka has demonstrated the ability to solve complex problems that were previ-\nously unsolved by expert-designed reward functions. \nGiven these research outcomes, integrating language models into robotic intelligence \npresents signi ﬁcant potential to enhance robot capabilities and applications dramatically, \nthereby rede ﬁning their roles in diverse industries and everyday life. Therefore, this sur-\nvey paper explores recent research trends in LLM- and VLM-based robot intelligence, \naiming to provide a comprehensive understanding of future development possibilities by \nexamining the application of language models in various robotic research ﬁelds. It also \nseeks to highlight research cases, identify cu rrent limitations, and suggest future research \ndirections. \nTo chronicle this advancement in robotics research ﬁelds, this review paper presents \nthe following contributions: \n• This paper summarizes and introduces the foundational elements and tuning meth-\nods of LLM architecture. \n• It explores and arranges prompt techniques  to enhance the problem-solving abilities \nof LLMs. \n• It  reviews and encapsulates how LLMs and VLMs have been employed to augment \nrobot intelligence across ﬁve topics as shown in Figure 1: (1) reward design for rein-\nforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation, \nand (5) scene understanding. \n \nFigure 1. Five categories for robot intelligence with large language models in this study. \nFigure 1.Five categories for robot intelligence with large language models in this study.Appl. Sci.2024, 14, 8868 4 of 39\nThe reward design in RLcategory represents a research ﬁeld in which an LLM devel-\nops and enhances reward functions employed in reinforcement learning via code-based\ndescriptions and natural language input. This enables robots to learn optimal policies\nfor speciﬁc tasks through reinforcement learning, even in complex environments. The\nlow-level controlcategory includes a research area in which LLMs and VLMs generate\ncommand sequences that directly control the robot’s actuators through natural language\nand visual input. Thehigh-level planningcategory is a research area where the LLM\nidentiﬁes the present circumstances and objective of the tasks, subsequently developing an\nexplainable plan based on the reasoning required for problem-solving. In this research area,\nthe LLM is also tasked with developing the optimal robot behavior plan, which entails\nevaluating the feasibility of the established plan. In themanipulation category, the LLM in-\nterprets high-level instructions and the VLM (and LLM) analyzes various conditions based\non their understanding of the surroundings to assist robot arms in performing the speciﬁc\ntasks. While this category can be broadly included in the high-level planning category,\nthere are numerous studies that are speciﬁcally related to manipulation with a robot arm,\nwhich is why the manipulation category was separated. Thescene understandingcategory\nrepresents a research area that seeks to combine LLMs and VLMs with the objective of\nassisting robots in comprehending their surrounding environment. This is accomplished\nby identifying objects based on natural language instructions and visual information, as\nwell as by evaluating the relationships between them. This research area is also closely\nrelated to the ﬁeld of autonomous visual navigation. From a boarder perspective, there is an\noverlap between the scene understanding category and the perception-related components\nof the high-level planning category. However, in this review, the scene understanding\ncategory was considered a distinct category due to its prevalence as an application of\nVLM models.\nTable 1 lists resources that aid in understanding robot intelligence based on language"
  },
  {
    "question": "How does RT-2 utilize vision and language commands for robotic control?",
    "chunk": "than 700 tests collected over 17 months using 13 robots.\nA notable feature of RT-1 is its ability to enhance performance by learning from\ndata gathered from heterogeneous robots or simulations. In the study [8], the authors\nevaluated the performance of a model trained exclusively on data from the EveryDay\nRobot (EDR) against a model trained using data from both EDR and Kuka IIWA robots.\nThey recorded a 12% improvement in the bin-picking test. Another experiment compared\nmodel performance using data from real environments and simulations for items not\nencountered in actual settings. The ﬁndings indicated that incorporating simulation data in\nRT-1 training enhances performance over using purely real environment data, suggesting\nthat RT-1 can substantially improve model performance by integrating diverse data from\nrobots of varied morphologies or simulations while sustaining existing task capabilities.Appl. Sci.2024, 14, 8868 15 of 39\nRT-2 [9] is deﬁned as a vision-language-action (VLA) model that facilitates ﬁne-grained\ncontrol of robots through vision and language commands. RT-2 is ﬁne-tuned with robotic\ntrajectory data based on VLM models such as PaLM-E [140], which has 12 billion parameters\nand is trained on VQA data, alongside PaLI-X [141], which has parameter sizes ranging\nfrom 5 billion to 55 billion. The RT-2 system operates as an integrated closed-loop robotic\nsystem that combines low-level and high-level control policies. Despite not explicitly\nlearning certain capabilities during pre-training, RT-2 exhibits improved task performance\nvia real-world generalization involving diverse objects, visual scenes, and instructional\ncontexts. The paper [9] quantitatively assesses RT-2’s emergent capabilities in areas such as\nreasoning, symbol understanding, and human recognition. Furthermore, applying chain-\nof-thought prompting techniques to RT-2 has proven effective in solving more complex\nsemantic inference tasks, such as using a rock as an improvised hammer or offering an\nenergy drink instead of a carbonated beverage to a thirsty person. In comparison with\nthe earlier study on RT-1, RT-2 demonstrates enhanced performance in both familiar and\nnovel tasks.\nAutoRT [10] is a follow-up study based on the research results of RT-1 and RT-2,\nestablishing an orchestration of large-scale robotic agents for data collection in real-world\nscenarios. AutoRT employed 53 robots to gather 77,000 real robot episodes over seven\nmonths through both teleoperation and autonomous robot policies. At the heart of AutoRT\nis a robust foundation model that generates ‘task proposals’ based on given visual observa-\ntions. Notably, AutoRT introduces a ‘Robot Constitution’ using constitutional prompting to\nensure actions during the task proposal process do not compromise the safety of the robot\nor nearby individuals. This Robot Constitution, inspired by Asimov’s three laws [142],\ncomprises basic rules, safety rules that identify unsafe or unwanted tasks, and embodiment\nrules that clarify the robot’s operational boundaries.\nAutoRT enhances data collection by initially scanning the surroundings to identify\ninteresting scenes or tasks (exploration). It interprets the given context through a VLM\nand proposes potential tasks via an LLM (task generation). Subsequently, tasks suggested\nby the LLM are screened (affordance) to assess their feasibility and the need for human\nintervention, employing the Robot Constitution. During this procedure, viable tasks are\nchosen and performed, while pertinent data are gathered (data collection). The collected\ndata are then assessed for (diversity scoring) the visual diversity of the robot trajectories\nand the linguistic diversity of the language instructions generated by AutoRT (LLM). The"
  },
  {
    "question": "How do trainable prompt vectors work in prompt tuning?",
    "chunk": "through a better understanding of natural language commands. Conversely, alignment\ntuning (or preference alignment) seeks to align the behavior of LLMs with human values\nand preferences. Prominent methods include reinforcement learning from human feedbackAppl. Sci.2024, 14, 8868 9 of 39\n(RLHF) [110], which involves ﬁne-tuning LLMs using human feedback to better reﬂect\nhuman values, and direct preference optimization (DPO) [111], focusing on training with\npairs of human preferences that usually include an input prompt and the preferred and\nnon-preferred responses.\nFor both instruction tuning and alignment tuning, which involve training LLMs\nwith extensively large model parameters, substantial GPU memory and computational\nresources are required, with high costs typically incurred when utilizing cloud-based\nresources. Under these conditions, parameter-efﬁcient ﬁne-tuning (PEFT) offers a method\ndesigned to efﬁciently conduct ﬁne-tuning of such LLMs [112].\nAmong the methods of PEFT, there are four major approaches as shown in Figure3:\nadapter tuning, prompt tuning, preﬁx tuning, and low-rank adaptation (LoRA). Adapter\ntuning [113,114] involves integrating small neural network modules, known as adapters,\ninto the core components of a transformer model, speciﬁcally into the attention and feed-\nforward layers. These adapters are inserted serially following these layers, allowing ﬁne-\ntuning of only the adapter modules according to speciﬁc task goals, while the parameters\nof the original language model remain unchanged. Consequently, adapter tuning effec-\ntively reduces the number of trainable parameters. Additionally, prompt tuning [115,116]\ndiverges from adapter tuning by adding trainable prompt vectors to the input layer. Preﬁx\ntuning [117] entails appending a sequence of preﬁxes to each transformer layer of the\nlanguage model, which consists of trainable continuous vectors. During ﬁne-tuning, the\nmodel focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM\nmodel inference.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 9 of 39 \n \nresources. Under these conditions, parameter-eﬃcient ﬁne-tuning (PEFT) oﬀers a method \ndesigned to eﬃciently conduct ﬁne-tuning of such LLMs [112]. \nAmong the methods of PEFT, there are four major approaches as shown in Figure 3: \nadapter tuning, prompt tuning, preﬁx tuning, and low-rank adaptation (LoRA). Adapter \ntuning [113,114] involves integrating small neural network modules, known as adapters, \ninto the core components of a transformer model, speciﬁcally into the attention and feed-\nforward layers. These adapters are inserted serially following these layers, allowing ﬁne-\ntuning of only the adapter modules according to speciﬁc task goals, while the parameters \nof the original language model remain unchanged. Consequently, adapter tuning e ﬀec-\ntively reduces the number of trainable parameters. Additionally, prompt tuning [115,116] \ndiverges from adapter tuning by adding trainable prompt vectors to the input layer. Preﬁx \ntuning [117] entails appending a sequence of preﬁxes to each transformer layer of the lan-\nguage model, which consists of trainable continuous vectors. During ﬁne-tuning, the"
  },
  {
    "question": "What challenges does Instruct2Act address compared to the previous method?",
    "chunk": "LLM. This process allows the LLM to assess the robot’s current state and capabilities, \nultimately generating an interpretable action plan. SayCan was evaluated across 101 robot \ntasks, achieving an 84% plan success rate and a 74% execution success rate in a simulated \nkitchen environment. In a real kitchen setting, the plan success rate decreased slightly to \n81% and the execution success rate fell to 60%, demonstrating that the policy and value \nfunctions generalize well to real-world settings. \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM\nto generate 3D affordance and constraint maps and design robot trajectories without additional\ntraining [165].\nAhn [166] introduced a framework named SayCan, which integrates LLMs with rein-\nforcement learning value functions, enabling robots to follow high-level text instructions.\nSayCan comprises two primary components: Say, which uses an LLM for task-based\ndecision-making, and Can, which evaluates the feasibility of these decisions via reinforce-\nment learning. Say leverages task-based knowledge from the LLM and reinforcement\nlearning functionality to assess the feasibility of task execution by robots in real-world\nscenarios. The LLM determines the actions necessary to achieve high-level goals and\nevaluates the effectiveness of each action in fulﬁlling the instructions. Learned through re-\ninforcement learning, the affordance function estimates each action’s success probability in\nthe current state, conﬁrming the executability of actions proposed by the LLM. This process\nallows the LLM to assess the robot’s current state and capabilities, ultimately generating\nan interpretable action plan. SayCan was evaluated across 101 robot tasks, achieving an\n84% plan success rate and a 74% execution success rate in a simulated kitchen environment.\nIn a real kitchen setting, the plan success rate decreased slightly to 81% and the execution\nsuccess rate fell to 60%, demonstrating that the policy and value functions generalize well\nto real-world settings.\nHuang [167] introduced the Instruct2Act framework, which employs LLMs to se-\nquentially map multi-modality instructions to robot actions. The previous method, CaP,\ngenerated robot policy program code directly from in-context examples based on language\ninstructions. However, this approach was constrained by the capabilities of the generated\ncode and encountered difﬁculties with longer, more complex commands due to the required\nhigh precision of code. To overcome these limitations, Instruct2Act introduced a novel\nstrategy that used multi-modality models and LLMs to simultaneously address recognition,\ntask planning, and low-level control modules. Instruct2Act utilized the segment anything\nmodel for identifying potential objects in input images for multi-modality recognition\nand the CLIP model for object classiﬁcation. As a result, Instruct2Act developed an inte-\ngrated search system capable of managing various input modalities and instruction types,\nincluding both pure language instructions and combined language-visual instructions,\nfacilitating the integration of diverse instruction types into a uniﬁed architecture. Moreover,\nfor pointer-language instructions, the framework supported task segmentation based on\nthe user’s clicks.\n4.5. Scene Understanding in LLMs and VLMs\nTo address the VQA problem, robotics research increasingly uses pre-trained VLMs\nto derive high-level information from visual data. This method is advantageous for scene\nunderstanding as it helps determine affordances that describe the relationship between the\ncurrent state and the next action based on images from cameras. Related studies focus on\naspects of scene understanding.Appl. Sci.2024, 14, 8868 25 of 39"
  },
  {
    "question": "Can you clarify what 'input data' refers to in prompt engineering?",
    "chunk": "neering (or prompting) entails supplying inputs  to the model to perform a distinct task, \ndesigning the input format to encapsulate the task’s purpose and context, and generating \nthe desired output. The four components of pr ompt engineering can be analyzed as fol-\nlows: within the prompt, “ Instructions” delineate the speci ﬁc tasks or directives for the \nmodel and “Context” provides external or additional contextual information that can tune \nthe model. Furthermore, “Input data” refers to the type of input or questions seeking an-\nswers, and “ Output data” deﬁnes the output type or format within the prompt, thereby \noptimizing the LLM’s performance for particular tasks. Various methodologies for creat-\ning prompts have been introduced, as described below. \nFigure 3. An overview of four strategies for parameter-efﬁcient ﬁne-tuning: (a) Adapter Tuning,\n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5].\nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a\nlow-rank constraint on transformer layers to approximate the update matrices through\ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates\nthe parameter updates using low-rank decomposition matrices. The primary beneﬁt of\nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning,\nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory\nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning.\nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119].\n3.3. Prompt Techniques for Increasing LLM Performance\nTo enhance the performance of LLMs, the most straightforward approach involves\ntraining with additional data via ﬁne-tuning techniques, which mirrors supervised learning\nin conventional machine learning. Another method for improving performance involves the\nuse of in-context learning, which capitalizes on prompts for zero-shot learning, a capability\nﬁrst observed in LLMs with the advent of GPT-3. The adaptation of these prompts for\nspeciﬁc tasks is known as prompt engineering. Fundamentally, prompt engineering (or\nprompting) entails supplying inputs to the model to perform a distinct task, designing the\ninput format to encapsulate the task’s purpose and context, and generating the desired\noutput. The four components of prompt engineering can be analyzed as follows: within\nthe prompt, “Instructions” delineate the speciﬁc tasks or directives for the model andAppl. Sci.2024, 14, 8868 10 of 39\n“Context” provides external or additional contextual information that can tune the model.\nFurthermore, “Input data” refers to the type of input or questions seeking answers, and\n“Output data” deﬁnes the output type or format within the prompt, thereby optimizing the\nLLM’s performance for particular tasks. Various methodologies for creating prompts have\nbeen introduced, as described below.\nZero-shot prompting[53] is a technique that allows the model to take on new tasks\nwith no prior examples. The model relies solely on the task description or instructions with-\nout additional training. Likewise,few-shot prompting[49] introduces a small number of\nexamples to aid the model in learning new tasks. This approach does not require extensive\ndatasets and can improve the model’s performance through a limited set of examples.\nChain-of-thought (CoT) [41] is a technique that explicitly describes intermediate"
  },
  {
    "question": "Can you explain how visual features are encoded in EmbodiedGPT?",
    "chunk": "Appl. Sci. 2024, 14, x FOR PEER REVIEW 18 of 39 \n \ntime. EmbodiedGPT utilizes pre-trained visi on transformers and the LLaMA language \nmodel to encode visual features and map them  to the language modality. The generated \nplan was subsequently converted into speci ﬁc task commands using general visual \ntokens, encoded by the vision model. The framework’s functionality comprises (1) \nencoding current visual features, (2) mapping visual features to the language modality \nvia a ttention-based interactions between visual tokens and text queries or learnable \nembedded queries, (3) generating plans with the LLaMA language model and translating \nthem into speciﬁc task commands, and (4) querying the encoded visual tokens from the \nvision model and translating them into low-level control commands through a \ndownstream policy network for task execution. Experimental results, utilizing the MS-\nCOCO dataset, revealed that EmbodiedG PT excels in object recognition and \nunderstanding spatial relationships. Notabl y, implementing a closed-loop design and a \n“chain-of-thought” training mode signi ﬁcantly enhanced EmbodiedGPT’s performance. \nThese results demonstrate that EmbodiedGPT e ﬀectively handles various autonomous \ntasks, exhibiting superior capability in object recognition, understanding spatial \nrelationships, and generating logical, executable plans. \n \nFigure 7. After encoding visual features, they are mappe d using visual tokens and text queries. A \nplan is then created with the LLaMA model and turned into task commands. The visual tokens are \nqueried and converted into low-level control commands to perform the task [150]. \nChen [151] introduced the language-model-based commonsense reasoning (LMCR) \nframework to assist robots in comprehendin g incomplete natural language instructions. \nThis framework enabled robots to receive instructions in natural language from humans, \nobserve their surroundings, and employ a commonsense reasoning method to \nautonomously infer missing information. LMCR utilized a model of commonsense \nreasoning learned from web-based text materials, allowing robots to understand \nincomplete instructions and autonomously execute tasks. The framework comprised three \nmain functions: language understanding, commonsense reasoning, and action planning. \nIn language understanding, LMCR translated human natural language instructions into a \nform interpretable by robots, parsing them into verb frames to convert them into \nexecutable structures. During the common sense reasoning phase, the robot analyzed \nsurrounding objects and employed a language model trained on large-scale unstructured \ntext materials to ﬁll in the missing details from the instructions. This model identiﬁed the \nFigure 7.After encoding visual features, they are mapped using visual tokens and text queries. A\nplan is then created with the LLaMA model and turned into task commands. The visual tokens are\nqueried and converted into low-level control commands to perform the task [150].\nChen [151] introduced the language-model-based commonsense reasoning (LMCR)\nframework to assist robots in comprehending incomplete natural language instructions.\nThis framework enabled robots to receive instructions in natural language from humans, ob-\nserve their surroundings, and employ a commonsense reasoning method to autonomously\ninfer missing information. LMCR utilized a model of commonsense reasoning learned\nfrom web-based text materials, allowing robots to understand incomplete instructions and\nautonomously execute tasks. The framework comprised three main functions: language\nunderstanding, commonsense reasoning, and action planning. In language understanding,\nLMCR translated human natural language instructions into a form interpretable by robots,"
  },
  {
    "question": "How do LLMs help robots execute complex commands?",
    "chunk": "of labeled data. This process is inherently resource-intensive. Moreover, these models\nare designed for a speciﬁc environment and require reconﬁguration whenever the task or\nenvironment changes. This renders the robots challenging to adapt and scale to disparate\nenvironments [3]. For practical robot systems, it is essential that they are able to ﬂexibly\nrespond to the ever-changing physical environment. From this perspective, the gener-\nalization of affordable tasks, environmental adaptability, and the accuracy of execution,\nplanning, and reasoning capabilities remain signiﬁcant challenges for traditional robotic\nintelligence systems [4].\nHowever, LLMs and VLMs help a robot intelligence system to enhance its general-\nization capability in dynamic and complex real-world environments. LLMs can leverage\npre-trained knowledge from extensive datasets to augment their ability to generalize to\neveryday tasks that are typically expected of robots. Unlike the conventional supervised\nmodels, LLMs can utilize zero-shot and few-shot learning to help robots quickly adapt\nto new environments without additional training [5]. This has the advantage of signiﬁ-\ncantly reducing the need for costly data collection and labeling. In addition, robot systems\nequipped with LLMs can process complex instructions based on their ability to understand\nand generate natural language, which can improve human–robot interactions. Furthermore,\nLLMs can be integrated with multimodal sensors such as LiDAR, depth, voice, tactile, pro-\nprioception, and visual information, which enables robots to comprehensively understand\nand adapt to their environment [6].\nLLMs have demonstrated exceptional capabilities in processing and understanding\ntext-based information, signiﬁcantly enhancing robotic communication abilities. For in-\nstance, robots can accurately comprehend and execute natural language commands via\nLLMs, providing scalability and ﬂexibility beyond traditional word-based robotic com-\nmand systems. Consequently, robots can respond more adaptably and intelligently in\ninteractions with human users, allowing them to engage in complex problem-solving and\ndecision-making processes beyond simple mechanical tasks.\nAdditionally, LLMs not only enhance a robot’s communication skills to improve HRI\nusability but also boost the robot’s planning abilities. Planning involves setting goals\nand devising a sequence of actions to achieve them, which are essential in determining a\nrobot’s autonomy and efﬁciency. LLMs interpret natural language from users and complex\ncommands, enabling robots to establish and execute suitable plans in various situations.\nMoreover, LLMs adapt ﬂexibly to new situations through a zero-shot approach and utilize\npast data for learning. These capabilities indicate that robots can play a vital role in\nautonomously navigating changing environments and resolving unexpected issues.\nMoreover, VLMs such as CLIP [7], which are trained to solve vision question answering\n(VQA) tasks, have the ability to process visual and linguistic information simultaneously.\nThis ability allows robots to visually perceive their surroundings and integrate this infor-\nmation into linguistic descriptions, enabling more sophisticated situational awareness. For\ninstance, using VLMs, a robot can recognize objects and provide descriptions, as well as\nunderstand and execute user commands based on visual cues. This integrated approach\nsigniﬁcantly enhances a robot’s autonomy and interaction capabilities.\nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which en-\nable low-level actuator control using LLMs and VLMs, Google has introduced AutoRT [10].\nAutoRT is a system where robots interact with real-world objects to collect motion data. ItAppl. Sci.2024, 14, 8868 3 of 39"
  },
  {
    "question": "Can you explain how DrEureka helps in real-world robot applications?",
    "chunk": "main randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 5.DrEureka leverages LLM to design reward functions and solves the sim-to-real problem\nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134].\nXie [136] introducedText2Reward, a framework that automatically generated dense\nreward functions for reinforcement learning using LLMs. Provided with a goal expressed\nin natural language, Text2Reward produced executable dense reward functions derived\nfrom a compact representation of the environment. This framework generated free-form\ndense reward codes and delivered performance comparable to or surpassing that of policies\ntrained with expert-designed codes across a variety of tasks, including 17 manipulator-\nrelated tasks and six novel locomotion behaviors. Additionally, Text2Reward incorporated\nuser feedback to iteratively enhance the generated reward functions, thereby increasing the\nsuccess rate of the learned policies.\nDi Palo [137] explored the use of LLMs and VLMs to improve reinforcement learning\nagents’ understanding of human intentions. They developed a framework that utilized\nlanguage as a primary inference tool, investigating how it could address key challengesAppl. Sci.2024, 14, 8868 14 of 39\nin reinforcement learning, such as efﬁcient exploration, data reuse in experience, skill\nscheduling, and observational learning. This framework employed LLMs and VLMs to\naddress these reinforcement learning challenges by (1) efﬁciently exploring environments\nwith sparse rewards, (2) reusing collected data to sequentially bootstrap the learning of\nnew tasks, (3) scheduling learned skills for novel tasks, and (4) acquiring knowledge from\nobserving expert agents.\nDu [138] developed success detectors that identiﬁed whether actions or tasks were\nsuccessfully completed, utilizing the large multimodal language model Flamingo and\nhuman reward annotations. The study on success detection spanned three distinct do-\nmains: (1) interactive language-conditioned agents in simulated households, (2) real-world\nrobotic manipulation tasks (inserting and removing small, medium, and large gears), and\n(3) “in-the-wild” human egocentric videos. These success detectors adapted to new lan-\nguage instructions and visual changes using VLMs such as Flamingo, which were trained\non a broad range of language and visual data. Furthermore, success detection was reframed\nas a VQA problem, enabling the tracking of task progress through multiple frames to ascer-\ntain whether tasks had been successfully completed. The proposed method proved to be\nmore accurate in detecting success compared to custom reward models in the ﬁrst two do-\nmains, even with new language instructions or visual changes. However, success detection\nin unseen real-world videos in the third domain posed a more challenging generalization\ntask, underscoring the need for additional research."
  },
  {
    "question": "What is SayPlan and how does it work for task planning?",
    "chunk": "recovery feedback mechanism and environmental state feedback [156].\nRana [157] introduced SayPlan, a scalable method for large-scale task planning using\nLLMs and based on a 3D Scene Graph (3DSG) representation. SayPlan involved the LLM\nsearching a collapsed 3D scene graph and task instructions to identify all relevant items\nand then locating the subgraph that contained the necessary items to complete the task.\nThe identiﬁed subgraph was subsequently used by the LLM to generate a high-level plan\nthat addressed the navigational aspect of the task. This plan was formatted as a JSON\n3D scene graph and subjected to a repetitive replanning process through feedback from\nthe scene graph simulator and a set of API calls for manipulation and operation until an\nexecutable plan was determined. SayPlan was tested in two large-scale environments,\nfeaturing up to three ﬂoors, 36 rooms, and 140 assets and objects, proving its capability\nto ground large-scale and long-horizon task plans from abstract and natural language\ninstructions, thereby enabling a mobile manipulator robot to execute these tasks.\nZeng [158], as shown in Figure11, proposed the Socratic model (SM), a modular\nframework that synergistically utilizes various forms of knowledge and employs multiple\npre-trained models to exchange information and leverage new multimodal capabilities. SM\noperates without ﬁne-tuning by integrating diverse pre-trained models and functions in\na zero-shot approach (e.g., using multimodal prompts), which enables it to harness new\nmultimodal capabilities. SM demonstrated state-of-the-art performance in zero-shot image\ncaptioning and video-to-text retrieval, and it effectively answered free-form questions about\negocentric video. Additionally, it supported interactions with external APIs and databases\n(e.g., web search) for multimodal assistive dialogue, robot perception, and planning, among\nother novel applications.Appl. Sci.2024, 14, 8868 22 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 22 of 39 \n \nFigure 10. ProgPrompt is a system that uses Pyth on programming structures to provide \nenvironmental information and actions, enhancing the success rate of robot task planning through \nan error recovery feedback mechanism and environmental state feedback [156]. \nRana [157] introduced SayPlan, a scalable method for large-scale task planning using \nLLMs and based on a 3D Scene Graph (3DSG) representation. SayPlan involved the LLM \nsearching a collapsed 3D scene graph and task instructions to identify all relevant items \nand then locating the subgraph that contained the necessary items to complete the task. \nThe identiﬁed subgraph was subsequently used by the LLM to generate a high-level plan \nthat addressed the navigational aspect of the task. This plan was formatted as a JSON 3D \nscene graph and subjected to a repetitive replanning process through feedback from the \nscene graph simulator and a set of API calls for manipulation and operation until an \nexecutable plan was determined. SayPlan wa s tested in two large-scale environments, \nfeaturing up to three ﬂoors, 36 rooms, and 140 assets and objects, proving its capability to \nground large-scale and long-horizon task plans from abstract and natural language \ninstructions, thereby enabling a mobile manipulator robot to execute these tasks. \nZeng [158], as shown in Figure 11, proposed the Socratic model (SM), a modular \nframework that synergistically utilizes various forms of knowledge and employs multiple \npre-trained models to exchange information and leverage new multimodal capabilities."
  },
  {
    "question": "How can robots generate obstacle maps in real-time during navigation?",
    "chunk": "within the environment. During this process, the robot utilized a graph search algorithm to\ndetermine optimal trajectories and to navigate along these paths in the real world. This\nmethod demonstrated LM-Nav’s ability to perform long-horizon navigation in complex\noutdoor environments using natural language instructions.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 27 of 39 \n \n \nFigure 13. LM-Nav uses three pre-trained models: ( a) VNM builds a topological graph from \nobservations, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, \n(d) A graph search algorithm then ﬁnds the best robot trajectory, and ( e) the robot executes the \nplanned path [173]. \nZhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow \ninstructions. NavGPT is a vision-language na vigation system that employs an LLM to \ntranslate visual inputs from a visual foundation model (VFM) into natural language. The \nLLM then interprets the current state and makes informed decisions to reach the intended \ngoal, based on these converted visuals, naviga tion history, and potential future routes. \nNavGPT conducts various functions, incl uding high-level planning, decomposing \ninstructions into sub-goals, identifying landmarks in observed scenes, monitoring \nnavigation progress, and modifying plans as necessary. Although NavGPT’s performance \non zero-shot tasks from the R2R dataset has not yet matched that of trained models, it \nunderscored the potential of utilizing multi-modality inputs with LLMs for visual \nnavigation and tapping into the explicit reasoning capabilities of LLMs to enhance learned \nmodels. \nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-\ntrained vision-language features with a 3D reconstruction of the physical world. VLMaps, \nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary \nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands. \nThese commands can be directly localized on a map and generate new obstacle maps in \nreal-time, facilitated by sharing among various robot types. Extensive experiments \nconducted in both simulated environments (using the Habitat simulator with the \nMatterport3D dataset and the AI2THOR simulator) and real-world settings (with the HSR \nmobile robot for indoor navigation) demonstrated that VLMs can navigate based on more \ncomplex language instructions than previous methods. The reviewed papers in this study \nare summarized in Table 5. \nTable 5. Summary of the reviewed papers in this study. \nName Explanation Ref. \nReward Design in \nRL \n• Eureka automatically generates and im proves reward functions based on the \nvirtual environment source code.$• Dr Eureka builds reward-aware physics \npriors using Eureka and supports eﬀective operation in the real world through \ndomain randomization.$• LLMs design and re ﬁne reward functions based on \nnatural language input.$• LLMs and VLMs integrate multimodal data to \ngenerate reward functions. \n[11,134,136–139,176–\n180] \nFigure 13.LM-Nav uses three pre-trained models: (a) VNM builds a topological graph from observa-\ntions, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, (d)A\ngraph search algorithm then ﬁnds the best robot trajectory, and (e) the robot executes the planned\npath [173]."
  },
  {
    "question": "What are the benefits of using automatic reasoning and tool-use?",
    "chunk": "demonstrations and calls on external tools as necessary to integrate their outputs into the\nreasoning process. The model generalizes from demonstrations using tools to decompose\nnew tasks and learns to use tools effectively. Enhancing ART’s performance is possible\nby modifying the task library or incorporating new tools.Automatic prompt engineer\n(APE) [129] is a framework designed for the automatic generation and selection of com-\nmands. The model generates command candidates for a problem and selects the most\nsuitable one based on a scoring function, such as execution accuracy or log probability.\nDirectional stimulus prompting[130] is a technique that directs the model to consider\nand generate responses in a particular direction. By deploying a tunable policy LM (e.g.,\nT5 [47]), it creates directional stimulus prompts for each input and uses these as cues to\nsteer the model toward producing the desired outcomes [131]. ReAct combines reasoning\nwith action within the model. It enables the model to perform reasoning in generating\nanswers, take actions based on external sources (e.g., documents, articles, and news), and\nreﬁne reasoning based on observations of these actions. This process facilitates the creation,\nmaintenance, and modiﬁcation of action plans while incorporating additional information\nfrom interactions with external sources.Reﬂexion [132] augments language-based agents\nwith language feedback. Reﬂexion involves three models: the actor, the evaluator, and self-\nreﬂection. The actor initiates actions within a speciﬁc environment to generate task steps,\nthe evaluator assesses these steps, and self-reﬂection provides linguistic feedback, which\nthe actor uses to formulate new steps and achieve the task’s objective. The introduced\nprompt techniques are summarized in Table4.\nTable 4.Prompt Techniques.\nName Explanation Ref.\nZero-Shot Prompting Enabling the model to perform new tasks without any examples [ 53]\nFew-Shot Prompting Providing a few examples to enable performing new tasks [ 49]\nChain-of-Thought Explicitly generating intermediate reasoning steps to perform\nstep-by-step inference [41]\nSelf-Consistency\nGenerating various reasoning paths independently through\nFew-Shot CoT, with each path going through a prompt generation\nprocess to select the most consistent answer\n[120]\nGenerated Knowledge Prompting\nIntegrating knowledge and information relevant to a question, and\nthen providing it along with the question to generate accurate\nanswers\n[126]\nPrompt Chaining Dividing a task into sub-tasks and connecting prompts for each\nsub-task as input–output pairs [125]\nTree of Thoughts\nDividing a problem into subproblems with intermediate steps that\nserve as “thoughts” towards solving the problem, where each\nthought undergoes an inference process and self-evaluates its\nprogress towards solving the problem\n[124]\nRetrieval Augmented Generation Combining external information retrieval with natural language\ngeneration [127]\nAutomatic Reasoning and Tool-use Using external tools to automatically generate intermediate\nreasoning steps [128]\nAutomatic Prompt Engineer Automatically generating and selecting commands [ 129]\nActive Prompt Addressing the issue that the effectiveness may be limited by\nhuman annotations [122]\nDirectional Stimulus Prompting Guiding the model to think and generate responses in a speciﬁc\ndirection [130]\nProgram-Aided Language Models Using models to understand natural language problems and\ngenerate programs as intermediate reasoning steps [123]\nReAct Combining reasoning and actions within a mode [ 131]\nReﬂexion Enhancing language-based agents through language feedback [ 132]"
  },
  {
    "question": "What role do landmarks play in the navigation process of NavGPT?",
    "chunk": "Zhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow\ninstructions. NavGPT is a vision-language navigation system that employs an LLM to trans-\nlate visual inputs from a visual foundation model (VFM) into natural language. The LLM\nthen interprets the current state and makes informed decisions to reach the intended goal,\nbased on these converted visuals, navigation history, and potential future routes. NavGPT\nconducts various functions, including high-level planning, decomposing instructions into\nsub-goals, identifying landmarks in observed scenes, monitoring navigation progress, and\nmodifying plans as necessary. Although NavGPT’s performance on zero-shot tasks from\nthe R2R dataset has not yet matched that of trained models, it underscored the potential\nof utilizing multi-modality inputs with LLMs for visual navigation and tapping into the\nexplicit reasoning capabilities of LLMs to enhance learned models.\nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-\ntrained vision-language features with a 3D reconstruction of the physical world. VLMaps,\nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary\nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands.\nThese commands can be directly localized on a map and generate new obstacle maps in real-Appl. Sci.2024, 14, 8868 27 of 39\ntime, facilitated by sharing among various robot types. Extensive experiments conducted in\nboth simulated environments (using the Habitat simulator with the Matterport3D dataset\nand the AI2THOR simulator) and real-world settings (with the HSR mobile robot for\nindoor navigation) demonstrated that VLMs can navigate based on more complex language\ninstructions than previous methods. The reviewed papers in this study are summarized in\nTable 5.\nTable 5.Summary of the reviewed papers in this study.\nName Explanation Ref.\nReward Design in RL\n• Eureka automatically generates and improves reward\nfunctions based on the virtual environment source\ncode.\n• DrEureka builds reward-aware physics priors using\nEureka and supports effective operation in the real\nworld through domain randomization.\n• LLMs design and reﬁne reward functions based on\nnatural language input.\n• LLMs and VLMs integrate multimodal data to\ngenerate reward functions.\n[11,134,136–139,176–180]\nLow-level\nControl\n• Generating commands to control actuators capable of\nlow-level control.\n• RT-1 and RT-2 enable robots to perform complex tasks\nbased on language-vision data.\n• AutoRT establishes a system where robots can\nautonomously collect and utilize data.\n[8–10,144–148,181–183]\nHigh-level\nPlanning\n• LLMs provide an effective methodology for tasks\nrelated to high-level planning within robotic systems.\n• By using natural language, LLMs can formulate plans\nto solve tasks that require long-horizon reasoning.\n• LLMs assess the feasibility of actions to determine and\nexecute the optimal robotic behavior.\n• LLMs generate behavior trees to structure complex\nrobotic actions accurately.\n[149–160,184–207]\nManipulation\n• Using LLMs and VLMs to integrate language and\nvision data allows various manipulations.\n• LLMs interpret high-level instructions to generate the\nnecessary robot actions and assess their feasibility.\n• VLMs extract object information from images to assist\nin performing manipulations.\n[161–167,208–215]\nScene\nUnderstanding\n• To solve VQA problems, use VLMs to extract\nhigh-level information from vision data."
  },
  {
    "question": "How are large language models changing the way robots interact with people?",
    "chunk": "4o [1] have signiﬁcantly altered the ﬁeld of robotic AI research. These LLMs, trained on\nvast amounts of textual data, have shown excellent performance in enabling robots to\ncommunicate with humans more naturally and efﬁciently. Moreover, beyond the impacts\non human–robot interaction (HRI), there is ongoing research aimed at surpassing the\nlimitations of traditional low-level robot control techniques and planning algorithms by\nutilizing the high-level situational awareness and knowledge-based planning capabilities\nof LLMs. Notably, the programming capabilities of ChatGPT in the research presented by\nMicrosoft’s ChatGPT for Robotics [2] have introduced a new paradigm for applying LLMs\nin the robotics ﬁeld.\nThe goal of robot intelligence is to enable robots to operate autonomously in complex\nenvironments, interact naturally with humans, and make high-level decisions. To promote\nAppl. Sci.2024, 14, 8868.https://doi.org/10.3390/app14198868 https://www.mdpi.com/journal/applsciAppl. Sci.2024, 14, 8868 2 of 39\nadvancements in robot intelligence, the adoption of foundation models, such as LLMs and\nvision-language models (VLMs), which boast large parameter scales and pre-training on\nmassive datasets, is accelerating. These foundation models can perform various tasks, such\nas complex language understanding and generation and visual perception, enabling robots\nto engage with their environment in a more human-like manner.\nWhile traditional robot intelligence systems are highly effective in structured and pre-\ndictable environments, they are signiﬁcantly limited in their ability to adapt to dynamically\nchanging and complex real-world scenarios. In general, the intelligence models used in\nthese robotic systems are based on supervised learning, which requires large amounts\nof labeled data. This process is inherently resource-intensive. Moreover, these models\nare designed for a speciﬁc environment and require reconﬁguration whenever the task or\nenvironment changes. This renders the robots challenging to adapt and scale to disparate\nenvironments [3]. For practical robot systems, it is essential that they are able to ﬂexibly\nrespond to the ever-changing physical environment. From this perspective, the gener-\nalization of affordable tasks, environmental adaptability, and the accuracy of execution,\nplanning, and reasoning capabilities remain signiﬁcant challenges for traditional robotic\nintelligence systems [4].\nHowever, LLMs and VLMs help a robot intelligence system to enhance its general-\nization capability in dynamic and complex real-world environments. LLMs can leverage\npre-trained knowledge from extensive datasets to augment their ability to generalize to\neveryday tasks that are typically expected of robots. Unlike the conventional supervised\nmodels, LLMs can utilize zero-shot and few-shot learning to help robots quickly adapt\nto new environments without additional training [5]. This has the advantage of signiﬁ-\ncantly reducing the need for costly data collection and labeling. In addition, robot systems\nequipped with LLMs can process complex instructions based on their ability to understand\nand generate natural language, which can improve human–robot interactions. Furthermore,\nLLMs can be integrated with multimodal sensors such as LiDAR, depth, voice, tactile, pro-\nprioception, and visual information, which enables robots to comprehensively understand\nand adapt to their environment [6].\nLLMs have demonstrated exceptional capabilities in processing and understanding\ntext-based information, signiﬁcantly enhancing robotic communication abilities. For in-\nstance, robots can accurately comprehend and execute natural language commands via\nLLMs, providing scalability and ﬂexibility beyond traditional word-based robotic com-"
  },
  {
    "question": "Can you explain the feedback mechanisms used in ProgPrompt for error recovery?",
    "chunk": "other novel applications.Appl. Sci.2024, 14, 8868 22 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 22 of 39 \n \nFigure 10. ProgPrompt is a system that uses Pyth on programming structures to provide \nenvironmental information and actions, enhancing the success rate of robot task planning through \nan error recovery feedback mechanism and environmental state feedback [156]. \nRana [157] introduced SayPlan, a scalable method for large-scale task planning using \nLLMs and based on a 3D Scene Graph (3DSG) representation. SayPlan involved the LLM \nsearching a collapsed 3D scene graph and task instructions to identify all relevant items \nand then locating the subgraph that contained the necessary items to complete the task. \nThe identiﬁed subgraph was subsequently used by the LLM to generate a high-level plan \nthat addressed the navigational aspect of the task. This plan was formatted as a JSON 3D \nscene graph and subjected to a repetitive replanning process through feedback from the \nscene graph simulator and a set of API calls for manipulation and operation until an \nexecutable plan was determined. SayPlan wa s tested in two large-scale environments, \nfeaturing up to three ﬂoors, 36 rooms, and 140 assets and objects, proving its capability to \nground large-scale and long-horizon task plans from abstract and natural language \ninstructions, thereby enabling a mobile manipulator robot to execute these tasks. \nZeng [158], as shown in Figure 11, proposed the Socratic model (SM), a modular \nframework that synergistically utilizes various forms of knowledge and employs multiple \npre-trained models to exchange information and leverage new multimodal capabilities. \nSM operates without ﬁne-tuning by integrating diverse pre-trained models and functions \nin a zero-shot approach (e.g., using multimodal prompts), which enables it to harness new \nmultimodal capabilities. SM demonstrated state-of-the-art performance in zero-shot \nimage captioning and video-to-text retrieval, and it e ﬀectively answered free-form \nquestions about egocentric vi deo. Additionally, it supported interactions with external \nAPIs and databases (e.g., web search) for multimodal assistive dialogue, robot perception, \nand planning, among other novel applications. \n \nFigure 11. SM integrates various types of knowledge by using multiple pre-trained models and \nprovides meaningful results even in complex computer vision tasks such as image captioning, \ncontext inference, and activity prediction [158]. \nLin [159] introduced Text2Motion, a language-based framework designed to handle \nsequential manipulation tasks that requ ire long-horizon reasoning. Text2Motion \ninterpreted natural language instructions to formulate task plans and generated multiple \ncandidate skill sequences, evaluating the ge ometric feasibility of each sequence. By \nemploying a greedy search strategy, it selected  the optimal skill sequence to verify and \nexecute the ﬁnal plan. This method enabled Text2Motion to perform complex sequential \nmanipulation tasks with a higher success rate compared to existing language-based \nplanning methods, such as Saycan-gs an d Innermono-gs, and provided semantically \ngeneralized characteristics among skills with geometric relationships. \nFigure 11. SM integrates various types of knowledge by using multiple pre-trained models and\nprovides meaningful results even in complex computer vision tasks such as image captioning, context\ninference, and activity prediction [158].\nLin [159] introduced Text2Motion, a language-based framework designed to handle"
  },
  {
    "question": "What advancements have been made in robotic intelligence through language models?",
    "chunk": "more, Eureka has demonstrated the ability to solve complex problems that were previ-\nously unsolved by expert-designed reward functions. \nGiven these research outcomes, integrating language models into robotic intelligence \npresents signi ﬁcant potential to enhance robot capabilities and applications dramatically, \nthereby rede ﬁning their roles in diverse industries and everyday life. Therefore, this sur-\nvey paper explores recent research trends in LLM- and VLM-based robot intelligence, \naiming to provide a comprehensive understanding of future development possibilities by \nexamining the application of language models in various robotic research ﬁelds. It also \nseeks to highlight research cases, identify cu rrent limitations, and suggest future research \ndirections. \nTo chronicle this advancement in robotics research ﬁelds, this review paper presents \nthe following contributions: \n• This paper summarizes and introduces the foundational elements and tuning meth-\nods of LLM architecture. \n• It explores and arranges prompt techniques  to enhance the problem-solving abilities \nof LLMs. \n• It  reviews and encapsulates how LLMs and VLMs have been employed to augment \nrobot intelligence across ﬁve topics as shown in Figure 1: (1) reward design for rein-\nforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation, \nand (5) scene understanding. \n \nFigure 1. Five categories for robot intelligence with large language models in this study. \nFigure 1.Five categories for robot intelligence with large language models in this study.Appl. Sci.2024, 14, 8868 4 of 39\nThe reward design in RLcategory represents a research ﬁeld in which an LLM devel-\nops and enhances reward functions employed in reinforcement learning via code-based\ndescriptions and natural language input. This enables robots to learn optimal policies\nfor speciﬁc tasks through reinforcement learning, even in complex environments. The\nlow-level controlcategory includes a research area in which LLMs and VLMs generate\ncommand sequences that directly control the robot’s actuators through natural language\nand visual input. Thehigh-level planningcategory is a research area where the LLM\nidentiﬁes the present circumstances and objective of the tasks, subsequently developing an\nexplainable plan based on the reasoning required for problem-solving. In this research area,\nthe LLM is also tasked with developing the optimal robot behavior plan, which entails\nevaluating the feasibility of the established plan. In themanipulation category, the LLM in-\nterprets high-level instructions and the VLM (and LLM) analyzes various conditions based\non their understanding of the surroundings to assist robot arms in performing the speciﬁc\ntasks. While this category can be broadly included in the high-level planning category,\nthere are numerous studies that are speciﬁcally related to manipulation with a robot arm,\nwhich is why the manipulation category was separated. Thescene understandingcategory\nrepresents a research area that seeks to combine LLMs and VLMs with the objective of\nassisting robots in comprehending their surrounding environment. This is accomplished\nby identifying objects based on natural language instructions and visual information, as\nwell as by evaluating the relationships between them. This research area is also closely\nrelated to the ﬁeld of autonomous visual navigation. From a boarder perspective, there is an\noverlap between the scene understanding category and the perception-related components\nof the high-level planning category. However, in this review, the scene understanding\ncategory was considered a distinct category due to its prevalence as an application of\nVLM models.\nTable 1 lists resources that aid in understanding robot intelligence based on language"
  },
  {
    "question": "How does EmbodiedGPT map visual features to language?",
    "chunk": "Appl. Sci. 2024, 14, x FOR PEER REVIEW 18 of 39 \n \ntime. EmbodiedGPT utilizes pre-trained visi on transformers and the LLaMA language \nmodel to encode visual features and map them  to the language modality. The generated \nplan was subsequently converted into speci ﬁc task commands using general visual \ntokens, encoded by the vision model. The framework’s functionality comprises (1) \nencoding current visual features, (2) mapping visual features to the language modality \nvia a ttention-based interactions between visual tokens and text queries or learnable \nembedded queries, (3) generating plans with the LLaMA language model and translating \nthem into speciﬁc task commands, and (4) querying the encoded visual tokens from the \nvision model and translating them into low-level control commands through a \ndownstream policy network for task execution. Experimental results, utilizing the MS-\nCOCO dataset, revealed that EmbodiedG PT excels in object recognition and \nunderstanding spatial relationships. Notabl y, implementing a closed-loop design and a \n“chain-of-thought” training mode signi ﬁcantly enhanced EmbodiedGPT’s performance. \nThese results demonstrate that EmbodiedGPT e ﬀectively handles various autonomous \ntasks, exhibiting superior capability in object recognition, understanding spatial \nrelationships, and generating logical, executable plans. \n \nFigure 7. After encoding visual features, they are mappe d using visual tokens and text queries. A \nplan is then created with the LLaMA model and turned into task commands. The visual tokens are \nqueried and converted into low-level control commands to perform the task [150]. \nChen [151] introduced the language-model-based commonsense reasoning (LMCR) \nframework to assist robots in comprehendin g incomplete natural language instructions. \nThis framework enabled robots to receive instructions in natural language from humans, \nobserve their surroundings, and employ a commonsense reasoning method to \nautonomously infer missing information. LMCR utilized a model of commonsense \nreasoning learned from web-based text materials, allowing robots to understand \nincomplete instructions and autonomously execute tasks. The framework comprised three \nmain functions: language understanding, commonsense reasoning, and action planning. \nIn language understanding, LMCR translated human natural language instructions into a \nform interpretable by robots, parsing them into verb frames to convert them into \nexecutable structures. During the common sense reasoning phase, the robot analyzed \nsurrounding objects and employed a language model trained on large-scale unstructured \ntext materials to ﬁll in the missing details from the instructions. This model identiﬁed the \nFigure 7.After encoding visual features, they are mapped using visual tokens and text queries. A\nplan is then created with the LLaMA model and turned into task commands. The visual tokens are\nqueried and converted into low-level control commands to perform the task [150].\nChen [151] introduced the language-model-based commonsense reasoning (LMCR)\nframework to assist robots in comprehending incomplete natural language instructions.\nThis framework enabled robots to receive instructions in natural language from humans, ob-\nserve their surroundings, and employ a commonsense reasoning method to autonomously\ninfer missing information. LMCR utilized a model of commonsense reasoning learned\nfrom web-based text materials, allowing robots to understand incomplete instructions and\nautonomously execute tasks. The framework comprised three main functions: language\nunderstanding, commonsense reasoning, and action planning. In language understanding,\nLMCR translated human natural language instructions into a form interpretable by robots,"
  },
  {
    "question": "Can you explain how visual features are encoded in EmbodiedGPT?",
    "chunk": "state representation, while the world model writer updated the system’s state according to\nexecution outcomes. By facilitating access to the world state ‘memory’, Statler improved\nLLMs’ ability to reason about planning tasks with extended time horizons, overcoming\nlimitations imposed by context length.\nMu [150], shown in Figure7, introduced EmbodiedGPT, a model speciﬁcally designed\nfor Embodied AI, which leverages LLMs. This framework processes visual observations\nand natural language to establish long-term plans and execute tasks in real-time. Em-\nbodiedGPT utilizes pre-trained vision transformers and the LLaMA language model to\nencode visual features and map them to the language modality. The generated plan was\nsubsequently converted into speciﬁc task commands using general visual tokens, encoded\nby the vision model. The framework’s functionality comprises (1) encoding current visual\nfeatures, (2) mapping visual features to the language modality via attention-based interac-\ntions between visual tokens and text queries or learnable embedded queries, (3) generating\nplans with the LLaMA language model and translating them into speciﬁc task commands,\nand (4) querying the encoded visual tokens from the vision model and translating them\ninto low-level control commands through a downstream policy network for task execu-\ntion. Experimental results, utilizing the MS-COCO dataset, revealed that EmbodiedGPT\nexcels in object recognition and understanding spatial relationships. Notably, implement-Appl. Sci.2024, 14, 8868 18 of 39\ning a closed-loop design and a “chain-of-thought” training mode signiﬁcantly enhanced\nEmbodiedGPT’s performance. These results demonstrate that EmbodiedGPT effectively\nhandles various autonomous tasks, exhibiting superior capability in object recognition,\nunderstanding spatial relationships, and generating logical, executable plans.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 18 of 39 \n \ntime. EmbodiedGPT utilizes pre-trained visi on transformers and the LLaMA language \nmodel to encode visual features and map them  to the language modality. The generated \nplan was subsequently converted into speci ﬁc task commands using general visual \ntokens, encoded by the vision model. The framework’s functionality comprises (1) \nencoding current visual features, (2) mapping visual features to the language modality \nvia a ttention-based interactions between visual tokens and text queries or learnable \nembedded queries, (3) generating plans with the LLaMA language model and translating \nthem into speciﬁc task commands, and (4) querying the encoded visual tokens from the \nvision model and translating them into low-level control commands through a \ndownstream policy network for task execution. Experimental results, utilizing the MS-\nCOCO dataset, revealed that EmbodiedG PT excels in object recognition and \nunderstanding spatial relationships. Notabl y, implementing a closed-loop design and a \n“chain-of-thought” training mode signi ﬁcantly enhanced EmbodiedGPT’s performance. \nThese results demonstrate that EmbodiedGPT e ﬀectively handles various autonomous \ntasks, exhibiting superior capability in object recognition, understanding spatial \nrelationships, and generating logical, executable plans. \n \nFigure 7. After encoding visual features, they are mappe d using visual tokens and text queries. A \nplan is then created with the LLaMA model and turned into task commands. The visual tokens are \nqueried and converted into low-level control commands to perform the task [150]. \nChen [151] introduced the language-model-based commonsense reasoning (LMCR)"
  },
  {
    "question": "How do robots gather additional information during task execution?",
    "chunk": "paper details experiments in a simulated tabletop rearrangement, a mini-grid 2D maze,\nand real-world kitchen mobile manipulation settings to evaluate long-horizon reasoning\nperformance. Comparative experiments with SayCan revealed that while SayCan limits\nthe range of robot actions, GD can represent a wider array of actions. In contrast to\nCLIPort, which executes high-level language instructions directly, GD achieves enhanced\nperformance through detailed, step-by-step planning.\nHuang [153], as shown in Figure8, proposed the inner monologue method, which\nallowed LLMs to plan and adjust based on feedback from the environment. This approach\nenabled robots to formulate plans in dynamic environments, retry upon facing failure,\nor seek human feedback to reﬁne their strategies. The author clariﬁed that this method\nemerged from integrating the LLM’s high-level planning capabilities with perceptual feed-\nback and low-level control, thereby facilitating more adaptable and intelligent interactions.\nInner monologue integrated various feedback sources into the language model to assist the\nrobot in executing given instructions, including text-based indicators of the robot’s action\nsuccess or failure, object recognition and descriptions within the scene, the robot’s ability to\nask questions to gather additional information, breaking down instructions into multiple\nsteps to establish an execution plan, and enabling the robot to interact with humans to\nexecute and reﬁne the instructions. The inner monologue method was evaluated in both\nsimulated and real-world environments, such as tabletop rearrangement tasks and manip-\nulation tasks in a real kitchen. The results showed that inner monologue was an effective\nframework, enabling robots to act intelligently in complex interactive settings by effectively\nintegrating environmental feedback to plan and execute tasks.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 20 of 39 \n \n \nFigure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable"
  },
  {
    "question": "What role do adapters play in adapter tuning for transformers?",
    "chunk": "through a better understanding of natural language commands. Conversely, alignment\ntuning (or preference alignment) seeks to align the behavior of LLMs with human values\nand preferences. Prominent methods include reinforcement learning from human feedbackAppl. Sci.2024, 14, 8868 9 of 39\n(RLHF) [110], which involves ﬁne-tuning LLMs using human feedback to better reﬂect\nhuman values, and direct preference optimization (DPO) [111], focusing on training with\npairs of human preferences that usually include an input prompt and the preferred and\nnon-preferred responses.\nFor both instruction tuning and alignment tuning, which involve training LLMs\nwith extensively large model parameters, substantial GPU memory and computational\nresources are required, with high costs typically incurred when utilizing cloud-based\nresources. Under these conditions, parameter-efﬁcient ﬁne-tuning (PEFT) offers a method\ndesigned to efﬁciently conduct ﬁne-tuning of such LLMs [112].\nAmong the methods of PEFT, there are four major approaches as shown in Figure3:\nadapter tuning, prompt tuning, preﬁx tuning, and low-rank adaptation (LoRA). Adapter\ntuning [113,114] involves integrating small neural network modules, known as adapters,\ninto the core components of a transformer model, speciﬁcally into the attention and feed-\nforward layers. These adapters are inserted serially following these layers, allowing ﬁne-\ntuning of only the adapter modules according to speciﬁc task goals, while the parameters\nof the original language model remain unchanged. Consequently, adapter tuning effec-\ntively reduces the number of trainable parameters. Additionally, prompt tuning [115,116]\ndiverges from adapter tuning by adding trainable prompt vectors to the input layer. Preﬁx\ntuning [117] entails appending a sequence of preﬁxes to each transformer layer of the\nlanguage model, which consists of trainable continuous vectors. During ﬁne-tuning, the\nmodel focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM\nmodel inference.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 9 of 39 \n \nresources. Under these conditions, parameter-eﬃcient ﬁne-tuning (PEFT) oﬀers a method \ndesigned to eﬃciently conduct ﬁne-tuning of such LLMs [112]. \nAmong the methods of PEFT, there are four major approaches as shown in Figure 3: \nadapter tuning, prompt tuning, preﬁx tuning, and low-rank adaptation (LoRA). Adapter \ntuning [113,114] involves integrating small neural network modules, known as adapters, \ninto the core components of a transformer model, speciﬁcally into the attention and feed-\nforward layers. These adapters are inserted serially following these layers, allowing ﬁne-\ntuning of only the adapter modules according to speciﬁc task goals, while the parameters \nof the original language model remain unchanged. Consequently, adapter tuning e ﬀec-\ntively reduces the number of trainable parameters. Additionally, prompt tuning [115,116] \ndiverges from adapter tuning by adding trainable prompt vectors to the input layer. Preﬁx \ntuning [117] entails appending a sequence of preﬁxes to each transformer layer of the lan-\nguage model, which consists of trainable continuous vectors. During ﬁne-tuning, the"
  },
  {
    "question": "What is the process of translating human instructions into robot-readable formats?",
    "chunk": "Appl. Sci. 2024, 14, x FOR PEER REVIEW 18 of 39 \n \ntime. EmbodiedGPT utilizes pre-trained visi on transformers and the LLaMA language \nmodel to encode visual features and map them  to the language modality. The generated \nplan was subsequently converted into speci ﬁc task commands using general visual \ntokens, encoded by the vision model. The framework’s functionality comprises (1) \nencoding current visual features, (2) mapping visual features to the language modality \nvia a ttention-based interactions between visual tokens and text queries or learnable \nembedded queries, (3) generating plans with the LLaMA language model and translating \nthem into speciﬁc task commands, and (4) querying the encoded visual tokens from the \nvision model and translating them into low-level control commands through a \ndownstream policy network for task execution. Experimental results, utilizing the MS-\nCOCO dataset, revealed that EmbodiedG PT excels in object recognition and \nunderstanding spatial relationships. Notabl y, implementing a closed-loop design and a \n“chain-of-thought” training mode signi ﬁcantly enhanced EmbodiedGPT’s performance. \nThese results demonstrate that EmbodiedGPT e ﬀectively handles various autonomous \ntasks, exhibiting superior capability in object recognition, understanding spatial \nrelationships, and generating logical, executable plans. \n \nFigure 7. After encoding visual features, they are mappe d using visual tokens and text queries. A \nplan is then created with the LLaMA model and turned into task commands. The visual tokens are \nqueried and converted into low-level control commands to perform the task [150]. \nChen [151] introduced the language-model-based commonsense reasoning (LMCR) \nframework to assist robots in comprehendin g incomplete natural language instructions. \nThis framework enabled robots to receive instructions in natural language from humans, \nobserve their surroundings, and employ a commonsense reasoning method to \nautonomously infer missing information. LMCR utilized a model of commonsense \nreasoning learned from web-based text materials, allowing robots to understand \nincomplete instructions and autonomously execute tasks. The framework comprised three \nmain functions: language understanding, commonsense reasoning, and action planning. \nIn language understanding, LMCR translated human natural language instructions into a \nform interpretable by robots, parsing them into verb frames to convert them into \nexecutable structures. During the common sense reasoning phase, the robot analyzed \nsurrounding objects and employed a language model trained on large-scale unstructured \ntext materials to ﬁll in the missing details from the instructions. This model identiﬁed the \nFigure 7.After encoding visual features, they are mapped using visual tokens and text queries. A\nplan is then created with the LLaMA model and turned into task commands. The visual tokens are\nqueried and converted into low-level control commands to perform the task [150].\nChen [151] introduced the language-model-based commonsense reasoning (LMCR)\nframework to assist robots in comprehending incomplete natural language instructions.\nThis framework enabled robots to receive instructions in natural language from humans, ob-\nserve their surroundings, and employ a commonsense reasoning method to autonomously\ninfer missing information. LMCR utilized a model of commonsense reasoning learned\nfrom web-based text materials, allowing robots to understand incomplete instructions and\nautonomously execute tasks. The framework comprised three main functions: language\nunderstanding, commonsense reasoning, and action planning. In language understanding,\nLMCR translated human natural language instructions into a form interpretable by robots,"
  },
  {
    "question": "What is LLM-BRAIn and how does it help in robot control?",
    "chunk": "robots to carry out instructions: (a) mobile manipulation and (b,c) tabletop manipulation, in both\nsimulated and real-world environments [153].Appl. Sci.2024, 14, 8868 20 of 39\nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn,\na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot\nbehavior trees (BTs) from textual descriptions. The developed model was compact enough\nto operate on a robot’s onboard microcomputer, while adept at constructing complex robot\nbehaviors. It provided structurally and logically correct BTs and demonstrated the ability\nto handle instructions that were not included in the training set.\nSong [155], as shown in Figure9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions\nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via\na low-level planner. It continuously updated environmental information as new objects\nwere detected during action implementation and revisited the LLM to adjust the plan if\nsubgoals failed or were delayed based on updated observations. This iterative process was\nrepeated until the subgoal was achieved, after which the system moved to the next goal.\nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated\ncompetitive performance with signiﬁcantly reduced training data and proved its ability to\ngeneralize in various tasks (e.g., ALFRED) with minimal examples.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 9.LLM-Planner is a system that creates high-level plans based on natural language commands,\nsets subgoals to determine actions, and continuously updates the plan to reﬂect environmental\nchanges [155].\nSingh [156], as shown in Figure10, introduced ProgPrompt, a programmatic LLM\nprompt structure designed for generating plans across diverse situated environments,\nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that\nleveraged LLMs and included a Python programming structure to facilitate information\nabout the environment and executable actions. It featured a feedback mechanism, using\nexecutable program plan examples and assertion statements to mitigate errors, enhancing"
  },
  {
    "question": "How are multimodal language models changing the landscape of robotics?",
    "chunk": "prompts have the potential to cause the entire robotic system to malfunction. To defend\nagainst this critical threat to the reliability and safety of robotic systems, various techniques\nhave been proposed, such as input validation, which ﬁlters the model’s input, and context\nlocking, which restricts access based on the history and content of the prompt. Furthermore,\nstrict guardrails that restrict harmful or unsafe outputs from models can be an alternative\nto improve the reliability of robotic systems. However, it is essential to recognize that the\nsecurity techniques may potentially lead to a decline in the performance of the robot system.\nConsequently, the trade-off between performance and safety must be carefully considered.\nSince the emergence of ChatGPT and Microsoft’s implementation of robot systems\nusing ChatGPT [2], artiﬁcial intelligence components have been applied more widely and\nintensively in robotics research. Despite existing challenges, it is expected that research\ninvolving foundation models to improve robot intelligence will persist across various\ndomains and methods, which will likely enhance the usability and market potential of\nrobot systems inﬂuenced by these advancements.\n6. Conclusions\nIn this paper, we have explored the potential impact and applicability of LLMs on\nrobotics research ﬁelds by summarizing studies that applied LLMs and VLMs to robots.\nFundamentally, LLMs can enhance the capability of robots in natural language processing\nto interact with humans and to improve the robots’ autonomy in various task scenarios.\nIn particular, the ability of LLMs to understand and generate natural language plays a\ncrucial role in enabling robots to comprehend and execute complex commands. This survey\nconﬁrmed that the scope of utilizing LLMs in robotics was not limited to simple natural\nlanguage processing but also extended to broader research areas. This study explored\nextensive LLM applications in the robotics literature, such as planning, manipulation, and\nscene understanding, as well as reinforcement learning automation frameworks such as\nEureka, and included robot actions in language models such as AutoRT. Moreover, the\nresearch direction of current generative AI models is transitioning towards multimodalAppl. Sci.2024, 14, 8868 30 of 39\nlanguage models, moving beyond information acquisition and cognition aspects such as\ntext, images, and videos to include actuator actions within large models in therobotics ﬁeld.\nWhile the surveyed studies indicated that LLMs play a promising role in the future\nof robotics, certain limitations were also identiﬁed. First, the increased computational\nresources and energy consumption associated with embedding LLMs into robotic systems\nmust be addressed. Second, biases in language models and ethical considerations are\nsigniﬁcant issues that need to be tackled in robotics. Therefore, continual efforts will be\nnecessary in future research to resolve these challenges.\nOverall, LLMs are valuable tools that can signiﬁcantly advance robotics. This review\nhas revealed that innovative robot applications are possible through the integration of\nLLMs and VLMs. Moreover, these foundation models are expected to serve as critical\nelements for future robot research and practical applications in the real world.\nAuthor Contributions:Conceptualization, S.S. and C.K.; methodology, S.S.; formal analysis, H.J.,\nH.L. and S.S.; investigation, H.J., H.L. and S.S.; resources, H.J., H.L., S.S. and C.K.; writing—original\ndraft preparation, H.J., H.L., S.S. and C.K.; writing—review and editing, H.J., H.L., S.S. and C.K.; vi-"
  },
  {
    "question": "How do automated annotations improve object understanding in robots?",
    "chunk": "sequential manipulation tasks that require long-horizon reasoning. Text2Motion interpreted\nnatural language instructions to formulate task plans and generated multiple candidate\nskill sequences, evaluating the geometric feasibility of each sequence. By employing a\ngreedy search strategy, it selected the optimal skill sequence to verify and execute the ﬁnal\nplan. This method enabled Text2Motion to perform complex sequential manipulation tasks\nwith a higher success rate compared to existing language-based planning methods, such\nas Saycan-gs and Innermono-gs, and provided semantically generalized characteristics\namong skills with geometric relationships.\nWu [160] investigated personalization in-home cleaning robots that organize and tidy\nspaces, using an LLM to convert user-provided object placement locations into generalized\nrules. By using a camera to identify objects and CLIP to categorize them, TidyBot efﬁciently\nrelocated objects according to these rules. This method attained an impressive accuracy\nof 91.2% for unseen objects in a benchmark dataset, which encompassed a variety of\nobjects, receptacles, and example placements of both “seen” and “unseen” objects across\n96 scenarios. Additionally, it achieved an 85% success rate in removing objects during\nreal-world tests.\n4.4. Manipulation by LLMs\nIn robotics research, the manipulation domain, which includes robotic arms and\nend effectors, encompasses various areas that beneﬁt from foundation models such as\nLLMs for language-based interactions and VLMs for object handling. Among the studies\nintegrating manipulation with foundation models, Stone [161] introduced an approach\ncalled manipulation of open-world objects (MOO). This approach determined whether a\nrobot could follow instructions involving unseen object categories by linking pre-trained\nmodels to robotic policies. MOO utilized pre-trained vision-language models to derive\nobject information from language commands and images, guiding the robot’s actions based\non the current image, command, and identiﬁed object data. Experimental use of real mobile\nmanipulation robots showed that MOO could adapt to new object types and environments\nin a zero-shot fashion. Moreover, MOO responded to non-verbal cues such as pointing at\nspeciﬁc objects, extending its scope to open-world exploration and manipulation.Appl. Sci.2024, 14, 8868 23 of 39\nExisting VLMs often lack a comprehensive understanding of physical concepts such\nas material and fragility, which limits their effectiveness in robotic manipulation tasks. To\naddress this issue, Gao [162] introduced PhysObjects, an object-centric dataset featuring\n39.6K crowd-sourced annotations and 417K automated annotations of physical concepts.\nThe automated annotations involved assigning speciﬁc concept values to predeﬁned object\ncategories or continuous concepts such as material and fragility. Fine-tuning a VLM on\nPhysObjects enhanced comprehension of physical concepts by capturing human biases\nrelated to the visual appearance of objects. Integrating this physically grounded VLM\nwith an LLM-based robotic planner framework improved performance in tasks requiring\nreasoning about physical concepts.\nThe traditional pre-training and ﬁne-tuning pipeline often suffers from decreased\nlearning efﬁciency and challenges in generalizing to unseen objects and tasks due to its\nreliance on domain-speciﬁc action information and domain-general visual information. To\naddress these limitations, Wang [163] proposed a modular approach named ProgramPort,\nwhich utilizes the syntactic and semantic structure of language instructions. Wang’s\nframework incorporated a semantic parser to reconstruct executable programs, composed of\nfunctional modules based on vision and action across multiple modalities. Each functional\nmodule combined deterministic computation with learnable neural networks. Program\nexecution involved generating parameters for general manipulation primitives used by the"
  },
  {
    "question": "What is the significance of using pre-trained VLMs in scene understanding?",
    "chunk": "In a real kitchen setting, the plan success rate decreased slightly to 81% and the execution\nsuccess rate fell to 60%, demonstrating that the policy and value functions generalize well\nto real-world settings.\nHuang [167] introduced the Instruct2Act framework, which employs LLMs to se-\nquentially map multi-modality instructions to robot actions. The previous method, CaP,\ngenerated robot policy program code directly from in-context examples based on language\ninstructions. However, this approach was constrained by the capabilities of the generated\ncode and encountered difﬁculties with longer, more complex commands due to the required\nhigh precision of code. To overcome these limitations, Instruct2Act introduced a novel\nstrategy that used multi-modality models and LLMs to simultaneously address recognition,\ntask planning, and low-level control modules. Instruct2Act utilized the segment anything\nmodel for identifying potential objects in input images for multi-modality recognition\nand the CLIP model for object classiﬁcation. As a result, Instruct2Act developed an inte-\ngrated search system capable of managing various input modalities and instruction types,\nincluding both pure language instructions and combined language-visual instructions,\nfacilitating the integration of diverse instruction types into a uniﬁed architecture. Moreover,\nfor pointer-language instructions, the framework supported task segmentation based on\nthe user’s clicks.\n4.5. Scene Understanding in LLMs and VLMs\nTo address the VQA problem, robotics research increasingly uses pre-trained VLMs\nto derive high-level information from visual data. This method is advantageous for scene\nunderstanding as it helps determine affordances that describe the relationship between the\ncurrent state and the next action based on images from cameras. Related studies focus on\naspects of scene understanding.Appl. Sci.2024, 14, 8868 25 of 39\nChen [168] explored methods to integrate commonsense into scene understanding\nusing LLMs and introduced three paradigms for classifying room types within indoor\nenvironments based on included objects. The zero-shot approach utilized a pre-trained\nlanguage model to identify the objects in a room and estimate their types. The feed-\nforward classiﬁer approach involved inputting sentences that listed a room’s objects into\nthe language model to generate embedding vectors, which were subsequently input into a\npre-trained shallow multilayer perceptron to predict each room type. Lastly, the classiﬁer\napproach embedded images of rooms alongside textual descriptions to identify the best-\nmatching description, thereby determining the room type. These paradigms demonstrated\nthe capacity to generalize to objects not presented in the training set and to make inferences\nwithin a space larger than that deﬁned by the trained object labels.\nYang [169] introduced the innovative zero-shot, open-vocabulary, LLM-based 3D\nvisual grounding pipeline called LLM-Grounder. This method breaks down complex natu-\nral language queries into semantic components and uses visual grounding tools such as\nOpenScene or LERF to locate objects within 3D scenes. Subsequently, the LLM evaluates\nspatial and commonsense relationships among these objects to achieve the ﬁnal grounding.\nRemarkably, LLM-Grounder operates without labeled training data and has proven its ca-\npacity to adapt to new 3D scenes and diverse text queries, enhancing grounding capabilities\nfor complex language queries and establishing itself as an effective solution.\nChen [170] developed NLMap, an open-vocabulary, queryable scene representation\nsystem. Designed to accumulate and incorporate contextual data within a scene repre-\nsentation for natural language queries, this system allows an LLM planner to visualize"
  },
  {
    "question": "What are VLMaps and how do they work?",
    "chunk": "trained vision-language features with a 3D reconstruction of the physical world. VLMaps, \nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary \nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands. \nThese commands can be directly localized on a map and generate new obstacle maps in \nreal-time, facilitated by sharing among various robot types. Extensive experiments \nconducted in both simulated environments (using the Habitat simulator with the \nMatterport3D dataset and the AI2THOR simulator) and real-world settings (with the HSR \nmobile robot for indoor navigation) demonstrated that VLMs can navigate based on more \ncomplex language instructions than previous methods. The reviewed papers in this study \nare summarized in Table 5. \nTable 5. Summary of the reviewed papers in this study. \nName Explanation Ref. \nReward Design in \nRL \n• Eureka automatically generates and im proves reward functions based on the \nvirtual environment source code.$• Dr Eureka builds reward-aware physics \npriors using Eureka and supports eﬀective operation in the real world through \ndomain randomization.$• LLMs design and re ﬁne reward functions based on \nnatural language input.$• LLMs and VLMs integrate multimodal data to \ngenerate reward functions. \n[11,134,136–139,176–\n180] \nFigure 13.LM-Nav uses three pre-trained models: (a) VNM builds a topological graph from observa-\ntions, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, (d)A\ngraph search algorithm then ﬁnds the best robot trajectory, and (e) the robot executes the planned\npath [173].\nZhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow\ninstructions. NavGPT is a vision-language navigation system that employs an LLM to trans-\nlate visual inputs from a visual foundation model (VFM) into natural language. The LLM\nthen interprets the current state and makes informed decisions to reach the intended goal,\nbased on these converted visuals, navigation history, and potential future routes. NavGPT\nconducts various functions, including high-level planning, decomposing instructions into\nsub-goals, identifying landmarks in observed scenes, monitoring navigation progress, and\nmodifying plans as necessary. Although NavGPT’s performance on zero-shot tasks from\nthe R2R dataset has not yet matched that of trained models, it underscored the potential\nof utilizing multi-modality inputs with LLMs for visual navigation and tapping into the\nexplicit reasoning capabilities of LLMs to enhance learned models.\nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-\ntrained vision-language features with a 3D reconstruction of the physical world. VLMaps,\nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary\nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands.\nThese commands can be directly localized on a map and generate new obstacle maps in real-Appl. Sci.2024, 14, 8868 27 of 39\ntime, facilitated by sharing among various robot types. Extensive experiments conducted in\nboth simulated environments (using the Habitat simulator with the Matterport3D dataset\nand the AI2THOR simulator) and real-world settings (with the HSR mobile robot for\nindoor navigation) demonstrated that VLMs can navigate based on more complex language\ninstructions than previous methods. The reviewed papers in this study are summarized in\nTable 5."
  },
  {
    "question": "What is the role of LLMs in robot navigation systems?",
    "chunk": "representation for natural language queries; then, an LLM-based object suggestion module\nreviews instructions, suggests relevant objects, and queries the scene for object availability\nand location. Using this information, the LLM planner devises plans uniquely tailored to\nthe scene’s context. NLMap equips robots with the ability to function without a predeﬁned\ncatalog of objects or actions, overcoming the constraints of earlier methods and enabling\nmore adaptable operations in environments with novel or absent objects.\nElhafsi [171] introduced a monitoring framework that employed an LLM with superior\ncontextual understanding and reasoning capabilities to detect edge cases and anomalies\nwithin vision-based policies. This framework monitored the robot’s perception stream\nthrough an LLM-based module, designed to detect semantic anomalies that might occur\nduring operations. By converting the robot’s visual observations into textual descriptions\nat regular intervals and integrating these into LLM prompts, it could pinpoint factors\nleading to policy errors, unsafe behavior, or task confusion. The conversion of visual\ninformation into natural language descriptions used various techniques, without restriction\nto any speciﬁc method. This ﬂexibility enabled both fully end-to-end policies and classical\nautonomy stacks using learned perception to align more closely with human intuition.\nThe ﬁndings indicated that semantic anomalies did not always correspond to semantically\nexplainable failures, and end-to-end policies could sometimes behave unpredictably.\nHon [172] introduced a new model family named 3D-LLM, which incorporated 3D\nworld information into LLMs. The 3D-LLM model utilized 3D point clouds and their\nfeatures as input, enabling it to handle a variety of spatially aware 3D tasks. These tasks\nincluded 3D captioning, dense captioning, 3D question answering, task decomposition, 3D\ngrounding, 3D-assisted dialogue, and navigation. The model used a 3D feature extractor to\nalign 3D features from multi-view images with language features, facilitating more precise\ntext generation and question answering based on spatial understanding. To train 3D-LLM,\na pre-trained 2D VLM formed the backbone, enhanced by the addition of 3D positional\nembeddings to better capture 3D spatial information. The model generated location tokens\nthrough linguistic descriptions of speciﬁc objects and was trained using 3D features as input.\nExperimental results showed that 3D-LLM excelled in various 3D-related tasks, achieving\napproximately a 9% higher BLEU-1 score compared to previous models on the ScanQA\ndataset. It demonstrated superior performance in 3D captioning, task composition, andAppl. Sci.2024, 14, 8868 26 of 39\n3D-assisted dialogue, outperforming 2D VLMs and displaying an improved understanding\nof object locations, shapes, and interactions.\nIn the extension of scene understanding using VLMs, the keyword VLN (vision-\nand-language navigation) is widely used in navigation-related research, where language\nfoundation models are increasingly utilized.\nShah [173], as shown in Figure13, introduced a robotic navigation system named\nLM-Nav, which capitalized on the advantages of training with large, unlabeled trajectory\ndatasets while providing a high-level interface for users. LM-Nav utilized three large-scale\npre-trained models: ViNG, CLIP, and GPT-3. Initially, the LLM translated natural language\ninstructions into a sequence of textual landmarks. The VLM integrated these textual\nlandmarks with images to identify the relevant images through probabilistic distribution.\nSubsequently, the VNM utilized these landmarks to plan and execute robot trajectories"
  },
  {
    "question": "How does the integration of various feedback sources impact robot performance?",
    "chunk": "parsing them into verb frames to convert them into executable structures. During the\ncommonsense reasoning phase, the robot analyzed surrounding objects and employed a\nlanguage model trained on large-scale unstructured text materials to ﬁll in the missing\ndetails from the instructions. This model identiﬁed the most suitable verb frame to com-\nplete the gaps. Subsequently, based on the completed verb frame, the robot formulated its\nactions using predeﬁned action plans for each verb to guide the movements of the robot\narm and execute the assignment. Experimental results showed that LMCR demonstrated\nsuperior generalization performance for novel concepts not presented in the training set\nand surpassed GCNGrasp, which depends on a predeﬁned graph structure for all concepts\nand their relationships. This indicated that LMCR was an effective tool, combining the se-\nmantic reasoning capabilities of language models with planning that adapted to the robot’s\nspeciﬁc environment and context, effectively managing complex and prolonged tasks.\nHuang [152] introduced a methodology named grounded decoding (GD), which offers\na method for generating LLM-based robot action plans. These plans enable robots to execute\nlong-term tasks across diverse physical environments. The methodology encompasses two\nprimary elements: linking the text generated by the language model to actionable taskAppl. Sci.2024, 14, 8868 19 of 39\ncommands in the physical world via GD and adjusting the tokens generated by the LLM\nto real-world conditions to formulate feasible commands. This approach synergizes the\nhigh-level semantic reasoning of LLMs with plans that are aligned with the robot’s physical\nenvironment and capabilities, thus facilitating the execution of complex and long-term\ntasks. The method addresses several limitations robots face in performing complex, long-\nterm tasks, such as a lack of physical world experience, an inability to process non-verbal\ncues, and a disregard for necessary robotic constraints such as safety and rewards. The\npaper details experiments in a simulated tabletop rearrangement, a mini-grid 2D maze,\nand real-world kitchen mobile manipulation settings to evaluate long-horizon reasoning\nperformance. Comparative experiments with SayCan revealed that while SayCan limits\nthe range of robot actions, GD can represent a wider array of actions. In contrast to\nCLIPort, which executes high-level language instructions directly, GD achieves enhanced\nperformance through detailed, step-by-step planning.\nHuang [153], as shown in Figure8, proposed the inner monologue method, which\nallowed LLMs to plan and adjust based on feedback from the environment. This approach\nenabled robots to formulate plans in dynamic environments, retry upon facing failure,\nor seek human feedback to reﬁne their strategies. The author clariﬁed that this method\nemerged from integrating the LLM’s high-level planning capabilities with perceptual feed-\nback and low-level control, thereby facilitating more adaptable and intelligent interactions.\nInner monologue integrated various feedback sources into the language model to assist the\nrobot in executing given instructions, including text-based indicators of the robot’s action\nsuccess or failure, object recognition and descriptions within the scene, the robot’s ability to\nask questions to gather additional information, breaking down instructions into multiple\nsteps to establish an execution plan, and enabling the robot to interact with humans to\nexecute and reﬁne the instructions. The inner monologue method was evaluated in both\nsimulated and real-world environments, such as tabletop rearrangement tasks and manip-\nulation tasks in a real kitchen. The results showed that inner monologue was an effective\nframework, enabling robots to act intelligently in complex interactive settings by effectively\nintegrating environmental feedback to plan and execute tasks.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 20 of 39"
  },
  {
    "question": "What is the significance of low-level control in robotic systems?",
    "chunk": "more, Eureka has demonstrated the ability to solve complex problems that were previ-\nously unsolved by expert-designed reward functions. \nGiven these research outcomes, integrating language models into robotic intelligence \npresents signi ﬁcant potential to enhance robot capabilities and applications dramatically, \nthereby rede ﬁning their roles in diverse industries and everyday life. Therefore, this sur-\nvey paper explores recent research trends in LLM- and VLM-based robot intelligence, \naiming to provide a comprehensive understanding of future development possibilities by \nexamining the application of language models in various robotic research ﬁelds. It also \nseeks to highlight research cases, identify cu rrent limitations, and suggest future research \ndirections. \nTo chronicle this advancement in robotics research ﬁelds, this review paper presents \nthe following contributions: \n• This paper summarizes and introduces the foundational elements and tuning meth-\nods of LLM architecture. \n• It explores and arranges prompt techniques  to enhance the problem-solving abilities \nof LLMs. \n• It  reviews and encapsulates how LLMs and VLMs have been employed to augment \nrobot intelligence across ﬁve topics as shown in Figure 1: (1) reward design for rein-\nforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation, \nand (5) scene understanding. \n \nFigure 1. Five categories for robot intelligence with large language models in this study. \nFigure 1.Five categories for robot intelligence with large language models in this study.Appl. Sci.2024, 14, 8868 4 of 39\nThe reward design in RLcategory represents a research ﬁeld in which an LLM devel-\nops and enhances reward functions employed in reinforcement learning via code-based\ndescriptions and natural language input. This enables robots to learn optimal policies\nfor speciﬁc tasks through reinforcement learning, even in complex environments. The\nlow-level controlcategory includes a research area in which LLMs and VLMs generate\ncommand sequences that directly control the robot’s actuators through natural language\nand visual input. Thehigh-level planningcategory is a research area where the LLM\nidentiﬁes the present circumstances and objective of the tasks, subsequently developing an\nexplainable plan based on the reasoning required for problem-solving. In this research area,\nthe LLM is also tasked with developing the optimal robot behavior plan, which entails\nevaluating the feasibility of the established plan. In themanipulation category, the LLM in-\nterprets high-level instructions and the VLM (and LLM) analyzes various conditions based\non their understanding of the surroundings to assist robot arms in performing the speciﬁc\ntasks. While this category can be broadly included in the high-level planning category,\nthere are numerous studies that are speciﬁcally related to manipulation with a robot arm,\nwhich is why the manipulation category was separated. Thescene understandingcategory\nrepresents a research area that seeks to combine LLMs and VLMs with the objective of\nassisting robots in comprehending their surrounding environment. This is accomplished\nby identifying objects based on natural language instructions and visual information, as\nwell as by evaluating the relationships between them. This research area is also closely\nrelated to the ﬁeld of autonomous visual navigation. From a boarder perspective, there is an\noverlap between the scene understanding category and the perception-related components\nof the high-level planning category. However, in this review, the scene understanding\ncategory was considered a distinct category due to its prevalence as an application of\nVLM models.\nTable 1 lists resources that aid in understanding robot intelligence based on language"
  },
  {
    "question": "How do LLMs generalize spatial reasoning tasks?",
    "chunk": "supporting the creation of high-level policies for robots and accommodating a variety of\nrobotic tasks. Speciﬁcally, CaP interpreted natural language instructions through descrip-\ntions and formulated an action plan for the robot. Moreover, it utilized VLMs such as ViLD\nand MDETR to identify objects and ascertain their locations. Based on this information,\nthe framework controlled the robot’s movements to carry out speciﬁed tasks. The paper\ndemonstrated the CaP framework across diverse domains, including whiteboard drawing,\ntabletop manipulation, and mobile robot navigation and manipulation. Experimental\nresults showed that CaP achieved similar or better success rates than existing systems\nsuch as CLIPort, displaying notably strong generalization capabilities for new tasks. These\nﬁndings underscored the ﬂexibility and efﬁcacy of the CaP framework, establishing its\neffectiveness across various robotic systems.\nMirchandani [148], shown in Figure6, suggested that pre-trained LLMs could autore-\ngressively complete complex token sequences and function as general sequence modelers\nthrough in-context learning without needing additional training. Expanding on this con-\ncept, the study evaluated LLMs’ ability to operate as pattern machines in three domains:\nsequence transformation, sequence completion, and sequence improvement. In sequence\ntransformation, the research demonstrated that LLMs could generalize speciﬁc sequence\ntransformations using benchmarks such as ARC (abstraction and reasoning corpus) and\nPCFG (probabilistic context-free grammar), thereby proving their utility in spatial reasoning\ntasks for robotics. In sequence completion, the study examined whether LLMs could ﬁnish\npatterns in elementary functions (e.g., sinusoids), illustrating their utility in robotic tasks\nsuch as extending a wiping motion from kinesthetic demonstrations or creating drawings\non a whiteboard. Finally, in sequence improvement, the research revealed that by utilizing\nreward-labeled trajectories as context and incorporating online interaction, LLM-based\nagents could explore small grids and reﬁne simple trajectories using human-in-the-loop\nmethods, such as optimizing a CartPole controller.Appl. Sci.2024, 14, 8868 17 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 17 of 39 \n \nsequence transformation, the research demonstrated that LLMs could generalize speci ﬁc \nsequence transformations using benchmarks such as ARC (abstraction and reasoning \ncorpus) and PCFG (probabilistic context-free grammar), thereby proving their utility in \nspatial reasoning tasks for robotics. In sequence completion, the study examined whether \nLLMs could ﬁnish pa tterns in elementary functions (e.g., sinusoids), illustrating their \nutility in robotic tasks such as extending a wiping motion from kinesthetic demonstrations \nor creating drawings on a whiteboard. Finally, in sequence improvement, the research \nrevealed that by utilizing reward-labeled trajectories as context and incorporating online \ninteraction, LLM-based agents could explore small grids and re ﬁne simple trajectories \nusing human-in-the-loop methods, such as optimizing a CartPole controller.  \n \nFigure 6. Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed \nin sequence transformation, completion, and improvement [148]. \n4.3. High-Level Planning (Including Decision-Making and Reasoning) \nThe abstraction and generalization capabilities of LLMs oﬀer eﬀective methodologies \nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various \nresearch outcomes have been realized in the ﬁelds of planning, decision-making,"
  },
  {
    "question": "How can robots benefit from domain randomization during training?",
    "chunk": "4. Language Models for Robotic Intelligence\n4.1. Reward Design in Reinforcement Learning\nResearch in reinforcement learning, closely associated with the ﬁeld of robotics, has\nactively incorporated studies using LLM models. Speciﬁcally, Nvidia has developed a\nGPU-based multi-environment reinforcement learning platform. Utilizing its Omniverse\n3D virtual environment platform, Nvidia created Isaac Sim, which is dedicated to robot\nsimulation. Isaac Sim published research ﬁndings on Isaac Gym (Preview), which achieved\nsigniﬁcant reductions in reinforcement learning training times through GPU-based multi-\nenvironment approaches. Subsequently, Isaac Gym (Preview)’s features were integrated\ninto Isaac Sim and released as Omni Isaac Gym. Later, Nvidia introduced Orbit [133],\nfacilitating the simulation of PhysX 5.1-based cloth, soft-body, ﬂuid, and rigid-body dy-\nnamics, along with RGBD, LiDAR, and contact sensor simulation. Orbit also incorporates\nvarious robot platforms into the simulation environment. Recently, Orbit was updated\nto Isaac Lab and integrated into Isaac Sim 4.0. Nvidia has continuously advanced dy-\nnamic simulation environment technologies for reinforcement learning using GPU parallel\ncomputation. Leveraging this GPU reinforcement learning, they launched Eureka [11],\nwhich automates the design of reward functions for reinforcement learning using LLMs.\nFollowing this, Nvidia introduced DrEureka [134], an automated platform addressing the\nSim2Real problem [135] in reinforcement learning based on Eureka.\nEureka (Evolution-driven Universal REward Kit for Agent) [11], shown in Figure4,\nautomatically generates reward functions for various tasks using different robots, elimi-\nnating the need for speciﬁc templates or tailored reward functions for the robot’s form or\nexplanations for reinforcement learning tasks. Eureka consists of three main components:\nenvironment-as-context, evolutionary search, andreward reﬂection. Environment-as-context\ngenerates executable reward functions in a zero-shot manner by utilizing virtual environ-\nment source (Python) code as context. Evolutionary search iteratively generates reward\nfunction candidates and proposes enhanced functions based on previously generated and\nbest-performing ones, while also creating new functions through mutation. Reward reﬂec-\ntion offers a text summary of reward function quality based on training statistics recorded\nduring reinforcement learning, which assists in generating subsequent reward functions\nas feedback for the performance of previous functions. The reward functions generated\noutperformed expert-generated functions in 83% of benchmark tests. Moreover, Eureka\nsolved the pen spinning problem where a robot hand must spin a pen as much as possi-\nble according to predeﬁned rotations, a task previously considered unsolvable through\nmanual reward engineering. Eureka introduces a universal reward function design algo-\nrithm based on a code LLM and in-context evolutionary search, facilitating human-level\nreward generation for various robots and tasks without the need for prompt engineering or\nhuman intervention.\nFollowing Eureka, DrEureka [134], shown in Figure5, was developed to address\nthe sim-to-real problem by automatically conﬁguring appropriate reward functions and\ndomain randomization for physical environments. DrEureka’s reward-aware physics pri-\nors mechanism deﬁnes the lower and upper bounds of physical environment parameters\nbased on policies trained through initial reinforcement learning, facilitating reinforcement\nlearning across various physical environment domains. This randomization enables the\ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation"
  },
  {
    "question": "How does the integration of language and vision improve robot interaction with the environment?",
    "chunk": "representation for natural language queries; then, an LLM-based object suggestion module\nreviews instructions, suggests relevant objects, and queries the scene for object availability\nand location. Using this information, the LLM planner devises plans uniquely tailored to\nthe scene’s context. NLMap equips robots with the ability to function without a predeﬁned\ncatalog of objects or actions, overcoming the constraints of earlier methods and enabling\nmore adaptable operations in environments with novel or absent objects.\nElhafsi [171] introduced a monitoring framework that employed an LLM with superior\ncontextual understanding and reasoning capabilities to detect edge cases and anomalies\nwithin vision-based policies. This framework monitored the robot’s perception stream\nthrough an LLM-based module, designed to detect semantic anomalies that might occur\nduring operations. By converting the robot’s visual observations into textual descriptions\nat regular intervals and integrating these into LLM prompts, it could pinpoint factors\nleading to policy errors, unsafe behavior, or task confusion. The conversion of visual\ninformation into natural language descriptions used various techniques, without restriction\nto any speciﬁc method. This ﬂexibility enabled both fully end-to-end policies and classical\nautonomy stacks using learned perception to align more closely with human intuition.\nThe ﬁndings indicated that semantic anomalies did not always correspond to semantically\nexplainable failures, and end-to-end policies could sometimes behave unpredictably.\nHon [172] introduced a new model family named 3D-LLM, which incorporated 3D\nworld information into LLMs. The 3D-LLM model utilized 3D point clouds and their\nfeatures as input, enabling it to handle a variety of spatially aware 3D tasks. These tasks\nincluded 3D captioning, dense captioning, 3D question answering, task decomposition, 3D\ngrounding, 3D-assisted dialogue, and navigation. The model used a 3D feature extractor to\nalign 3D features from multi-view images with language features, facilitating more precise\ntext generation and question answering based on spatial understanding. To train 3D-LLM,\na pre-trained 2D VLM formed the backbone, enhanced by the addition of 3D positional\nembeddings to better capture 3D spatial information. The model generated location tokens\nthrough linguistic descriptions of speciﬁc objects and was trained using 3D features as input.\nExperimental results showed that 3D-LLM excelled in various 3D-related tasks, achieving\napproximately a 9% higher BLEU-1 score compared to previous models on the ScanQA\ndataset. It demonstrated superior performance in 3D captioning, task composition, andAppl. Sci.2024, 14, 8868 26 of 39\n3D-assisted dialogue, outperforming 2D VLMs and displaying an improved understanding\nof object locations, shapes, and interactions.\nIn the extension of scene understanding using VLMs, the keyword VLN (vision-\nand-language navigation) is widely used in navigation-related research, where language\nfoundation models are increasingly utilized.\nShah [173], as shown in Figure13, introduced a robotic navigation system named\nLM-Nav, which capitalized on the advantages of training with large, unlabeled trajectory\ndatasets while providing a high-level interface for users. LM-Nav utilized three large-scale\npre-trained models: ViNG, CLIP, and GPT-3. Initially, the LLM translated natural language\ninstructions into a sequence of textual landmarks. The VLM integrated these textual\nlandmarks with images to identify the relevant images through probabilistic distribution.\nSubsequently, the VNM utilized these landmarks to plan and execute robot trajectories"
  },
  {
    "question": "What types of environments were used for testing VLMs?",
    "chunk": "Zhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow\ninstructions. NavGPT is a vision-language navigation system that employs an LLM to trans-\nlate visual inputs from a visual foundation model (VFM) into natural language. The LLM\nthen interprets the current state and makes informed decisions to reach the intended goal,\nbased on these converted visuals, navigation history, and potential future routes. NavGPT\nconducts various functions, including high-level planning, decomposing instructions into\nsub-goals, identifying landmarks in observed scenes, monitoring navigation progress, and\nmodifying plans as necessary. Although NavGPT’s performance on zero-shot tasks from\nthe R2R dataset has not yet matched that of trained models, it underscored the potential\nof utilizing multi-modality inputs with LLMs for visual navigation and tapping into the\nexplicit reasoning capabilities of LLMs to enhance learned models.\nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-\ntrained vision-language features with a 3D reconstruction of the physical world. VLMaps,\nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary\nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands.\nThese commands can be directly localized on a map and generate new obstacle maps in real-Appl. Sci.2024, 14, 8868 27 of 39\ntime, facilitated by sharing among various robot types. Extensive experiments conducted in\nboth simulated environments (using the Habitat simulator with the Matterport3D dataset\nand the AI2THOR simulator) and real-world settings (with the HSR mobile robot for\nindoor navigation) demonstrated that VLMs can navigate based on more complex language\ninstructions than previous methods. The reviewed papers in this study are summarized in\nTable 5.\nTable 5.Summary of the reviewed papers in this study.\nName Explanation Ref.\nReward Design in RL\n• Eureka automatically generates and improves reward\nfunctions based on the virtual environment source\ncode.\n• DrEureka builds reward-aware physics priors using\nEureka and supports effective operation in the real\nworld through domain randomization.\n• LLMs design and reﬁne reward functions based on\nnatural language input.\n• LLMs and VLMs integrate multimodal data to\ngenerate reward functions.\n[11,134,136–139,176–180]\nLow-level\nControl\n• Generating commands to control actuators capable of\nlow-level control.\n• RT-1 and RT-2 enable robots to perform complex tasks\nbased on language-vision data.\n• AutoRT establishes a system where robots can\nautonomously collect and utilize data.\n[8–10,144–148,181–183]\nHigh-level\nPlanning\n• LLMs provide an effective methodology for tasks\nrelated to high-level planning within robotic systems.\n• By using natural language, LLMs can formulate plans\nto solve tasks that require long-horizon reasoning.\n• LLMs assess the feasibility of actions to determine and\nexecute the optimal robotic behavior.\n• LLMs generate behavior trees to structure complex\nrobotic actions accurately.\n[149–160,184–207]\nManipulation\n• Using LLMs and VLMs to integrate language and\nvision data allows various manipulations.\n• LLMs interpret high-level instructions to generate the\nnecessary robot actions and assess their feasibility.\n• VLMs extract object information from images to assist\nin performing manipulations.\n[161–167,208–215]\nScene\nUnderstanding\n• To solve VQA problems, use VLMs to extract\nhigh-level information from vision data."
  },
  {
    "question": "How does EmbodiedGPT excel in object recognition?",
    "chunk": "Yoneda [149] introduced Statler, a framew ork designed to provide LLMs with an \nexplicit world state representation through a continuously maintained ‘memory’. The core \nof Statler consisted of two components: the world model reader and the world model \nwriter. These components interacted with and sustained the world state. The world model \nreader interpreted user commands and generated executable code based on the current \nstate representation, while the world model wr iter updated the system’s state according \nto execution outcomes. By facilitating access to the world state ‘memory’, Statler improved \nLLMs’ ability to reason about planning task s with extended time horizons, overcoming \nlimitations imposed by context length. \nMu [150], shown in Figure 7, introduced EmbodiedGPT, a model speci ﬁcally \ndesigned for Embodied AI, which leverages LLMs. This framework processes visual \nobservations and natural language to establish long-term plans and execute tasks in real-\nFigure 6.Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed in\nsequence transformation, completion, and improvement [148].\n4.3. High-Level Planning (Including Decision-Making and Reasoning)\nThe abstraction and generalization capabilities of LLMs offer effective methodologies\nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various\nresearch outcomes have been realized in the ﬁelds of planning, decision-making, reasoning,\nand behavior trees within robotics.\nYoneda [149] introduced Statler, a framework designed to provide LLMs with an\nexplicit world state representation through a continuously maintained ‘memory’. The core\nof Statler consisted of two components: the world model reader and the world model\nwriter. These components interacted with and sustained the world state. The world model\nreader interpreted user commands and generated executable code based on the current\nstate representation, while the world model writer updated the system’s state according to\nexecution outcomes. By facilitating access to the world state ‘memory’, Statler improved\nLLMs’ ability to reason about planning tasks with extended time horizons, overcoming\nlimitations imposed by context length.\nMu [150], shown in Figure7, introduced EmbodiedGPT, a model speciﬁcally designed\nfor Embodied AI, which leverages LLMs. This framework processes visual observations\nand natural language to establish long-term plans and execute tasks in real-time. Em-\nbodiedGPT utilizes pre-trained vision transformers and the LLaMA language model to\nencode visual features and map them to the language modality. The generated plan was\nsubsequently converted into speciﬁc task commands using general visual tokens, encoded\nby the vision model. The framework’s functionality comprises (1) encoding current visual\nfeatures, (2) mapping visual features to the language modality via attention-based interac-\ntions between visual tokens and text queries or learnable embedded queries, (3) generating\nplans with the LLaMA language model and translating them into speciﬁc task commands,\nand (4) querying the encoded visual tokens from the vision model and translating them\ninto low-level control commands through a downstream policy network for task execu-\ntion. Experimental results, utilizing the MS-COCO dataset, revealed that EmbodiedGPT\nexcels in object recognition and understanding spatial relationships. Notably, implement-Appl. Sci.2024, 14, 8868 18 of 39\ning a closed-loop design and a “chain-of-thought” training mode signiﬁcantly enhanced\nEmbodiedGPT’s performance. These results demonstrate that EmbodiedGPT effectively\nhandles various autonomous tasks, exhibiting superior capability in object recognition,"
  },
  {
    "question": "What research directions are being explored for improving robot intelligence?",
    "chunk": "and multiple months of training. Developing SLMs to excel within speciﬁc domains for\nrobotic systems and ensuring real-time performance with minimal computational resources\nare essential research directions for advancing robot intelligence with SLMs.\nThe third implication is that LLMs, based on text-centered natural language process-\ning, are limited as single-modality models when applied to real-world robotic systems\nwhere information often blends in diverse ways. Research on LLMs is transitioning from\nsingle-modality to multimodality models, as evidenced by VLMs and OpenAI Sora [228],\nwith increasing demand for such models. Currently, to address the limitations of LLMs’\nsingle-modality, robotic systems are being developed with multimodality models that\nintegrate vision, such as VLMs. However, relying solely on text and images falls short\nof the diverse information range required in the real world, including images, sounds,\nvideos, and proprioceptive sensory information (such as the position, orientation, balance,\nmovement degree, and direction of various parts of the robot). Proprioceptive sensory\ninformation related to actions and movements is particularly vital for enhancing dynamic\nhuman interaction, information processing, and manipulation and planning skills based on\ndynamic movements. For instance, the integrated VLA model, which facilitates low-level\ncontrol based on LLMs and VLMs as shown by Google’s RT-2 model, highlights the neces-\nsity for models capable of integrating information from a broader range of modalities to\nenhance robot intelligence.Appl. Sci.2024, 14, 8868 29 of 39\nFinally, the fourth area to consider is how to address safety and ethical issues when\nLLM is applied to robotic intelligence systems. Studies were conducted to address the\nissue of discriminatory and unsafe behaviors that may be generated by robot applications\npowered by LLMs [229]. The outputs of LLMs have the potential to generate content\nthat is biased based on personal characteristics (such as race, nationality, religion, gender,\ndisability, and so forth). In addition, they can also be used to instruct robotic systems to\nengage in violent or illegal behaviors such as misstatements, sexual predation, etc. Notable\nexamples include discriminatory behaviors such as inadequate recognition of children\nor individuals with speciﬁc skin tones in human detection systems, and the exclusion\nof individuals with disabilities from task assignments. It is imperative to consider the\npotential social biases of LLM when integrating with robotic systems. Although this kind of\nconsideration was secondary in traditional robotic systems because of the limitation of their\nlanguage capability, it is a necessary consideration for LLMs to be able to generate human-\nlike language. To address this issue, previous studies have attempted to resolve it in various\nways, such as AutoRT’s constitutional rules [10], DrEureka’s safety instructions [134], and\nNeMo’s guardrails [230]. The guideline-based output control of LLMs can represent an\naccessible method to ensure safety.\nAs an extension of this point, safety issues can be identiﬁed when integrating LLMs\nand VLMs into robotic intelligence systems [231]. Typically, in robotic intelligence systems,\nLLM models generate high-level action plans in various forms, such as programming\ncodes and behavior trees based on natural language or vector prompts. At this point, a\nprompt attack has the potential to disrupt the inference of the LLMs, thereby threatening\nthe reliability and safety of the robotic system. Prompt injection is one of the prompt attacks,\nwhereby the inference of LLMs is subtly altered through speciﬁc inputs. Jailbreak, another\nprompt attack, bypasses safety rules and causes LLMs to generate abnormal behaviors to"
  },
  {
    "question": "What are the different feedback sources integrated into the inner monologue?",
    "chunk": "paper details experiments in a simulated tabletop rearrangement, a mini-grid 2D maze,\nand real-world kitchen mobile manipulation settings to evaluate long-horizon reasoning\nperformance. Comparative experiments with SayCan revealed that while SayCan limits\nthe range of robot actions, GD can represent a wider array of actions. In contrast to\nCLIPort, which executes high-level language instructions directly, GD achieves enhanced\nperformance through detailed, step-by-step planning.\nHuang [153], as shown in Figure8, proposed the inner monologue method, which\nallowed LLMs to plan and adjust based on feedback from the environment. This approach\nenabled robots to formulate plans in dynamic environments, retry upon facing failure,\nor seek human feedback to reﬁne their strategies. The author clariﬁed that this method\nemerged from integrating the LLM’s high-level planning capabilities with perceptual feed-\nback and low-level control, thereby facilitating more adaptable and intelligent interactions.\nInner monologue integrated various feedback sources into the language model to assist the\nrobot in executing given instructions, including text-based indicators of the robot’s action\nsuccess or failure, object recognition and descriptions within the scene, the robot’s ability to\nask questions to gather additional information, breaking down instructions into multiple\nsteps to establish an execution plan, and enabling the robot to interact with humans to\nexecute and reﬁne the instructions. The inner monologue method was evaluated in both\nsimulated and real-world environments, such as tabletop rearrangement tasks and manip-\nulation tasks in a real kitchen. The results showed that inner monologue was an effective\nframework, enabling robots to act intelligently in complex interactive settings by effectively\nintegrating environmental feedback to plan and execute tasks.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 20 of 39 \n \n \nFigure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable"
  },
  {
    "question": "How does SM interact with external APIs for enhanced functionality?",
    "chunk": "recovery feedback mechanism and environmental state feedback [156].\nRana [157] introduced SayPlan, a scalable method for large-scale task planning using\nLLMs and based on a 3D Scene Graph (3DSG) representation. SayPlan involved the LLM\nsearching a collapsed 3D scene graph and task instructions to identify all relevant items\nand then locating the subgraph that contained the necessary items to complete the task.\nThe identiﬁed subgraph was subsequently used by the LLM to generate a high-level plan\nthat addressed the navigational aspect of the task. This plan was formatted as a JSON\n3D scene graph and subjected to a repetitive replanning process through feedback from\nthe scene graph simulator and a set of API calls for manipulation and operation until an\nexecutable plan was determined. SayPlan was tested in two large-scale environments,\nfeaturing up to three ﬂoors, 36 rooms, and 140 assets and objects, proving its capability\nto ground large-scale and long-horizon task plans from abstract and natural language\ninstructions, thereby enabling a mobile manipulator robot to execute these tasks.\nZeng [158], as shown in Figure11, proposed the Socratic model (SM), a modular\nframework that synergistically utilizes various forms of knowledge and employs multiple\npre-trained models to exchange information and leverage new multimodal capabilities. SM\noperates without ﬁne-tuning by integrating diverse pre-trained models and functions in\na zero-shot approach (e.g., using multimodal prompts), which enables it to harness new\nmultimodal capabilities. SM demonstrated state-of-the-art performance in zero-shot image\ncaptioning and video-to-text retrieval, and it effectively answered free-form questions about\negocentric video. Additionally, it supported interactions with external APIs and databases\n(e.g., web search) for multimodal assistive dialogue, robot perception, and planning, among\nother novel applications.Appl. Sci.2024, 14, 8868 22 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 22 of 39 \n \nFigure 10. ProgPrompt is a system that uses Pyth on programming structures to provide \nenvironmental information and actions, enhancing the success rate of robot task planning through \nan error recovery feedback mechanism and environmental state feedback [156]. \nRana [157] introduced SayPlan, a scalable method for large-scale task planning using \nLLMs and based on a 3D Scene Graph (3DSG) representation. SayPlan involved the LLM \nsearching a collapsed 3D scene graph and task instructions to identify all relevant items \nand then locating the subgraph that contained the necessary items to complete the task. \nThe identiﬁed subgraph was subsequently used by the LLM to generate a high-level plan \nthat addressed the navigational aspect of the task. This plan was formatted as a JSON 3D \nscene graph and subjected to a repetitive replanning process through feedback from the \nscene graph simulator and a set of API calls for manipulation and operation until an \nexecutable plan was determined. SayPlan wa s tested in two large-scale environments, \nfeaturing up to three ﬂoors, 36 rooms, and 140 assets and objects, proving its capability to \nground large-scale and long-horizon task plans from abstract and natural language \ninstructions, thereby enabling a mobile manipulator robot to execute these tasks. \nZeng [158], as shown in Figure 11, proposed the Socratic model (SM), a modular \nframework that synergistically utilizes various forms of knowledge and employs multiple \npre-trained models to exchange information and leverage new multimodal capabilities."
  },
  {
    "question": "How do robots learn to adapt their actions based on language and visual inputs?",
    "chunk": "constraints from free-form language instructions. Further, by harnessing code generation\ncapabilities, Huang developed 3D value maps for the agent’s observation space through\ninteractions with VLMs. These 3D value maps were integrated into a model-based planning\nframework to generate closed-loop robot trajectories robust to dynamic perturbations in\na zero-shot approach. The proposed framework demonstrated efﬁcient learning of the\ndynamics model for scenes with contact-rich interactions and provided advantages in these\ncomplex scenarios.Appl. Sci.2024, 14, 8868 24 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 24 of 39 \n \nsuccess was achieved. This approach enhanced  t h e  s u c c e s s  r a t e  o f  d a t a  c o l l e c t i o n  a n d  \nmore e ﬀectively mitigated the low-level understanding gap in LLMs by incorporating \nretry processes as part of the robot’s experiences. The second component, distilling down \nto language-conditioned visuomotor policy, transformed robot experiences into a policy \nthat deduced control sequences from visual observations and natural language task \ndescriptions. By extending di ﬀusion policies, this component handled language-based \nconditioning for multi-task learning. To assess long-horizon behavior, commonsense \nreasoning, tool use, and intuitive physics, a new multi-task benchmark comprising 18 \ntasks related to robot manipulation across ﬁve domains (mailbox, transport, drawer, \ncatapult, and bus balance) was developed. This benchmark e ﬀectively supported the \nlearning of retry behaviors in the data collection process and enhanced success rates. \nHuang [165], as shown in Figure 12, aimed to synthesize dense robot trajectories, \nincluding 6-DoF end-eﬀector waypoints, for various manipulation tasks using an open set \nof instructions and objects. Huang note d that LLMs were skilled at deriving a ﬀordances \nand constraints from free-form language in structions. Further, by harnessing code \ngeneration capabilities, Huang developed 3D value maps for the agent’s observation \nspace through interactions with VLMs. Thes e 3D value maps were integrated into a \nmodel-based planning framework to generate closed-loop robot trajectories robust to \ndynamic perturbations in a zero-shot approach. The proposed framework demonstrated \neﬃcient learning of the dynamics model for sc enes with contact-rich interactions and \nprovided advantages in these complex scenarios. \n \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM to \ngenerate 3D a ﬀordance and constraint maps and design robot trajectories without additional \ntraining [165]. \nAhn [166] introduced a framework named SayCan, which integrates LLMs with \nreinforcement learning value functions, enabling robots to follow high-level text \ninstructions. SayCan comprises two primary components: Say, which uses an LLM for \ntask-based decision-making, and Can, which evaluates the feasibility of these decisions \nvia reinforcement learning. Say leverages task-based knowledge from the LLM and \nreinforcement learning functionality to assess the feasibility of task execution by robots in \nreal-world scenarios. The LLM determines th e actions necessary to achieve high-level \ngoals and evaluates the eﬀectiveness of each action in fulﬁlling the instructions. Learned \nthrough reinforcement learning, the a ﬀordance function estimate s each action’s success"
  },
  {
    "question": "What are some applications of SM in assistive dialogue?",
    "chunk": "SM operates without ﬁne-tuning by integrating diverse pre-trained models and functions \nin a zero-shot approach (e.g., using multimodal prompts), which enables it to harness new \nmultimodal capabilities. SM demonstrated state-of-the-art performance in zero-shot \nimage captioning and video-to-text retrieval, and it e ﬀectively answered free-form \nquestions about egocentric vi deo. Additionally, it supported interactions with external \nAPIs and databases (e.g., web search) for multimodal assistive dialogue, robot perception, \nand planning, among other novel applications. \n \nFigure 11. SM integrates various types of knowledge by using multiple pre-trained models and \nprovides meaningful results even in complex computer vision tasks such as image captioning, \ncontext inference, and activity prediction [158]. \nLin [159] introduced Text2Motion, a language-based framework designed to handle \nsequential manipulation tasks that requ ire long-horizon reasoning. Text2Motion \ninterpreted natural language instructions to formulate task plans and generated multiple \ncandidate skill sequences, evaluating the ge ometric feasibility of each sequence. By \nemploying a greedy search strategy, it selected  the optimal skill sequence to verify and \nexecute the ﬁnal plan. This method enabled Text2Motion to perform complex sequential \nmanipulation tasks with a higher success rate compared to existing language-based \nplanning methods, such as Saycan-gs an d Innermono-gs, and provided semantically \ngeneralized characteristics among skills with geometric relationships. \nFigure 11. SM integrates various types of knowledge by using multiple pre-trained models and\nprovides meaningful results even in complex computer vision tasks such as image captioning, context\ninference, and activity prediction [158].\nLin [159] introduced Text2Motion, a language-based framework designed to handle\nsequential manipulation tasks that require long-horizon reasoning. Text2Motion interpreted\nnatural language instructions to formulate task plans and generated multiple candidate\nskill sequences, evaluating the geometric feasibility of each sequence. By employing a\ngreedy search strategy, it selected the optimal skill sequence to verify and execute the ﬁnal\nplan. This method enabled Text2Motion to perform complex sequential manipulation tasks\nwith a higher success rate compared to existing language-based planning methods, such\nas Saycan-gs and Innermono-gs, and provided semantically generalized characteristics\namong skills with geometric relationships.\nWu [160] investigated personalization in-home cleaning robots that organize and tidy\nspaces, using an LLM to convert user-provided object placement locations into generalized\nrules. By using a camera to identify objects and CLIP to categorize them, TidyBot efﬁciently\nrelocated objects according to these rules. This method attained an impressive accuracy\nof 91.2% for unseen objects in a benchmark dataset, which encompassed a variety of\nobjects, receptacles, and example placements of both “seen” and “unseen” objects across\n96 scenarios. Additionally, it achieved an 85% success rate in removing objects during\nreal-world tests.\n4.4. Manipulation by LLMs\nIn robotics research, the manipulation domain, which includes robotic arms and\nend effectors, encompasses various areas that beneﬁt from foundation models such as\nLLMs for language-based interactions and VLMs for object handling. Among the studies\nintegrating manipulation with foundation models, Stone [161] introduced an approach\ncalled manipulation of open-world objects (MOO). This approach determined whether a\nrobot could follow instructions involving unseen object categories by linking pre-trained\nmodels to robotic policies. MOO utilized pre-trained vision-language models to derive\nobject information from language commands and images, guiding the robot’s actions based"
  },
  {
    "question": "What did the evaluations reveal about the generalization of the policy and value functions?",
    "chunk": "including 6-DoF end-eﬀector waypoints, for various manipulation tasks using an open set \nof instructions and objects. Huang note d that LLMs were skilled at deriving a ﬀordances \nand constraints from free-form language in structions. Further, by harnessing code \ngeneration capabilities, Huang developed 3D value maps for the agent’s observation \nspace through interactions with VLMs. Thes e 3D value maps were integrated into a \nmodel-based planning framework to generate closed-loop robot trajectories robust to \ndynamic perturbations in a zero-shot approach. The proposed framework demonstrated \neﬃcient learning of the dynamics model for sc enes with contact-rich interactions and \nprovided advantages in these complex scenarios. \n \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM to \ngenerate 3D a ﬀordance and constraint maps and design robot trajectories without additional \ntraining [165]. \nAhn [166] introduced a framework named SayCan, which integrates LLMs with \nreinforcement learning value functions, enabling robots to follow high-level text \ninstructions. SayCan comprises two primary components: Say, which uses an LLM for \ntask-based decision-making, and Can, which evaluates the feasibility of these decisions \nvia reinforcement learning. Say leverages task-based knowledge from the LLM and \nreinforcement learning functionality to assess the feasibility of task execution by robots in \nreal-world scenarios. The LLM determines th e actions necessary to achieve high-level \ngoals and evaluates the eﬀectiveness of each action in fulﬁlling the instructions. Learned \nthrough reinforcement learning, the a ﬀordance function estimate s each action’s success \nprobability in the current state, con ﬁrming the executability of actions proposed by the \nLLM. This process allows the LLM to assess the robot’s current state and capabilities, \nultimately generating an interpretable action plan. SayCan was evaluated across 101 robot \ntasks, achieving an 84% plan success rate and a 74% execution success rate in a simulated \nkitchen environment. In a real kitchen setting, the plan success rate decreased slightly to \n81% and the execution success rate fell to 60%, demonstrating that the policy and value \nfunctions generalize well to real-world settings. \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM\nto generate 3D affordance and constraint maps and design robot trajectories without additional\ntraining [165].\nAhn [166] introduced a framework named SayCan, which integrates LLMs with rein-\nforcement learning value functions, enabling robots to follow high-level text instructions.\nSayCan comprises two primary components: Say, which uses an LLM for task-based\ndecision-making, and Can, which evaluates the feasibility of these decisions via reinforce-\nment learning. Say leverages task-based knowledge from the LLM and reinforcement\nlearning functionality to assess the feasibility of task execution by robots in real-world\nscenarios. The LLM determines the actions necessary to achieve high-level goals and\nevaluates the effectiveness of each action in fulﬁlling the instructions. Learned through re-\ninforcement learning, the affordance function estimates each action’s success probability in\nthe current state, conﬁrming the executability of actions proposed by the LLM. This process\nallows the LLM to assess the robot’s current state and capabilities, ultimately generating\nan interpretable action plan. SayCan was evaluated across 101 robot tasks, achieving an\n84% plan success rate and a 74% execution success rate in a simulated kitchen environment."
  },
  {
    "question": "Can you explain how LLM-BRAIn generates behavior trees for robots?",
    "chunk": "robots to carry out instructions: (a) mobile manipulation and (b,c) tabletop manipulation, in both\nsimulated and real-world environments [153].Appl. Sci.2024, 14, 8868 20 of 39\nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn,\na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot\nbehavior trees (BTs) from textual descriptions. The developed model was compact enough\nto operate on a robot’s onboard microcomputer, while adept at constructing complex robot\nbehaviors. It provided structurally and logically correct BTs and demonstrated the ability\nto handle instructions that were not included in the training set.\nSong [155], as shown in Figure9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions\nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via\na low-level planner. It continuously updated environmental information as new objects\nwere detected during action implementation and revisited the LLM to adjust the plan if\nsubgoals failed or were delayed based on updated observations. This iterative process was\nrepeated until the subgoal was achieved, after which the system moved to the next goal.\nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated\ncompetitive performance with signiﬁcantly reduced training data and proved its ability to\ngeneralize in various tasks (e.g., ALFRED) with minimal examples.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 9.LLM-Planner is a system that creates high-level plans based on natural language commands,\nsets subgoals to determine actions, and continuously updates the plan to reﬂect environmental\nchanges [155].\nSingh [156], as shown in Figure10, introduced ProgPrompt, a programmatic LLM\nprompt structure designed for generating plans across diverse situated environments,\nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that\nleveraged LLMs and included a Python programming structure to facilitate information\nabout the environment and executable actions. It featured a feedback mechanism, using\nexecutable program plan examples and assertion statements to mitigate errors, enhancing"
  },
  {
    "question": "Can you explain the concept of reward design in reinforcement learning as it relates to navigation?",
    "chunk": "Zhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow\ninstructions. NavGPT is a vision-language navigation system that employs an LLM to trans-\nlate visual inputs from a visual foundation model (VFM) into natural language. The LLM\nthen interprets the current state and makes informed decisions to reach the intended goal,\nbased on these converted visuals, navigation history, and potential future routes. NavGPT\nconducts various functions, including high-level planning, decomposing instructions into\nsub-goals, identifying landmarks in observed scenes, monitoring navigation progress, and\nmodifying plans as necessary. Although NavGPT’s performance on zero-shot tasks from\nthe R2R dataset has not yet matched that of trained models, it underscored the potential\nof utilizing multi-modality inputs with LLMs for visual navigation and tapping into the\nexplicit reasoning capabilities of LLMs to enhance learned models.\nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-\ntrained vision-language features with a 3D reconstruction of the physical world. VLMaps,\nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary\nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands.\nThese commands can be directly localized on a map and generate new obstacle maps in real-Appl. Sci.2024, 14, 8868 27 of 39\ntime, facilitated by sharing among various robot types. Extensive experiments conducted in\nboth simulated environments (using the Habitat simulator with the Matterport3D dataset\nand the AI2THOR simulator) and real-world settings (with the HSR mobile robot for\nindoor navigation) demonstrated that VLMs can navigate based on more complex language\ninstructions than previous methods. The reviewed papers in this study are summarized in\nTable 5.\nTable 5.Summary of the reviewed papers in this study.\nName Explanation Ref.\nReward Design in RL\n• Eureka automatically generates and improves reward\nfunctions based on the virtual environment source\ncode.\n• DrEureka builds reward-aware physics priors using\nEureka and supports effective operation in the real\nworld through domain randomization.\n• LLMs design and reﬁne reward functions based on\nnatural language input.\n• LLMs and VLMs integrate multimodal data to\ngenerate reward functions.\n[11,134,136–139,176–180]\nLow-level\nControl\n• Generating commands to control actuators capable of\nlow-level control.\n• RT-1 and RT-2 enable robots to perform complex tasks\nbased on language-vision data.\n• AutoRT establishes a system where robots can\nautonomously collect and utilize data.\n[8–10,144–148,181–183]\nHigh-level\nPlanning\n• LLMs provide an effective methodology for tasks\nrelated to high-level planning within robotic systems.\n• By using natural language, LLMs can formulate plans\nto solve tasks that require long-horizon reasoning.\n• LLMs assess the feasibility of actions to determine and\nexecute the optimal robotic behavior.\n• LLMs generate behavior trees to structure complex\nrobotic actions accurately.\n[149–160,184–207]\nManipulation\n• Using LLMs and VLMs to integrate language and\nvision data allows various manipulations.\n• LLMs interpret high-level instructions to generate the\nnecessary robot actions and assess their feasibility.\n• VLMs extract object information from images to assist\nin performing manipulations.\n[161–167,208–215]\nScene\nUnderstanding\n• To solve VQA problems, use VLMs to extract\nhigh-level information from vision data."
  },
  {
    "question": "How can LLMs help robots adapt to changing environments?",
    "chunk": "Citation: Jeong, H.; Lee, H.; Kim, C.;\nShin, S. A Survey of Robot\nIntelligence with Large Language\nModels. Appl. Sci.2024, 14, 8868.\nhttps://doi.org/10.3390/app14198868\nAcademic Editors: Luis Gracia and J.\nErnesto Solanes\nReceived: 6 September 2024\nRevised: 24 September 2024\nAccepted: 25 September 2024\nPublished: 2 October 2024\nCopyright: © 2024 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\napplied  sciences\nReview\nA Survey of Robot Intelligence with Large Language Models\nHyeongyo Jeong1,† , Haechan Lee1,† , Changwon Kim2,* and Sungtae Shin1,*\n1 Department of Mechanical Engineering, Dong-A University, Busan 49315, Republic of Korea\n2 School of Mechanical Engineering, Pukyong National University, Busan 48513, Republic of Korea\n* Correspondence: ckim@pknu.ac.kr (C.K.); stshin@dau.ac.kr (S.S.)\n† These authors contributed equally to this work.\nAbstract: Since the emergence of ChatGPT, research on large language models (LLMs) has actively\nprogressed across various ﬁelds. LLMs, pre-trained on vast text datasets, have exhibited exceptional\nabilities in understanding natural language and planning tasks. These abilities of LLMs are promis-\ning in robotics. In general, traditional supervised learning-based robot intelligence systems have a\nsigniﬁcant lack of adaptability to dynamically changing environments. However, LLMs help a robot\nintelligence system to improve its generalization ability in dynamic and complex real-world environ-\nments. Indeed, ﬁndings from ongoing robotics studies indicate that LLMs can signiﬁcantly improve\nrobots’ behavior planning and execution capabilities. Additionally, vision-language models (VLMs),\ntrained on extensive visual and linguistic data for the vision question answering (VQA) problem,\nexcel at integrating computer vision with natural language processing. VLMs can comprehend visual\ncontexts and execute actions through natural language. They also provide descriptions of scenes\nin natural language. Several studies have explored the enhancement of robot intelligence using\nmultimodal data, including object recognition and description by VLMs, along with the execution\nof language-driven commands integrated with visual information. This review paper thoroughly\ninvestigates how foundation models such as LLMs and VLMs have been employed to boost robot\nintelligence. For clarity, the research areas are categorized into ﬁve topics: reward design in rein-\nforcement learning, low-level control, high-level planning, manipulation, and scene understanding.\nThis review also summarizes studies that show how foundation models, such as the Eureka model\nfor automating reward function design in reinforcement learning, RT-2 for integrating visual data,\nlanguage, and robot actions in vision-language-action models, and AutoRT for generating feasible\ntasks and executing robot behavior policies via LLMs, have improved robot intelligence.\nKeywords: embodied intelligence; foundation model; large language model (LLM); vision-language\nmodel (VLM); vision-language-action (VLA) model; robotics\n1. Introduction\nTo enhance the intelligence of robots in real-world environments that interact with\nhumans, developing robots capable of perceiving, acting, and interacting like humans is"
  },
  {
    "question": "What challenges do robots face when executing plans in dynamic environments?",
    "chunk": "mental feedback during plan execution and revised the plan accordingly. The results\nindicated that the integration of programming language features substantially improved\ntask performance in contexts such as VirtualHome and real-world manipulation tasks in\nterms of success rate, goal conditions recall, and executability.Appl. Sci.2024, 14, 8868 21 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 10.ProgPrompt is a system that uses Python programming structures to provide environ-\nmental information and actions, enhancing the success rate of robot task planning through an error\nrecovery feedback mechanism and environmental state feedback [156].\nRana [157] introduced SayPlan, a scalable method for large-scale task planning using\nLLMs and based on a 3D Scene Graph (3DSG) representation. SayPlan involved the LLM\nsearching a collapsed 3D scene graph and task instructions to identify all relevant items\nand then locating the subgraph that contained the necessary items to complete the task.\nThe identiﬁed subgraph was subsequently used by the LLM to generate a high-level plan\nthat addressed the navigational aspect of the task. This plan was formatted as a JSON\n3D scene graph and subjected to a repetitive replanning process through feedback from\nthe scene graph simulator and a set of API calls for manipulation and operation until an\nexecutable plan was determined. SayPlan was tested in two large-scale environments,\nfeaturing up to three ﬂoors, 36 rooms, and 140 assets and objects, proving its capability\nto ground large-scale and long-horizon task plans from abstract and natural language\ninstructions, thereby enabling a mobile manipulator robot to execute these tasks.\nZeng [158], as shown in Figure11, proposed the Socratic model (SM), a modular\nframework that synergistically utilizes various forms of knowledge and employs multiple\npre-trained models to exchange information and leverage new multimodal capabilities. SM\noperates without ﬁne-tuning by integrating diverse pre-trained models and functions in\na zero-shot approach (e.g., using multimodal prompts), which enables it to harness new\nmultimodal capabilities. SM demonstrated state-of-the-art performance in zero-shot image\ncaptioning and video-to-text retrieval, and it effectively answered free-form questions about\negocentric video. Additionally, it supported interactions with external APIs and databases\n(e.g., web search) for multimodal assistive dialogue, robot perception, and planning, among"
  },
  {
    "question": "What advancements does the LLM-Grounder bring to visual grounding?",
    "chunk": "In a real kitchen setting, the plan success rate decreased slightly to 81% and the execution\nsuccess rate fell to 60%, demonstrating that the policy and value functions generalize well\nto real-world settings.\nHuang [167] introduced the Instruct2Act framework, which employs LLMs to se-\nquentially map multi-modality instructions to robot actions. The previous method, CaP,\ngenerated robot policy program code directly from in-context examples based on language\ninstructions. However, this approach was constrained by the capabilities of the generated\ncode and encountered difﬁculties with longer, more complex commands due to the required\nhigh precision of code. To overcome these limitations, Instruct2Act introduced a novel\nstrategy that used multi-modality models and LLMs to simultaneously address recognition,\ntask planning, and low-level control modules. Instruct2Act utilized the segment anything\nmodel for identifying potential objects in input images for multi-modality recognition\nand the CLIP model for object classiﬁcation. As a result, Instruct2Act developed an inte-\ngrated search system capable of managing various input modalities and instruction types,\nincluding both pure language instructions and combined language-visual instructions,\nfacilitating the integration of diverse instruction types into a uniﬁed architecture. Moreover,\nfor pointer-language instructions, the framework supported task segmentation based on\nthe user’s clicks.\n4.5. Scene Understanding in LLMs and VLMs\nTo address the VQA problem, robotics research increasingly uses pre-trained VLMs\nto derive high-level information from visual data. This method is advantageous for scene\nunderstanding as it helps determine affordances that describe the relationship between the\ncurrent state and the next action based on images from cameras. Related studies focus on\naspects of scene understanding.Appl. Sci.2024, 14, 8868 25 of 39\nChen [168] explored methods to integrate commonsense into scene understanding\nusing LLMs and introduced three paradigms for classifying room types within indoor\nenvironments based on included objects. The zero-shot approach utilized a pre-trained\nlanguage model to identify the objects in a room and estimate their types. The feed-\nforward classiﬁer approach involved inputting sentences that listed a room’s objects into\nthe language model to generate embedding vectors, which were subsequently input into a\npre-trained shallow multilayer perceptron to predict each room type. Lastly, the classiﬁer\napproach embedded images of rooms alongside textual descriptions to identify the best-\nmatching description, thereby determining the room type. These paradigms demonstrated\nthe capacity to generalize to objects not presented in the training set and to make inferences\nwithin a space larger than that deﬁned by the trained object labels.\nYang [169] introduced the innovative zero-shot, open-vocabulary, LLM-based 3D\nvisual grounding pipeline called LLM-Grounder. This method breaks down complex natu-\nral language queries into semantic components and uses visual grounding tools such as\nOpenScene or LERF to locate objects within 3D scenes. Subsequently, the LLM evaluates\nspatial and commonsense relationships among these objects to achieve the ﬁnal grounding.\nRemarkably, LLM-Grounder operates without labeled training data and has proven its ca-\npacity to adapt to new 3D scenes and diverse text queries, enhancing grounding capabilities\nfor complex language queries and establishing itself as an effective solution.\nChen [170] developed NLMap, an open-vocabulary, queryable scene representation\nsystem. Designed to accumulate and incorporate contextual data within a scene repre-\nsentation for natural language queries, this system allows an LLM planner to visualize"
  },
  {
    "question": "What future research directions are suggested for LLMs in robotics?",
    "chunk": "understand the situation and an LLM to propose possible tasks. By inputting the robot’s\noperational guidelines and safety constraints into the LLM as prompts, AutoRT assesses\nthe validity of the proposed tasks and the necessity for human intervention. Throughout\nthis process, AutoRT safely selects and executes feasible tasks while collecting relevant\ndata.\nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for\nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical causal-\nity in the real world, problem-solving through trial-and-error feedback, and code generation\nabilities. Eureka can autonomously generate reward functions for a variety of tasks and\nrobots without needing speciﬁc templates for each. This allows for the generation of\nhuman-level reward functions for diverse robots and tasks without human input. Further-\nmore, Eureka has demonstrated the ability to solve complex problems that were previously\nunsolved by expert-designed reward functions.\nGiven these research outcomes, integrating language models into robotic intelligence\npresents signiﬁcant potential to enhance robot capabilities and applications dramatically,\nthereby redeﬁning their roles in diverse industries and everyday life. Therefore, this survey\npaper explores recent research trends in LLM- and VLM-based robot intelligence, aiming to\nprovide a comprehensive understanding of future development possibilities by examining\nthe application of language models in various robotic research ﬁelds. It also seeks to\nhighlight research cases, identify current limitations, and suggest future research directions.\nTo chronicle this advancement in robotics research ﬁelds, this review paper presents\nthe following contributions:\n• This paper summarizes and introduces the foundational elements and tuning methods\nof LLM architecture.\n• It explores and arranges prompt techniques to enhance the problem-solving abilities\nof LLMs.\n• It reviews and encapsulates how LLMs and VLMs have been employed to augment\nrobot intelligence across ﬁve topics as shown in Figure1: (1) reward design for\nreinforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation,\nand (5) scene understanding.\nAppl. Sci. 2024 , 14 , x FOR PEER REVIEW 3 of 39 \n \nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which \nenable low-level actuator control using LLM s and VLMs, Google has introduced AutoRT \n[10]. AutoRT is a system where robots interact  with real-world objects to collect motion \ndata. It begins by exploring th e surrounding space to identify feasible tasks, then uses a \nVLM to understand the situation and an LLM to propose possible tasks. By inpu tting the \nrobot’s operational guidelines and safety co nstraints into the LLM as prompts, AutoRT \nassesses the validity of the proposed tasks and the necessity for human intervention. \nThroughout this process, AutoRT safely selects and executes feasible tasks while collecting \nrelevant data. \nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for \nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical cau-\nsality in the real world, problem-solving through trial-and-error feedback, and code gen-\neration abilities. Eureka can autonomously generate reward functions for a variety of tasks \nand robots without needing speci ﬁc templates for each. This allows for the generation of \nhuman-level reward functions for diverse robo ts and tasks without human input. Further-"
  },
  {
    "question": "Can you describe the results of using LLM-Planner and ProgPrompt in real-world tasks?",
    "chunk": "Figure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 9.LLM-Planner is a system that creates high-level plans based on natural language commands,\nsets subgoals to determine actions, and continuously updates the plan to reﬂect environmental\nchanges [155].\nSingh [156], as shown in Figure10, introduced ProgPrompt, a programmatic LLM\nprompt structure designed for generating plans across diverse situated environments,\nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that\nleveraged LLMs and included a Python programming structure to facilitate information\nabout the environment and executable actions. It featured a feedback mechanism, using\nexecutable program plan examples and assertion statements to mitigate errors, enhancing\ntask success rates. Additionally, ProgPrompt veriﬁed the current state through environ-\nmental feedback during plan execution and revised the plan accordingly. The results\nindicated that the integration of programming language features substantially improved\ntask performance in contexts such as VirtualHome and real-world manipulation tasks in\nterms of success rate, goal conditions recall, and executability.Appl. Sci.2024, 14, 8868 21 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 10.ProgPrompt is a system that uses Python programming structures to provide environ-\nmental information and actions, enhancing the success rate of robot task planning through an error"
  },
  {
    "question": "What advantages does EmbodiedGPT offer in terms of reasoning about tasks?",
    "chunk": "state representation, while the world model writer updated the system’s state according to\nexecution outcomes. By facilitating access to the world state ‘memory’, Statler improved\nLLMs’ ability to reason about planning tasks with extended time horizons, overcoming\nlimitations imposed by context length.\nMu [150], shown in Figure7, introduced EmbodiedGPT, a model speciﬁcally designed\nfor Embodied AI, which leverages LLMs. This framework processes visual observations\nand natural language to establish long-term plans and execute tasks in real-time. Em-\nbodiedGPT utilizes pre-trained vision transformers and the LLaMA language model to\nencode visual features and map them to the language modality. The generated plan was\nsubsequently converted into speciﬁc task commands using general visual tokens, encoded\nby the vision model. The framework’s functionality comprises (1) encoding current visual\nfeatures, (2) mapping visual features to the language modality via attention-based interac-\ntions between visual tokens and text queries or learnable embedded queries, (3) generating\nplans with the LLaMA language model and translating them into speciﬁc task commands,\nand (4) querying the encoded visual tokens from the vision model and translating them\ninto low-level control commands through a downstream policy network for task execu-\ntion. Experimental results, utilizing the MS-COCO dataset, revealed that EmbodiedGPT\nexcels in object recognition and understanding spatial relationships. Notably, implement-Appl. Sci.2024, 14, 8868 18 of 39\ning a closed-loop design and a “chain-of-thought” training mode signiﬁcantly enhanced\nEmbodiedGPT’s performance. These results demonstrate that EmbodiedGPT effectively\nhandles various autonomous tasks, exhibiting superior capability in object recognition,\nunderstanding spatial relationships, and generating logical, executable plans.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 18 of 39 \n \ntime. EmbodiedGPT utilizes pre-trained visi on transformers and the LLaMA language \nmodel to encode visual features and map them  to the language modality. The generated \nplan was subsequently converted into speci ﬁc task commands using general visual \ntokens, encoded by the vision model. The framework’s functionality comprises (1) \nencoding current visual features, (2) mapping visual features to the language modality \nvia a ttention-based interactions between visual tokens and text queries or learnable \nembedded queries, (3) generating plans with the LLaMA language model and translating \nthem into speciﬁc task commands, and (4) querying the encoded visual tokens from the \nvision model and translating them into low-level control commands through a \ndownstream policy network for task execution. Experimental results, utilizing the MS-\nCOCO dataset, revealed that EmbodiedG PT excels in object recognition and \nunderstanding spatial relationships. Notabl y, implementing a closed-loop design and a \n“chain-of-thought” training mode signi ﬁcantly enhanced EmbodiedGPT’s performance. \nThese results demonstrate that EmbodiedGPT e ﬀectively handles various autonomous \ntasks, exhibiting superior capability in object recognition, understanding spatial \nrelationships, and generating logical, executable plans. \n \nFigure 7. After encoding visual features, they are mappe d using visual tokens and text queries. A \nplan is then created with the LLaMA model and turned into task commands. The visual tokens are \nqueried and converted into low-level control commands to perform the task [150]. \nChen [151] introduced the language-model-based commonsense reasoning (LMCR)"
  },
  {
    "question": "How does the architecture of LLMs relate to their training methods?",
    "chunk": "2021-11 ViTMAE Facebook AI [ 93] 2024-04 IDEFICS 2 Hugging Face [ 94]\n2021-12 Stable\nDiffusion LMU Munich, IWR [95] 2024-05 Ideﬁcs2 Hugging Face [ 96]\n2022-03 R3M Meta AI, Stanford\nUniv. [97] 2024-05 Chameleon Meta AI Research [ 98]\n2022-04 Flamingo DeepMind [ 99] 2024-07 InternLM-\nXComposer-2.5 Shanghai AI Lab. [ 98]\n2023-01 BLIP-2 Salesforce Research [100] 2024-07 PaliGemma Univ. of Hong\nKong [101]\n2023-04 SAM Meta AI Research [ 102] 2024-08 SAM 2 Meta AI [ 103]\n2023-04 DINOv2 Meta AI Research [ 104] 2024-08 Qwen-VL2 Alibaba Group [ 105]Appl. Sci.2024, 14, 8868 8 of 39\n3.2. LLM Architectures and Tunings\nThe architecture of LLMs fundamentally utilizes the transformer architecture, with\nthree representative types based on different transformer conﬁgurations as shown in\nFigure 2. Firstly, the prevalent encoder–decoder structure of transformers employs the\nencoder to process the input sequence and generate a latent representation through multi-\nhead self-attention layers; the decoder then uses cross-attention on this representation to\nautoregressively produce the target sequence. Notable encoder–decoder PLMs include\nT5 [47] and BART [33], with Flan-T5 [106] being an encoder–decoder-based LLM. Secondly,\nthe causal decoder employs a unidirectional attention mask to restrict each input token\nto attend only to past and present tokens, processing input and output tokens similarly\nthrough the decoder. This method underpins the development of the GPT series. Lastly, the\npreﬁx decoder, resembling the causal decoder’s masking mechanism, allows bidirectional\nattention on preﬁx tokens [107] and unidirectional attention on generated tokens. Similar\nto the encoder–decoder, the preﬁx decoder bidirectionally encodes the preﬁx sequence and\nsequentially predicts output tokens individually. Examples of preﬁx decoder-based LLMs\ninclude GLM-130B [108] and U-PaLM [109]. Additionally, various architectures have been\nproposed to address efﬁciency challenges during training or inference with long inputs,\ndue to the quadratic computational complexity of the traditional transformer architecture.\nFor instance, the Mixture-of-Experts (MoE) scaling method [34] sparsely activates a subset\nof the neural network for each input.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 8 of 39 \n \n[47] and BART [33], with Flan-T5 [106] be ing an encoder–decoder-based LLM. Secondly, \nthe causal decoder employs a unidirectional attention mask to restrict each input token to \nattend only to past and present tokens, proc essing input and output tokens similarly \nthrough the decoder. This method underpins the development of the GPT series. Lastly, \nthe preﬁx decoder, resembling the causal decoder’s masking mechanism, allows bidirec-\ntional attention on preﬁx tokens [107] and unidirectional attention on generated tokens. \nSimilar to the encoder–decoder, the pre ﬁx decoder bidirectionally encodes the pre ﬁx se-"
  },
  {
    "question": "What is the purpose of self-consistency in reasoning?",
    "chunk": "demonstrations and calls on external tools as necessary to integrate their outputs into the\nreasoning process. The model generalizes from demonstrations using tools to decompose\nnew tasks and learns to use tools effectively. Enhancing ART’s performance is possible\nby modifying the task library or incorporating new tools.Automatic prompt engineer\n(APE) [129] is a framework designed for the automatic generation and selection of com-\nmands. The model generates command candidates for a problem and selects the most\nsuitable one based on a scoring function, such as execution accuracy or log probability.\nDirectional stimulus prompting[130] is a technique that directs the model to consider\nand generate responses in a particular direction. By deploying a tunable policy LM (e.g.,\nT5 [47]), it creates directional stimulus prompts for each input and uses these as cues to\nsteer the model toward producing the desired outcomes [131]. ReAct combines reasoning\nwith action within the model. It enables the model to perform reasoning in generating\nanswers, take actions based on external sources (e.g., documents, articles, and news), and\nreﬁne reasoning based on observations of these actions. This process facilitates the creation,\nmaintenance, and modiﬁcation of action plans while incorporating additional information\nfrom interactions with external sources.Reﬂexion [132] augments language-based agents\nwith language feedback. Reﬂexion involves three models: the actor, the evaluator, and self-\nreﬂection. The actor initiates actions within a speciﬁc environment to generate task steps,\nthe evaluator assesses these steps, and self-reﬂection provides linguistic feedback, which\nthe actor uses to formulate new steps and achieve the task’s objective. The introduced\nprompt techniques are summarized in Table4.\nTable 4.Prompt Techniques.\nName Explanation Ref.\nZero-Shot Prompting Enabling the model to perform new tasks without any examples [ 53]\nFew-Shot Prompting Providing a few examples to enable performing new tasks [ 49]\nChain-of-Thought Explicitly generating intermediate reasoning steps to perform\nstep-by-step inference [41]\nSelf-Consistency\nGenerating various reasoning paths independently through\nFew-Shot CoT, with each path going through a prompt generation\nprocess to select the most consistent answer\n[120]\nGenerated Knowledge Prompting\nIntegrating knowledge and information relevant to a question, and\nthen providing it along with the question to generate accurate\nanswers\n[126]\nPrompt Chaining Dividing a task into sub-tasks and connecting prompts for each\nsub-task as input–output pairs [125]\nTree of Thoughts\nDividing a problem into subproblems with intermediate steps that\nserve as “thoughts” towards solving the problem, where each\nthought undergoes an inference process and self-evaluates its\nprogress towards solving the problem\n[124]\nRetrieval Augmented Generation Combining external information retrieval with natural language\ngeneration [127]\nAutomatic Reasoning and Tool-use Using external tools to automatically generate intermediate\nreasoning steps [128]\nAutomatic Prompt Engineer Automatically generating and selecting commands [ 129]\nActive Prompt Addressing the issue that the effectiveness may be limited by\nhuman annotations [122]\nDirectional Stimulus Prompting Guiding the model to think and generate responses in a speciﬁc\ndirection [130]\nProgram-Aided Language Models Using models to understand natural language problems and\ngenerate programs as intermediate reasoning steps [123]\nReAct Combining reasoning and actions within a mode [ 131]\nReﬂexion Enhancing language-based agents through language feedback [ 132]"
  },
  {
    "question": "What challenges do robots face when dealing with incomplete instructions?",
    "chunk": "framework to assist robots in comprehendin g incomplete natural language instructions. \nThis framework enabled robots to receive instructions in natural language from humans, \nobserve their surroundings, and employ a commonsense reasoning method to \nautonomously infer missing information. LMCR utilized a model of commonsense \nreasoning learned from web-based text materials, allowing robots to understand \nincomplete instructions and autonomously execute tasks. The framework comprised three \nmain functions: language understanding, commonsense reasoning, and action planning. \nIn language understanding, LMCR translated human natural language instructions into a \nform interpretable by robots, parsing them into verb frames to convert them into \nexecutable structures. During the common sense reasoning phase, the robot analyzed \nsurrounding objects and employed a language model trained on large-scale unstructured \ntext materials to ﬁll in the missing details from the instructions. This model identiﬁed the \nFigure 7.After encoding visual features, they are mapped using visual tokens and text queries. A\nplan is then created with the LLaMA model and turned into task commands. The visual tokens are\nqueried and converted into low-level control commands to perform the task [150].\nChen [151] introduced the language-model-based commonsense reasoning (LMCR)\nframework to assist robots in comprehending incomplete natural language instructions.\nThis framework enabled robots to receive instructions in natural language from humans, ob-\nserve their surroundings, and employ a commonsense reasoning method to autonomously\ninfer missing information. LMCR utilized a model of commonsense reasoning learned\nfrom web-based text materials, allowing robots to understand incomplete instructions and\nautonomously execute tasks. The framework comprised three main functions: language\nunderstanding, commonsense reasoning, and action planning. In language understanding,\nLMCR translated human natural language instructions into a form interpretable by robots,\nparsing them into verb frames to convert them into executable structures. During the\ncommonsense reasoning phase, the robot analyzed surrounding objects and employed a\nlanguage model trained on large-scale unstructured text materials to ﬁll in the missing\ndetails from the instructions. This model identiﬁed the most suitable verb frame to com-\nplete the gaps. Subsequently, based on the completed verb frame, the robot formulated its\nactions using predeﬁned action plans for each verb to guide the movements of the robot\narm and execute the assignment. Experimental results showed that LMCR demonstrated\nsuperior generalization performance for novel concepts not presented in the training set\nand surpassed GCNGrasp, which depends on a predeﬁned graph structure for all concepts\nand their relationships. This indicated that LMCR was an effective tool, combining the se-\nmantic reasoning capabilities of language models with planning that adapted to the robot’s\nspeciﬁc environment and context, effectively managing complex and prolonged tasks.\nHuang [152] introduced a methodology named grounded decoding (GD), which offers\na method for generating LLM-based robot action plans. These plans enable robots to execute\nlong-term tasks across diverse physical environments. The methodology encompasses two\nprimary elements: linking the text generated by the language model to actionable taskAppl. Sci.2024, 14, 8868 19 of 39\ncommands in the physical world via GD and adjusting the tokens generated by the LLM\nto real-world conditions to formulate feasible commands. This approach synergizes the\nhigh-level semantic reasoning of LLMs with plans that are aligned with the robot’s physical\nenvironment and capabilities, thus facilitating the execution of complex and long-term\ntasks. The method addresses several limitations robots face in performing complex, long-\nterm tasks, such as a lack of physical world experience, an inability to process non-verbal"
  },
  {
    "question": "How do different structures of language models impact their capabilities?",
    "chunk": "characteristics, numerous subsequent studies employing pre-training and ﬁne-tuning have\nbeen introduced, featuring varied structures [33,34] (e.g., BART [33] and GPT-2 [35]) and\nenhancing pre-training strategies [36–38].\nBased on subsequent studies, it has been found that increasing the model size or data\nsize of PLMs typically enhances the performance of LM models [39]. This has prompted\nresearch into training large-scale PLMs, such as GPT-3 with 175B parameters and PaLM\nwith 540B parameters. The focus of this research, grounded in scaling laws, primarily\ncenters on augmenting model sizes and exploring the capabilities of larger models. These\ncapabilities, known as the emergent abilities of LLMs, have sparked signiﬁcant interest. For\nexample, GPT-3 can address problems it has not been trained on with minimal examples\nthrough in-context learning, a feat GPT-2 ﬁnds challenging. Due to these characteristics,\nthe academic community commonly designates these large PLMs as LLMs [40–43]. Conse-\nquently, research in this area is highly active. Notably, since the introduction of OpenAI’s\nChatGPT, there has been a surge in the number of arXiv papers on LLMs. Following\nMicrosoft’s announcement [2] about integrating ChatGPT into robotics, a variety of studies\nhave explored the application of LLMs across different areas of robotics research. The\navailable LLM models are presented in chronological order in Table2. Additionally, Table3\nincludes the VLM models.Appl. Sci.2024, 14, 8868 7 of 39\nTable 2.Chronicle of LLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref.\n2018-06 GPT-1 OpenAI [ 44] 2024-02 OLMo Allen Institute for\nAI [45]\n2019-02 GPT-2 OpenAI [ 35] 2024-02 StarCoder2 Hugging Face [ 46]\n2019-10 T5 Google [ 47] 2024-03 Claude 3 Anthropic [ 48]\n2020-05 GPT-3 OpenAI [ 49] 2024-03 InternLM2 Shanghai AI Lab [ 50]\n2021-07 Codex OpenAI [ 51] 2024-03 Jamba AI21Labs [ 52]\n2021-09 FLAN Google [ 53] 2024-04 Stabe Code Stability AI [ 54]\n2021-10 T0 Hugging Face [ 37] 2024-04 HyperCLOVA Naver [ 55]\n2021-12 Gopher DeepMind [ 56] 2024-04 Grok-1.5 xAI [ 57]\n2022-03 InstructGPT OpenAI [ 58] 2024-04 Llama3 Meta AI Research [ 59]\n2022-04 PaLM Google [ 60] 2024-04 Phi-3 Microsoft [ 61]\n2022-05 OPT Meta AI Research [ 62] 2024-05 GPT-4o OpenAI [ 1]\n2023-02 LLaMA Meta AI Research [ 63] 2024-06 Claude 3.5 Anthropic [ 64]\n2023-03 Alpaca Stanford Univ. [ 65] 2024-07 GPT-4o mini OpenAI [ 66]\n2023-03 GPT-4 OpenAI [ 50] 2024-07 Falcon2-11B TII [ 67]"
  },
  {
    "question": "In what ways can LLMs act as sequence modelers?",
    "chunk": "supporting the creation of high-level policies for robots and accommodating a variety of\nrobotic tasks. Speciﬁcally, CaP interpreted natural language instructions through descrip-\ntions and formulated an action plan for the robot. Moreover, it utilized VLMs such as ViLD\nand MDETR to identify objects and ascertain their locations. Based on this information,\nthe framework controlled the robot’s movements to carry out speciﬁed tasks. The paper\ndemonstrated the CaP framework across diverse domains, including whiteboard drawing,\ntabletop manipulation, and mobile robot navigation and manipulation. Experimental\nresults showed that CaP achieved similar or better success rates than existing systems\nsuch as CLIPort, displaying notably strong generalization capabilities for new tasks. These\nﬁndings underscored the ﬂexibility and efﬁcacy of the CaP framework, establishing its\neffectiveness across various robotic systems.\nMirchandani [148], shown in Figure6, suggested that pre-trained LLMs could autore-\ngressively complete complex token sequences and function as general sequence modelers\nthrough in-context learning without needing additional training. Expanding on this con-\ncept, the study evaluated LLMs’ ability to operate as pattern machines in three domains:\nsequence transformation, sequence completion, and sequence improvement. In sequence\ntransformation, the research demonstrated that LLMs could generalize speciﬁc sequence\ntransformations using benchmarks such as ARC (abstraction and reasoning corpus) and\nPCFG (probabilistic context-free grammar), thereby proving their utility in spatial reasoning\ntasks for robotics. In sequence completion, the study examined whether LLMs could ﬁnish\npatterns in elementary functions (e.g., sinusoids), illustrating their utility in robotic tasks\nsuch as extending a wiping motion from kinesthetic demonstrations or creating drawings\non a whiteboard. Finally, in sequence improvement, the research revealed that by utilizing\nreward-labeled trajectories as context and incorporating online interaction, LLM-based\nagents could explore small grids and reﬁne simple trajectories using human-in-the-loop\nmethods, such as optimizing a CartPole controller.Appl. Sci.2024, 14, 8868 17 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 17 of 39 \n \nsequence transformation, the research demonstrated that LLMs could generalize speci ﬁc \nsequence transformations using benchmarks such as ARC (abstraction and reasoning \ncorpus) and PCFG (probabilistic context-free grammar), thereby proving their utility in \nspatial reasoning tasks for robotics. In sequence completion, the study examined whether \nLLMs could ﬁnish pa tterns in elementary functions (e.g., sinusoids), illustrating their \nutility in robotic tasks such as extending a wiping motion from kinesthetic demonstrations \nor creating drawings on a whiteboard. Finally, in sequence improvement, the research \nrevealed that by utilizing reward-labeled trajectories as context and incorporating online \ninteraction, LLM-based agents could explore small grids and re ﬁne simple trajectories \nusing human-in-the-loop methods, such as optimizing a CartPole controller.  \n \nFigure 6. Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed \nin sequence transformation, completion, and improvement [148]. \n4.3. High-Level Planning (Including Decision-Making and Reasoning) \nThe abstraction and generalization capabilities of LLMs oﬀer eﬀective methodologies \nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various \nresearch outcomes have been realized in the ﬁelds of planning, decision-making,"
  },
  {
    "question": "How does step-by-step reasoning improve model performance?",
    "chunk": "reasoning steps, enabling the model to perform step-by-step reasoning. This approach\nallows the model to incrementally solve complex tasks. For instance, when asked, “If\nsomeone’s age will be 30 in 5 years, how old are they now?”, the model uses the information\n“age in 5 years is 30” to perform the intermediate reasoning step of “30→ 5 = 25” to derive\nthe ﬁnal answer.Self-consistency [120] involves the model generating various independent\nreasoning paths through few-Shot CoT, ultimately selecting the most consistent answer\namong the outputs. This method enhances the performance of CoT prompts in both\narithmetic and commonsense reasoning tasks.Multimodal CoT [121] is a two-stage\nframework that integrates text and visual modalities. Initially, intermediate reasoning steps\nare generated through rationale generation based on multimodal data. Subsequently, the\nanswer inferences are intertwined, and the informative rationales are utilized to derive the\nﬁnal answer.\nGenerally, CoT relies on human-generated annotations, which may not always provide\nthe optimal solution for problem-solving. To overcome this limitation,active prompt[122]\nhas been proposed. Active prompt enhances model performance by intensively training\nthe model on questions with higher uncertainty levels. It evaluates the uncertainty of\nanswers by posing questions to the model, with or without CoT examples. Questions with\nhigh uncertainty are selected for human annotation, and newly annotated examples are\nused to reason through each question.Program-aided language models(PAL) [123] is a\ntechnique that employs the model to understand natural language problems and generate\nprograms as intermediate reasoning steps. Unlike CoT, PAL solves problems stepwise\nusing a program runtime such as Python rather than free-form text.\nTree of thoughts(ToT) [124] is a method whereby the model breaks down a problem\ninto smaller units called thoughts, which it then assesses through a reasoning process to\ngauge its progress toward a solution. The ability of the model to generate and evaluate these\nthoughts is integrated with search algorithms such as breadth-ﬁrst and depth-ﬁrst search,\nfacilitating systematic thought exploration with lookahead and backtracking capabilities.\nIn contrast to the CoT method, which addresses problems sequentially, ToT concurrently\nexamines multiple pathways to ﬁnd a solution.Prompt chaining[125] is a strategy where\nthe model divides a task into sub-tasks, uses the outputs of each sub-task as subsequent\ninputs, and links prompts in input–output pairs. This approach improves the precision and\nconsistency of the outputs at each stage and simpliﬁes the handling of complex tasks by\nsubdividing them into manageable sub-tasks.\nGenerated knowledge prompting[126] is a technique in which the model incorpo-\nrates knowledge and information pertinent to the question and provides it alongside the\nquestion to generate more accurate answers. This method not only enhances the common-\nsense reasoning capabilities but also retains the ﬂexibility of existing models.Retrieval\naugmented generation(RAG) [127] merges external information retrieval with natural lan-\nguage generation. RAG can be ﬁne-tuned for knowledge-intensive downstream tasks and\nenables straightforward modiﬁcations or additions of knowledge within the framework.\nThis facilitates an increase in the model’s factual consistency, enhances the reliability of\ngenerated responses, and helps alleviate issues with hallucination.Automatic reasoning\nand tool-use(ART) [128] is a framework that utilizes external tools to autonomously gen-Appl. Sci.2024, 14, 8868 11 of 39\nerate intermediate reasoning steps. It chooses relevant tasks from a library that includes"
  },
  {
    "question": "What is the role of external APIs in the Socratic model's functionality?",
    "chunk": "other novel applications.Appl. Sci.2024, 14, 8868 22 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 22 of 39 \n \nFigure 10. ProgPrompt is a system that uses Pyth on programming structures to provide \nenvironmental information and actions, enhancing the success rate of robot task planning through \nan error recovery feedback mechanism and environmental state feedback [156]. \nRana [157] introduced SayPlan, a scalable method for large-scale task planning using \nLLMs and based on a 3D Scene Graph (3DSG) representation. SayPlan involved the LLM \nsearching a collapsed 3D scene graph and task instructions to identify all relevant items \nand then locating the subgraph that contained the necessary items to complete the task. \nThe identiﬁed subgraph was subsequently used by the LLM to generate a high-level plan \nthat addressed the navigational aspect of the task. This plan was formatted as a JSON 3D \nscene graph and subjected to a repetitive replanning process through feedback from the \nscene graph simulator and a set of API calls for manipulation and operation until an \nexecutable plan was determined. SayPlan wa s tested in two large-scale environments, \nfeaturing up to three ﬂoors, 36 rooms, and 140 assets and objects, proving its capability to \nground large-scale and long-horizon task plans from abstract and natural language \ninstructions, thereby enabling a mobile manipulator robot to execute these tasks. \nZeng [158], as shown in Figure 11, proposed the Socratic model (SM), a modular \nframework that synergistically utilizes various forms of knowledge and employs multiple \npre-trained models to exchange information and leverage new multimodal capabilities. \nSM operates without ﬁne-tuning by integrating diverse pre-trained models and functions \nin a zero-shot approach (e.g., using multimodal prompts), which enables it to harness new \nmultimodal capabilities. SM demonstrated state-of-the-art performance in zero-shot \nimage captioning and video-to-text retrieval, and it e ﬀectively answered free-form \nquestions about egocentric vi deo. Additionally, it supported interactions with external \nAPIs and databases (e.g., web search) for multimodal assistive dialogue, robot perception, \nand planning, among other novel applications. \n \nFigure 11. SM integrates various types of knowledge by using multiple pre-trained models and \nprovides meaningful results even in complex computer vision tasks such as image captioning, \ncontext inference, and activity prediction [158]. \nLin [159] introduced Text2Motion, a language-based framework designed to handle \nsequential manipulation tasks that requ ire long-horizon reasoning. Text2Motion \ninterpreted natural language instructions to formulate task plans and generated multiple \ncandidate skill sequences, evaluating the ge ometric feasibility of each sequence. By \nemploying a greedy search strategy, it selected  the optimal skill sequence to verify and \nexecute the ﬁnal plan. This method enabled Text2Motion to perform complex sequential \nmanipulation tasks with a higher success rate compared to existing language-based \nplanning methods, such as Saycan-gs an d Innermono-gs, and provided semantically \ngeneralized characteristics among skills with geometric relationships. \nFigure 11. SM integrates various types of knowledge by using multiple pre-trained models and\nprovides meaningful results even in complex computer vision tasks such as image captioning, context\ninference, and activity prediction [158].\nLin [159] introduced Text2Motion, a language-based framework designed to handle"
  },
  {
    "question": "Can you describe the training process for models like RT-1 in robot control?",
    "chunk": "agents’ understanding of human intentions. They developed a framework that utilized\nlanguage as a primary inference tool, investigating how it could address key challengesAppl. Sci.2024, 14, 8868 14 of 39\nin reinforcement learning, such as efﬁcient exploration, data reuse in experience, skill\nscheduling, and observational learning. This framework employed LLMs and VLMs to\naddress these reinforcement learning challenges by (1) efﬁciently exploring environments\nwith sparse rewards, (2) reusing collected data to sequentially bootstrap the learning of\nnew tasks, (3) scheduling learned skills for novel tasks, and (4) acquiring knowledge from\nobserving expert agents.\nDu [138] developed success detectors that identiﬁed whether actions or tasks were\nsuccessfully completed, utilizing the large multimodal language model Flamingo and\nhuman reward annotations. The study on success detection spanned three distinct do-\nmains: (1) interactive language-conditioned agents in simulated households, (2) real-world\nrobotic manipulation tasks (inserting and removing small, medium, and large gears), and\n(3) “in-the-wild” human egocentric videos. These success detectors adapted to new lan-\nguage instructions and visual changes using VLMs such as Flamingo, which were trained\non a broad range of language and visual data. Furthermore, success detection was reframed\nas a VQA problem, enabling the tracking of task progress through multiple frames to ascer-\ntain whether tasks had been successfully completed. The proposed method proved to be\nmore accurate in detecting success compared to custom reward models in the ﬁrst two do-\nmains, even with new language instructions or visual changes. However, success detection\nin unseen real-world videos in the third domain posed a more challenging generalization\ntask, underscoring the need for additional research.\nDu [139] introduced theELLM (exploring with LLMs) framework, which provided\nguidelines for pre-training reinforcement learning using LLMs. ELLM utilized the natural\nlanguage processing capabilities of LLMs to deﬁne goals and furnish reward functions for\nreinforcement learning agents. This strategy enabled agents to undertake meaningful explo-\nration and learning within their environments. The paper assessed ELLM’s performance in\ntwo settings: Crafter, a 2D version of Minecraft, and Housekeep, involving the task of rear-\nranging household objects. Experimental results demonstrated that ELLM surpassed other\nmethods in both settings. In the Crafter setting, ELLM attained high performance through\ngoal-oriented learning, proving especially effective in scenarios with sparse reward signals.\nIn the Housekeep setting, the agent conducted sensible exploration by adhering to goals\nset by the LLM, achieving a high success rate. While the accuracy of goal setting by the\nLLM varied with the objects and locations, it generally showed high performance. These\nexperimental ﬁndings suggested that ELLM was successful in enhancing reinforcement\nlearning performance across diverse environments, highlighting the vital role of providing\nreward signals based on human commonsense.\n4.2. Low-Level Control\nResearch is also being conducted on generating commands that directly control a\nrobot’s actuators (i.e., enabling low-level control) through various applications of LLM\nmodels. Among these projects, the Google research team developed RT-1 [8], which consists\nof ﬁlm-conditioned EfﬁcientNet-B3, TokenLearner, and Transformer. RT-1 is a model that\nreceives images and natural language instructions at a rate of 3Hz and outputs discretized\nrobot actions. RT-1 was trained on a vast demo dataset with over 130k episodes from more"
  },
  {
    "question": "What is the significance of using minimal training data with LLM-Planner?",
    "chunk": "Figure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable\nrobots to carry out instructions: (a) mobile manipulation and (b,c) tabletop manipulation, in both\nsimulated and real-world environments [153].Appl. Sci.2024, 14, 8868 20 of 39\nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn,\na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot\nbehavior trees (BTs) from textual descriptions. The developed model was compact enough\nto operate on a robot’s onboard microcomputer, while adept at constructing complex robot\nbehaviors. It provided structurally and logically correct BTs and demonstrated the ability\nto handle instructions that were not included in the training set.\nSong [155], as shown in Figure9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions\nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via\na low-level planner. It continuously updated environmental information as new objects\nwere detected during action implementation and revisited the LLM to adjust the plan if\nsubgoals failed or were delayed based on updated observations. This iterative process was\nrepeated until the subgoal was achieved, after which the system moved to the next goal.\nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated\ncompetitive performance with signiﬁcantly reduced training data and proved its ability to\ngeneralize in various tasks (e.g., ALFRED) with minimal examples.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39"
  },
  {
    "question": "How do neural networks enhance language modeling?",
    "chunk": "language models and their variants have been utilized to enhance robot intelligence in\nthe literature.\nAll articles that met the above criteria were included in this review. Following an\nintensive survey of the abstracts of the selected articles, we categorized the research topics\ninto ﬁve groups: reward design for reinforcement learning, low-level control, high-level\nplanning, manipulation, and scene understanding. Figure1 illustrates these ﬁve categories.\nOur categorization was based on a thorough review of the sources in the literature. Subse-\nquently, duplicate articles were removed, and those not meeting the speciﬁed eligibility\ncriteria were excluded. The exclusion criteria included: (1) articles in languages other than\nEnglish and (2) articles discussing general concepts that do not focus on deep reinforcement\nlearning-based manipulation.\n3. Related Works\n3.1. Language Model\nZhao’s LLM review paper [5] categorizes the evolution of Language Models (LMs)\ninto four phases. The initial stage, the statistical language model (SLM) [16–19], utilizes\nmethods based on statistical learning techniques and the Markov assumption to construct\nword prediction models. A notable method from this phase is the n-gram language model,\nwhich predicts words based on a ﬁxed context length of n. Although SLMs have enhanced\nperformance in various domains such as information retrieval (IR) [16,20] and naturalAppl. Sci.2024, 14, 8868 6 of 39\nlanguage processing (NLP) [21–23], higher-order language models have encountered lim-\nitations due to the curse of dimensionality, which necessitates estimating exponentially\nincreasing transition probabilities.\nThe subsequent phase of LMs, termed neural language models (NLMs), leveraged\nneural networks such as multi-layer perceptron (MLP) and recurrent neural networks\n(RNNs) to model the probability of word sequences [24–26]. A key element of this stage\nis the development of word vectors, also known as word embeddings, which form word\nprediction models based on vectors that use a distributed representation of words [24,27].\nWord2vec, a simpliﬁed shallow neural network approach, was introduced to learn these\ndistributed word representations [28,29]. It proved highly effective across various NLP\ntasks by calculating meaningful similarities between word vectors. NLMs progressed\nfrom basic word sequence modeling to sophisticated techniques for representing language\nthrough word2vec.\nFollowing the NLM phase, the ﬁeld advanced to pre-trained language models (PLMs),\nwhich encompass models such as ELMO [30] and BERT [31]. PLMs, utilizing large-scale\ntext data, learn text patterns, structures, and meanings to develop pre-trained context-\nsensitive word representations. They have successfully executed a variety of language\nunderstanding and generation tasks using this acquired knowledge. ELMo [30] introduced\na pre-training method employing bidirectional LSTM (biLSTM) networks for modeling\ndeep contextualized word representations, optimizing performance through speciﬁc ﬁne-\ntuning of the trained biLSTM network for downstream tasks. ELMo is also characterized\nas a bidirectional language model for its dual-directional use of language models.\nAnother PLM model, BERT [31], leverages the transformer architecture [32], exhibiting\nremarkable effectiveness with self-attention mechanisms and parallel processing. BERT,\na pre-trained bidirectional language model, utilizes extensive unlabeled text data. The\nmethod of unsupervised learning-based pre-training in BERT comprises two primary tasks:\nmasked language models and next sentence prediction. PLMs that provide pre-trained\ncontext-aware word representations are profoundly effective in general-purpose semantic"
  },
  {
    "question": "What is the significance of using natural language in robot manipulation tasks?",
    "chunk": "learning objective. Experimental results demonstrated that the model effectively separated\naction and perception, achieving enhanced zero-shot and compositional generalization\nacross various manipulation tasks, speciﬁcally 16 tasks related to robot manipulation.\nHa [164] proposed a framework aimed at robot skill acquisition. This framework\nprovided a comprehensive solution by utilizing language guidance, without necessitating\nexpert demonstrations or reward speciﬁcation/engineering. It consisted of two main\ncomponents. The ﬁrst component, scaling up language-guided data generation, employed\nLLMs to break down tasks into subtasks and generate a hierarchical plan or task tree. This\nplan was materialized into various robot trajectories using 6-DoF exploration primitives.\nThese trajectories were subsequently veriﬁed and retries were performed as needed until\nsuccess was achieved. This approach enhanced the success rate of data collection and\nmore effectively mitigated the low-level understanding gap in LLMs by incorporating retry\nprocesses as part of the robot’s experiences. The second component, distilling down to\nlanguage-conditioned visuomotor policy, transformed robot experiences into a policy that\ndeduced control sequences from visual observations and natural language task descriptions.\nBy extending diffusion policies, this component handled language-based conditioning for\nmulti-task learning. To assess long-horizon behavior, commonsense reasoning, tool use,\nand intuitive physics, a new multi-task benchmark comprising 18 tasks related to robot\nmanipulation across ﬁve domains (mailbox, transport, drawer, catapult, and bus balance)\nwas developed. This benchmark effectively supported the learning of retry behaviors in\nthe data collection process and enhanced success rates.\nHuang [165], as shown in Figure12, aimed to synthesize dense robot trajectories,\nincluding 6-DoF end-effector waypoints, for various manipulation tasks using an open set\nof instructions and objects. Huang noted that LLMs were skilled at deriving affordances and\nconstraints from free-form language instructions. Further, by harnessing code generation\ncapabilities, Huang developed 3D value maps for the agent’s observation space through\ninteractions with VLMs. These 3D value maps were integrated into a model-based planning\nframework to generate closed-loop robot trajectories robust to dynamic perturbations in\na zero-shot approach. The proposed framework demonstrated efﬁcient learning of the\ndynamics model for scenes with contact-rich interactions and provided advantages in these\ncomplex scenarios.Appl. Sci.2024, 14, 8868 24 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 24 of 39 \n \nsuccess was achieved. This approach enhanced  t h e  s u c c e s s  r a t e  o f  d a t a  c o l l e c t i o n  a n d  \nmore e ﬀectively mitigated the low-level understanding gap in LLMs by incorporating \nretry processes as part of the robot’s experiences. The second component, distilling down \nto language-conditioned visuomotor policy, transformed robot experiences into a policy \nthat deduced control sequences from visual observations and natural language task \ndescriptions. By extending di ﬀusion policies, this component handled language-based \nconditioning for multi-task learning. To assess long-horizon behavior, commonsense \nreasoning, tool use, and intuitive physics, a new multi-task benchmark comprising 18 \ntasks related to robot manipulation across ﬁve domains (mailbox, transport, drawer, \ncatapult, and bus balance) was developed. This benchmark e ﬀectively supported the \nlearning of retry behaviors in the data collection process and enhanced success rates."
  },
  {
    "question": "How do language models redefine robot roles in various industries?",
    "chunk": "mand systems. Consequently, robots can respond more adaptably and intelligently in\ninteractions with human users, allowing them to engage in complex problem-solving and\ndecision-making processes beyond simple mechanical tasks.\nAdditionally, LLMs not only enhance a robot’s communication skills to improve HRI\nusability but also boost the robot’s planning abilities. Planning involves setting goals\nand devising a sequence of actions to achieve them, which are essential in determining a\nrobot’s autonomy and efﬁciency. LLMs interpret natural language from users and complex\ncommands, enabling robots to establish and execute suitable plans in various situations.\nMoreover, LLMs adapt ﬂexibly to new situations through a zero-shot approach and utilize\npast data for learning. These capabilities indicate that robots can play a vital role in\nautonomously navigating changing environments and resolving unexpected issues.\nMoreover, VLMs such as CLIP [7], which are trained to solve vision question answering\n(VQA) tasks, have the ability to process visual and linguistic information simultaneously.\nThis ability allows robots to visually perceive their surroundings and integrate this infor-\nmation into linguistic descriptions, enabling more sophisticated situational awareness. For\ninstance, using VLMs, a robot can recognize objects and provide descriptions, as well as\nunderstand and execute user commands based on visual cues. This integrated approach\nsigniﬁcantly enhances a robot’s autonomy and interaction capabilities.\nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which en-\nable low-level actuator control using LLMs and VLMs, Google has introduced AutoRT [10].\nAutoRT is a system where robots interact with real-world objects to collect motion data. ItAppl. Sci.2024, 14, 8868 3 of 39\nbegins by exploring the surrounding space to identify feasible tasks, then uses a VLM to\nunderstand the situation and an LLM to propose possible tasks. By inputting the robot’s\noperational guidelines and safety constraints into the LLM as prompts, AutoRT assesses\nthe validity of the proposed tasks and the necessity for human intervention. Throughout\nthis process, AutoRT safely selects and executes feasible tasks while collecting relevant\ndata.\nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for\nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical causal-\nity in the real world, problem-solving through trial-and-error feedback, and code generation\nabilities. Eureka can autonomously generate reward functions for a variety of tasks and\nrobots without needing speciﬁc templates for each. This allows for the generation of\nhuman-level reward functions for diverse robots and tasks without human input. Further-\nmore, Eureka has demonstrated the ability to solve complex problems that were previously\nunsolved by expert-designed reward functions.\nGiven these research outcomes, integrating language models into robotic intelligence\npresents signiﬁcant potential to enhance robot capabilities and applications dramatically,\nthereby redeﬁning their roles in diverse industries and everyday life. Therefore, this survey\npaper explores recent research trends in LLM- and VLM-based robot intelligence, aiming to\nprovide a comprehensive understanding of future development possibilities by examining\nthe application of language models in various robotic research ﬁelds. It also seeks to\nhighlight research cases, identify current limitations, and suggest future research directions.\nTo chronicle this advancement in robotics research ﬁelds, this review paper presents\nthe following contributions:\n• This paper summarizes and introduces the foundational elements and tuning methods\nof LLM architecture.\n• It explores and arranges prompt techniques to enhance the problem-solving abilities\nof LLMs."
  },
  {
    "question": "What does the ReAct method combine in AI reasoning?",
    "chunk": "prompt techniques are summarized in Table4.\nTable 4.Prompt Techniques.\nName Explanation Ref.\nZero-Shot Prompting Enabling the model to perform new tasks without any examples [ 53]\nFew-Shot Prompting Providing a few examples to enable performing new tasks [ 49]\nChain-of-Thought Explicitly generating intermediate reasoning steps to perform\nstep-by-step inference [41]\nSelf-Consistency\nGenerating various reasoning paths independently through\nFew-Shot CoT, with each path going through a prompt generation\nprocess to select the most consistent answer\n[120]\nGenerated Knowledge Prompting\nIntegrating knowledge and information relevant to a question, and\nthen providing it along with the question to generate accurate\nanswers\n[126]\nPrompt Chaining Dividing a task into sub-tasks and connecting prompts for each\nsub-task as input–output pairs [125]\nTree of Thoughts\nDividing a problem into subproblems with intermediate steps that\nserve as “thoughts” towards solving the problem, where each\nthought undergoes an inference process and self-evaluates its\nprogress towards solving the problem\n[124]\nRetrieval Augmented Generation Combining external information retrieval with natural language\ngeneration [127]\nAutomatic Reasoning and Tool-use Using external tools to automatically generate intermediate\nreasoning steps [128]\nAutomatic Prompt Engineer Automatically generating and selecting commands [ 129]\nActive Prompt Addressing the issue that the effectiveness may be limited by\nhuman annotations [122]\nDirectional Stimulus Prompting Guiding the model to think and generate responses in a speciﬁc\ndirection [130]\nProgram-Aided Language Models Using models to understand natural language problems and\ngenerate programs as intermediate reasoning steps [123]\nReAct Combining reasoning and actions within a mode [ 131]\nReﬂexion Enhancing language-based agents through language feedback [ 132]\nMultimodal CoT A two-stage framework that integrates text and vision modalities [ 121]Appl. Sci.2024, 14, 8868 12 of 39\n4. Language Models for Robotic Intelligence\n4.1. Reward Design in Reinforcement Learning\nResearch in reinforcement learning, closely associated with the ﬁeld of robotics, has\nactively incorporated studies using LLM models. Speciﬁcally, Nvidia has developed a\nGPU-based multi-environment reinforcement learning platform. Utilizing its Omniverse\n3D virtual environment platform, Nvidia created Isaac Sim, which is dedicated to robot\nsimulation. Isaac Sim published research ﬁndings on Isaac Gym (Preview), which achieved\nsigniﬁcant reductions in reinforcement learning training times through GPU-based multi-\nenvironment approaches. Subsequently, Isaac Gym (Preview)’s features were integrated\ninto Isaac Sim and released as Omni Isaac Gym. Later, Nvidia introduced Orbit [133],\nfacilitating the simulation of PhysX 5.1-based cloth, soft-body, ﬂuid, and rigid-body dy-\nnamics, along with RGBD, LiDAR, and contact sensor simulation. Orbit also incorporates\nvarious robot platforms into the simulation environment. Recently, Orbit was updated\nto Isaac Lab and integrated into Isaac Sim 4.0. Nvidia has continuously advanced dy-\nnamic simulation environment technologies for reinforcement learning using GPU parallel\ncomputation. Leveraging this GPU reinforcement learning, they launched Eureka [11],\nwhich automates the design of reward functions for reinforcement learning using LLMs.\nFollowing this, Nvidia introduced DrEureka [134], an automated platform addressing the\nSim2Real problem [135] in reinforcement learning based on Eureka.\nEureka (Evolution-driven Universal REward Kit for Agent) [11], shown in Figure4,\nautomatically generates reward functions for various tasks using different robots, elimi-"
  },
  {
    "question": "What kind of tasks can LLMs perform with the help of Statler?",
    "chunk": "Yoneda [149] introduced Statler, a framew ork designed to provide LLMs with an \nexplicit world state representation through a continuously maintained ‘memory’. The core \nof Statler consisted of two components: the world model reader and the world model \nwriter. These components interacted with and sustained the world state. The world model \nreader interpreted user commands and generated executable code based on the current \nstate representation, while the world model wr iter updated the system’s state according \nto execution outcomes. By facilitating access to the world state ‘memory’, Statler improved \nLLMs’ ability to reason about planning task s with extended time horizons, overcoming \nlimitations imposed by context length. \nMu [150], shown in Figure 7, introduced EmbodiedGPT, a model speci ﬁcally \ndesigned for Embodied AI, which leverages LLMs. This framework processes visual \nobservations and natural language to establish long-term plans and execute tasks in real-\nFigure 6.Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed in\nsequence transformation, completion, and improvement [148].\n4.3. High-Level Planning (Including Decision-Making and Reasoning)\nThe abstraction and generalization capabilities of LLMs offer effective methodologies\nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various\nresearch outcomes have been realized in the ﬁelds of planning, decision-making, reasoning,\nand behavior trees within robotics.\nYoneda [149] introduced Statler, a framework designed to provide LLMs with an\nexplicit world state representation through a continuously maintained ‘memory’. The core\nof Statler consisted of two components: the world model reader and the world model\nwriter. These components interacted with and sustained the world state. The world model\nreader interpreted user commands and generated executable code based on the current\nstate representation, while the world model writer updated the system’s state according to\nexecution outcomes. By facilitating access to the world state ‘memory’, Statler improved\nLLMs’ ability to reason about planning tasks with extended time horizons, overcoming\nlimitations imposed by context length.\nMu [150], shown in Figure7, introduced EmbodiedGPT, a model speciﬁcally designed\nfor Embodied AI, which leverages LLMs. This framework processes visual observations\nand natural language to establish long-term plans and execute tasks in real-time. Em-\nbodiedGPT utilizes pre-trained vision transformers and the LLaMA language model to\nencode visual features and map them to the language modality. The generated plan was\nsubsequently converted into speciﬁc task commands using general visual tokens, encoded\nby the vision model. The framework’s functionality comprises (1) encoding current visual\nfeatures, (2) mapping visual features to the language modality via attention-based interac-\ntions between visual tokens and text queries or learnable embedded queries, (3) generating\nplans with the LLaMA language model and translating them into speciﬁc task commands,\nand (4) querying the encoded visual tokens from the vision model and translating them\ninto low-level control commands through a downstream policy network for task execu-\ntion. Experimental results, utilizing the MS-COCO dataset, revealed that EmbodiedGPT\nexcels in object recognition and understanding spatial relationships. Notably, implement-Appl. Sci.2024, 14, 8868 18 of 39\ning a closed-loop design and a “chain-of-thought” training mode signiﬁcantly enhanced\nEmbodiedGPT’s performance. These results demonstrate that EmbodiedGPT effectively\nhandles various autonomous tasks, exhibiting superior capability in object recognition,"
  },
  {
    "question": "What impact do language models have on the capabilities of robots?",
    "chunk": "• It reviews and encapsulates how LLMs and VLMs have been employed to augment\nrobot intelligence across ﬁve topics as shown in Figure1: (1) reward design for\nreinforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation,\nand (5) scene understanding.\nAppl. Sci. 2024 , 14 , x FOR PEER REVIEW 3 of 39 \n \nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which \nenable low-level actuator control using LLM s and VLMs, Google has introduced AutoRT \n[10]. AutoRT is a system where robots interact  with real-world objects to collect motion \ndata. It begins by exploring th e surrounding space to identify feasible tasks, then uses a \nVLM to understand the situation and an LLM to propose possible tasks. By inpu tting the \nrobot’s operational guidelines and safety co nstraints into the LLM as prompts, AutoRT \nassesses the validity of the proposed tasks and the necessity for human intervention. \nThroughout this process, AutoRT safely selects and executes feasible tasks while collecting \nrelevant data. \nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for \nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical cau-\nsality in the real world, problem-solving through trial-and-error feedback, and code gen-\neration abilities. Eureka can autonomously generate reward functions for a variety of tasks \nand robots without needing speci ﬁc templates for each. This allows for the generation of \nhuman-level reward functions for diverse robo ts and tasks without human input. Further-\nmore, Eureka has demonstrated the ability to solve complex problems that were previ-\nously unsolved by expert-designed reward functions. \nGiven these research outcomes, integrating language models into robotic intelligence \npresents signi ﬁcant potential to enhance robot capabilities and applications dramatically, \nthereby rede ﬁning their roles in diverse industries and everyday life. Therefore, this sur-\nvey paper explores recent research trends in LLM- and VLM-based robot intelligence, \naiming to provide a comprehensive understanding of future development possibilities by \nexamining the application of language models in various robotic research ﬁelds. It also \nseeks to highlight research cases, identify cu rrent limitations, and suggest future research \ndirections. \nTo chronicle this advancement in robotics research ﬁelds, this review paper presents \nthe following contributions: \n• This paper summarizes and introduces the foundational elements and tuning meth-\nods of LLM architecture. \n• It explores and arranges prompt techniques  to enhance the problem-solving abilities \nof LLMs. \n• It  reviews and encapsulates how LLMs and VLMs have been employed to augment \nrobot intelligence across ﬁve topics as shown in Figure 1: (1) reward design for rein-\nforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation, \nand (5) scene understanding. \n \nFigure 1. Five categories for robot intelligence with large language models in this study. \nFigure 1.Five categories for robot intelligence with large language models in this study.Appl. Sci.2024, 14, 8868 4 of 39\nThe reward design in RLcategory represents a research ﬁeld in which an LLM devel-\nops and enhances reward functions employed in reinforcement learning via code-based"
  },
  {
    "question": "What are the advantages of using natural language processing in ELLM?",
    "chunk": "Du [139] introduced theELLM (exploring with LLMs) framework, which provided\nguidelines for pre-training reinforcement learning using LLMs. ELLM utilized the natural\nlanguage processing capabilities of LLMs to deﬁne goals and furnish reward functions for\nreinforcement learning agents. This strategy enabled agents to undertake meaningful explo-\nration and learning within their environments. The paper assessed ELLM’s performance in\ntwo settings: Crafter, a 2D version of Minecraft, and Housekeep, involving the task of rear-\nranging household objects. Experimental results demonstrated that ELLM surpassed other\nmethods in both settings. In the Crafter setting, ELLM attained high performance through\ngoal-oriented learning, proving especially effective in scenarios with sparse reward signals.\nIn the Housekeep setting, the agent conducted sensible exploration by adhering to goals\nset by the LLM, achieving a high success rate. While the accuracy of goal setting by the\nLLM varied with the objects and locations, it generally showed high performance. These\nexperimental ﬁndings suggested that ELLM was successful in enhancing reinforcement\nlearning performance across diverse environments, highlighting the vital role of providing\nreward signals based on human commonsense.\n4.2. Low-Level Control\nResearch is also being conducted on generating commands that directly control a\nrobot’s actuators (i.e., enabling low-level control) through various applications of LLM\nmodels. Among these projects, the Google research team developed RT-1 [8], which consists\nof ﬁlm-conditioned EfﬁcientNet-B3, TokenLearner, and Transformer. RT-1 is a model that\nreceives images and natural language instructions at a rate of 3Hz and outputs discretized\nrobot actions. RT-1 was trained on a vast demo dataset with over 130k episodes from more\nthan 700 tests collected over 17 months using 13 robots.\nA notable feature of RT-1 is its ability to enhance performance by learning from\ndata gathered from heterogeneous robots or simulations. In the study [8], the authors\nevaluated the performance of a model trained exclusively on data from the EveryDay\nRobot (EDR) against a model trained using data from both EDR and Kuka IIWA robots.\nThey recorded a 12% improvement in the bin-picking test. Another experiment compared\nmodel performance using data from real environments and simulations for items not\nencountered in actual settings. The ﬁndings indicated that incorporating simulation data in\nRT-1 training enhances performance over using purely real environment data, suggesting\nthat RT-1 can substantially improve model performance by integrating diverse data from\nrobots of varied morphologies or simulations while sustaining existing task capabilities.Appl. Sci.2024, 14, 8868 15 of 39\nRT-2 [9] is deﬁned as a vision-language-action (VLA) model that facilitates ﬁne-grained\ncontrol of robots through vision and language commands. RT-2 is ﬁne-tuned with robotic\ntrajectory data based on VLM models such as PaLM-E [140], which has 12 billion parameters\nand is trained on VQA data, alongside PaLI-X [141], which has parameter sizes ranging\nfrom 5 billion to 55 billion. The RT-2 system operates as an integrated closed-loop robotic\nsystem that combines low-level and high-level control policies. Despite not explicitly\nlearning certain capabilities during pre-training, RT-2 exhibits improved task performance\nvia real-world generalization involving diverse objects, visual scenes, and instructional\ncontexts. The paper [9] quantitatively assesses RT-2’s emergent capabilities in areas such as\nreasoning, symbol understanding, and human recognition. Furthermore, applying chain-"
  },
  {
    "question": "How do language-based approaches improve robotic manipulation?",
    "chunk": "for speciﬁc tasks through reinforcement learning, even in complex environments. The\nlow-level controlcategory includes a research area in which LLMs and VLMs generate\ncommand sequences that directly control the robot’s actuators through natural language\nand visual input. Thehigh-level planningcategory is a research area where the LLM\nidentiﬁes the present circumstances and objective of the tasks, subsequently developing an\nexplainable plan based on the reasoning required for problem-solving. In this research area,\nthe LLM is also tasked with developing the optimal robot behavior plan, which entails\nevaluating the feasibility of the established plan. In themanipulation category, the LLM in-\nterprets high-level instructions and the VLM (and LLM) analyzes various conditions based\non their understanding of the surroundings to assist robot arms in performing the speciﬁc\ntasks. While this category can be broadly included in the high-level planning category,\nthere are numerous studies that are speciﬁcally related to manipulation with a robot arm,\nwhich is why the manipulation category was separated. Thescene understandingcategory\nrepresents a research area that seeks to combine LLMs and VLMs with the objective of\nassisting robots in comprehending their surrounding environment. This is accomplished\nby identifying objects based on natural language instructions and visual information, as\nwell as by evaluating the relationships between them. This research area is also closely\nrelated to the ﬁeld of autonomous visual navigation. From a boarder perspective, there is an\noverlap between the scene understanding category and the perception-related components\nof the high-level planning category. However, in this review, the scene understanding\ncategory was considered a distinct category due to its prevalence as an application of\nVLM models.\nTable 1 lists resources that aid in understanding robot intelligence based on language\nmodels. The review [5] examined recent advancements in LLMs with a particular emphasis\non four key areas: pre-training, adaptation tuning, utilization, and capacity evaluation. Fur-\nthermore, it provided a summary of the resources currently available for the development\nof LLMs and discussed potential future directions for research in this ﬁeld. The survey [12]\nconducted a comprehensive and systematic review of VLMs for visual recognition tasks.\nIt addressed the evolution of the visual recognition paradigm, the principal architectures\nand datasets, and the fundamental principles of VLMs. Moreover, the paper provided\nan overview of the pre-training, transfer learning, and knowledge distillation methods\nemployed in the context of VLMs. The review [3] examined the potential for leveraging\nexisting natural language processing and computer vision foundation models in robotics.\nIn addition, it explored the possibility of developing a robot-speciﬁc foundation model. The\nreview [13] presented an analysis of recent studies on language-based approaches to robotic\nmanipulation. It comprised an analysis of learning paradigms integrated with foundation\nmodels related to manipulation tasks, including semantic information extraction, environ-\nment and evaluation, auxiliary tasks, task representation, safety issues, and other pertinent\nconsiderations. The survey paper [14] presented an analysis of recent research articles that\nemployed foundation models to address robotics challenges. It investigated the extent to\nwhich foundation models enhanced robot performance in perception, decision-making, and\ncontrol. In addition, it examined the obstacles impeding the implementation of foundation\nmodels in robot autonomy and proposed avenues for future advancements. The review\npaper [15] presented a comprehensive review of the research in the ﬁeld of vision and\nlanguage navigation (VLN), encompassing tasks, evaluation metrics, and methodologies\nrelated to autonomous navigation.Appl. Sci.2024, 14, 8868 5 of 39\nTable 1.Useful Review Papers.\nTitle Keywords Ref."
  },
  {
    "question": "Can you explain how different robotic morphologies affect training outcomes?",
    "chunk": "than 700 tests collected over 17 months using 13 robots.\nA notable feature of RT-1 is its ability to enhance performance by learning from\ndata gathered from heterogeneous robots or simulations. In the study [8], the authors\nevaluated the performance of a model trained exclusively on data from the EveryDay\nRobot (EDR) against a model trained using data from both EDR and Kuka IIWA robots.\nThey recorded a 12% improvement in the bin-picking test. Another experiment compared\nmodel performance using data from real environments and simulations for items not\nencountered in actual settings. The ﬁndings indicated that incorporating simulation data in\nRT-1 training enhances performance over using purely real environment data, suggesting\nthat RT-1 can substantially improve model performance by integrating diverse data from\nrobots of varied morphologies or simulations while sustaining existing task capabilities.Appl. Sci.2024, 14, 8868 15 of 39\nRT-2 [9] is deﬁned as a vision-language-action (VLA) model that facilitates ﬁne-grained\ncontrol of robots through vision and language commands. RT-2 is ﬁne-tuned with robotic\ntrajectory data based on VLM models such as PaLM-E [140], which has 12 billion parameters\nand is trained on VQA data, alongside PaLI-X [141], which has parameter sizes ranging\nfrom 5 billion to 55 billion. The RT-2 system operates as an integrated closed-loop robotic\nsystem that combines low-level and high-level control policies. Despite not explicitly\nlearning certain capabilities during pre-training, RT-2 exhibits improved task performance\nvia real-world generalization involving diverse objects, visual scenes, and instructional\ncontexts. The paper [9] quantitatively assesses RT-2’s emergent capabilities in areas such as\nreasoning, symbol understanding, and human recognition. Furthermore, applying chain-\nof-thought prompting techniques to RT-2 has proven effective in solving more complex\nsemantic inference tasks, such as using a rock as an improvised hammer or offering an\nenergy drink instead of a carbonated beverage to a thirsty person. In comparison with\nthe earlier study on RT-1, RT-2 demonstrates enhanced performance in both familiar and\nnovel tasks.\nAutoRT [10] is a follow-up study based on the research results of RT-1 and RT-2,\nestablishing an orchestration of large-scale robotic agents for data collection in real-world\nscenarios. AutoRT employed 53 robots to gather 77,000 real robot episodes over seven\nmonths through both teleoperation and autonomous robot policies. At the heart of AutoRT\nis a robust foundation model that generates ‘task proposals’ based on given visual observa-\ntions. Notably, AutoRT introduces a ‘Robot Constitution’ using constitutional prompting to\nensure actions during the task proposal process do not compromise the safety of the robot\nor nearby individuals. This Robot Constitution, inspired by Asimov’s three laws [142],\ncomprises basic rules, safety rules that identify unsafe or unwanted tasks, and embodiment\nrules that clarify the robot’s operational boundaries.\nAutoRT enhances data collection by initially scanning the surroundings to identify\ninteresting scenes or tasks (exploration). It interprets the given context through a VLM\nand proposes potential tasks via an LLM (task generation). Subsequently, tasks suggested\nby the LLM are screened (affordance) to assess their feasibility and the need for human\nintervention, employing the Robot Constitution. During this procedure, viable tasks are\nchosen and performed, while pertinent data are gathered (data collection). The collected\ndata are then assessed for (diversity scoring) the visual diversity of the robot trajectories\nand the linguistic diversity of the language instructions generated by AutoRT (LLM). The"
  },
  {
    "question": "What are visual tokens and how are they used in task execution?",
    "chunk": "Appl. Sci. 2024, 14, x FOR PEER REVIEW 18 of 39 \n \ntime. EmbodiedGPT utilizes pre-trained visi on transformers and the LLaMA language \nmodel to encode visual features and map them  to the language modality. The generated \nplan was subsequently converted into speci ﬁc task commands using general visual \ntokens, encoded by the vision model. The framework’s functionality comprises (1) \nencoding current visual features, (2) mapping visual features to the language modality \nvia a ttention-based interactions between visual tokens and text queries or learnable \nembedded queries, (3) generating plans with the LLaMA language model and translating \nthem into speciﬁc task commands, and (4) querying the encoded visual tokens from the \nvision model and translating them into low-level control commands through a \ndownstream policy network for task execution. Experimental results, utilizing the MS-\nCOCO dataset, revealed that EmbodiedG PT excels in object recognition and \nunderstanding spatial relationships. Notabl y, implementing a closed-loop design and a \n“chain-of-thought” training mode signi ﬁcantly enhanced EmbodiedGPT’s performance. \nThese results demonstrate that EmbodiedGPT e ﬀectively handles various autonomous \ntasks, exhibiting superior capability in object recognition, understanding spatial \nrelationships, and generating logical, executable plans. \n \nFigure 7. After encoding visual features, they are mappe d using visual tokens and text queries. A \nplan is then created with the LLaMA model and turned into task commands. The visual tokens are \nqueried and converted into low-level control commands to perform the task [150]. \nChen [151] introduced the language-model-based commonsense reasoning (LMCR) \nframework to assist robots in comprehendin g incomplete natural language instructions. \nThis framework enabled robots to receive instructions in natural language from humans, \nobserve their surroundings, and employ a commonsense reasoning method to \nautonomously infer missing information. LMCR utilized a model of commonsense \nreasoning learned from web-based text materials, allowing robots to understand \nincomplete instructions and autonomously execute tasks. The framework comprised three \nmain functions: language understanding, commonsense reasoning, and action planning. \nIn language understanding, LMCR translated human natural language instructions into a \nform interpretable by robots, parsing them into verb frames to convert them into \nexecutable structures. During the common sense reasoning phase, the robot analyzed \nsurrounding objects and employed a language model trained on large-scale unstructured \ntext materials to ﬁll in the missing details from the instructions. This model identiﬁed the \nFigure 7.After encoding visual features, they are mapped using visual tokens and text queries. A\nplan is then created with the LLaMA model and turned into task commands. The visual tokens are\nqueried and converted into low-level control commands to perform the task [150].\nChen [151] introduced the language-model-based commonsense reasoning (LMCR)\nframework to assist robots in comprehending incomplete natural language instructions.\nThis framework enabled robots to receive instructions in natural language from humans, ob-\nserve their surroundings, and employ a commonsense reasoning method to autonomously\ninfer missing information. LMCR utilized a model of commonsense reasoning learned\nfrom web-based text materials, allowing robots to understand incomplete instructions and\nautonomously execute tasks. The framework comprised three main functions: language\nunderstanding, commonsense reasoning, and action planning. In language understanding,\nLMCR translated human natural language instructions into a form interpretable by robots,"
  },
  {
    "question": "What does scene understanding entail within the context of this research?",
    "chunk": "Toward General-Purpose Robots via Foundation\nModels: A Survey and Meta-Analysis Foundation Models [ 3]\nA Survey of Large Language Models LLM [ 5]\nVision-Language Models for Vision Tasks: A Survey VLM [ 12]\nLanguage-conditioned Learning for Robotic\nManipulation: A Survey LLM, VLM, Manipulation [ 13]\nFoundation Models in Robotics: Applications,\nChallenges, and the Future Foundation Models [ 14]\nVision-and-Language Navigation: A Survey of Tasks,\nMethods, and Future Directions VLN [ 15]\n2. Review Protocol\nThis survey covered four databases: Web of Science, ScienceDirect, IEEE Xplore, and\narXiv. In fact, many of the articles surveyed had not been peer-reviewed and published at\nthe time of our search because the subject matter was relatively recent. Therefore, a consid-\nerable number of articles reviewed in this survey were sourced from the arXiv database.\nThe selection process of this study primarily relied on two iterations:\n• The titles and abstracts of the articles were reviewed to eliminate duplicates and\nirrelevant articles.\n• The full texts of the selected articles from the ﬁrst iteration were thoroughly examined\nand categorized.\n• Article searching began on 18 September 2023.\nRegarding the search queries,\n• the publication years were those after 2020,\n• the keywords of Robotics and LLM, which were ((“Robotic” OR “Robotics”) AND\n(“LLM” OR “LM” OR “Large Language Model” OR “Language Model”)), and relevant\njournal and conference articles written in English were considered.\nFrom these search criteria, recent studies utilizing language models in robotics research\nwere expected to be collected. Our aim is to provide a robust understanding of how\nlanguage models and their variants have been utilized to enhance robot intelligence in\nthe literature.\nAll articles that met the above criteria were included in this review. Following an\nintensive survey of the abstracts of the selected articles, we categorized the research topics\ninto ﬁve groups: reward design for reinforcement learning, low-level control, high-level\nplanning, manipulation, and scene understanding. Figure1 illustrates these ﬁve categories.\nOur categorization was based on a thorough review of the sources in the literature. Subse-\nquently, duplicate articles were removed, and those not meeting the speciﬁed eligibility\ncriteria were excluded. The exclusion criteria included: (1) articles in languages other than\nEnglish and (2) articles discussing general concepts that do not focus on deep reinforcement\nlearning-based manipulation.\n3. Related Works\n3.1. Language Model\nZhao’s LLM review paper [5] categorizes the evolution of Language Models (LMs)\ninto four phases. The initial stage, the statistical language model (SLM) [16–19], utilizes\nmethods based on statistical learning techniques and the Markov assumption to construct\nword prediction models. A notable method from this phase is the n-gram language model,\nwhich predicts words based on a ﬁxed context length of n. Although SLMs have enhanced\nperformance in various domains such as information retrieval (IR) [16,20] and naturalAppl. Sci.2024, 14, 8868 6 of 39\nlanguage processing (NLP) [21–23], higher-order language models have encountered lim-\nitations due to the curse of dimensionality, which necessitates estimating exponentially\nincreasing transition probabilities.\nThe subsequent phase of LMs, termed neural language models (NLMs), leveraged\nneural networks such as multi-layer perceptron (MLP) and recurrent neural networks"
  },
  {
    "question": "How can LLMs facilitate the design of reward functions in complex environments?",
    "chunk": "Figure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses expert-\ndesigned functions through iterative improvements [11]. \nFollowing Eureka, DrEureka [134], shown in Figure 5, was developed to address the \nsim-to-real problem by automatically con ﬁguring appropriate reward functions and do-\nmain randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses\nexpert-designed functions through iterative improvements [11].\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 13 of 39 \n \n \nFigure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses expert-\ndesigned functions through iterative improvements [11]. \nFollowing Eureka, DrEureka [134], shown in Figure 5, was developed to address the \nsim-to-real problem by automatically con ﬁguring appropriate reward functions and do-\nmain randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 5.DrEureka leverages LLM to design reward functions and solves the sim-to-real problem\nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134].\nXie [136] introducedText2Reward, a framework that automatically generated dense\nreward functions for reinforcement learning using LLMs. Provided with a goal expressed\nin natural language, Text2Reward produced executable dense reward functions derived\nfrom a compact representation of the environment. This framework generated free-form\ndense reward codes and delivered performance comparable to or surpassing that of policies\ntrained with expert-designed codes across a variety of tasks, including 17 manipulator-\nrelated tasks and six novel locomotion behaviors. Additionally, Text2Reward incorporated\nuser feedback to iteratively enhance the generated reward functions, thereby increasing the\nsuccess rate of the learned policies.\nDi Palo [137] explored the use of LLMs and VLMs to improve reinforcement learning"
  },
  {
    "question": "What improvements were seen in task success rates with the new robot interfaces?",
    "chunk": "of-thought prompting techniques to RT-2 has proven effective in solving more complex\nsemantic inference tasks, such as using a rock as an improvised hammer or offering an\nenergy drink instead of a carbonated beverage to a thirsty person. In comparison with\nthe earlier study on RT-1, RT-2 demonstrates enhanced performance in both familiar and\nnovel tasks.\nAutoRT [10] is a follow-up study based on the research results of RT-1 and RT-2,\nestablishing an orchestration of large-scale robotic agents for data collection in real-world\nscenarios. AutoRT employed 53 robots to gather 77,000 real robot episodes over seven\nmonths through both teleoperation and autonomous robot policies. At the heart of AutoRT\nis a robust foundation model that generates ‘task proposals’ based on given visual observa-\ntions. Notably, AutoRT introduces a ‘Robot Constitution’ using constitutional prompting to\nensure actions during the task proposal process do not compromise the safety of the robot\nor nearby individuals. This Robot Constitution, inspired by Asimov’s three laws [142],\ncomprises basic rules, safety rules that identify unsafe or unwanted tasks, and embodiment\nrules that clarify the robot’s operational boundaries.\nAutoRT enhances data collection by initially scanning the surroundings to identify\ninteresting scenes or tasks (exploration). It interprets the given context through a VLM\nand proposes potential tasks via an LLM (task generation). Subsequently, tasks suggested\nby the LLM are screened (affordance) to assess their feasibility and the need for human\nintervention, employing the Robot Constitution. During this procedure, viable tasks are\nchosen and performed, while pertinent data are gathered (data collection). The collected\ndata are then assessed for (diversity scoring) the visual diversity of the robot trajectories\nand the linguistic diversity of the language instructions generated by AutoRT (LLM). The\naim of this diversity evaluation is to conﬁrm that, unlike simulations, real-world data\ncollection by robots is labor-intensive, making it essential to gather data across a broad\nspectrum of tasks. Experimental outcomes illustrate that AutoRT achieves higher visual\nand linguistic diversity compared to RT-1 or BC-Z [143].\nOther researchers include Tang [144], who developed an approach that connects\nnatural language user commands with a locomotion controller using foot contact patterns as\nan interface for low-level commands. This innovative interface translates human commands\ninto the robot’s foot contact patterns, allowing the robot to move at a speciﬁed speed with\nprecise timing for each foot’s contact with the ground. To achieve this, the robot used a\ncyclic sliding window to extract foot contact ﬂags from a pattern template, thus generating\nthe required foot contact patterns. During training, a random pattern generator created\nfoot contact patterns, and during testing, an LLM translated human commands into these\npatterns. The robot then adjusted its movements based on the foot contact patterns it\nlearned through deep reinforcement learning, closely adhering to the intended foot contact\npatterns and speed commands. This approach demonstrated a 50% higher success rate in\ntask evaluation (across 30 tasks, including standing still) compared to two baselines (which\nemployed discrete gaits and sinusoidal functions as interfaces), successfully solving 10\nmore tasks than the baselines.\nMandi [145] introduced a novel method for multi-robot collaboration that utilizes\nLLMs for both high-level communication and low-level path planning. In this method, the\nrobots employ the LLM to discuss and reason about task strategies. They generate sub-taskAppl. Sci.2024, 14, 8868 16 of 39\nplans and task space waypoint paths, which a multi-arm motion planner then uses to expe-"
  },
  {
    "question": "What are the computational requirements for tuning large language models?",
    "chunk": "quence and sequentially predicts output tokens individually. Examples of preﬁx decoder-\nbased LLMs include GLM-130B [108] and U-PaLM [109]. Additionally, various architec-\ntures have been proposed to address e ﬃciency challenges during training or inference \nwith long inputs, due to the quadratic computational complexity of the traditional trans-\nformer architecture. For instance, the Mixture-of-Experts (MoE) scaling method [34] \nsparsely activates a subset of the neural network for each input. \n \nFigure 2. Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx De-\ncoder (middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectan-\ngles represent attention between preﬁx tokens, attention between preﬁx and target tokens, atten-\ntion between target tokens, and masked attention [5]. \nIn terms of the tuning of LLMs, these models are essentially pre-trained on massive \ndatasets and require ﬁne-tuning for diﬀerent application domains. However, the consid-\nerable model size and number of parameters pose challenges for ﬁne-tuning on standard \ncomputers and GPUs. The subsequent sections will discuss methods to address these chal-\nlenges. \nLLM tuning is broadly divided into two cate gories based on the training objective. \nInstruction tuning is a form of supervised learning where the training data typically in-\nclude descriptions of tasks, inputs, and corresponding outputs. This type of tuning is de-\nsigned (1) to enhance the functional capabilities of LLMs, (2) to specialize them by training \nwith discipline-speci ﬁc information, and (3) to improve task generalization and con-\nsistency through a be tter understanding of natural language commands. Conversely, \nalignment tuning (or preference alignment) seeks to align the behavior of LLMs with hu-\nman values and preferences. Prominent methods include reinforcement learning from hu-\nman feedback (RLHF) [110], which involves ﬁne-tuning LLMs using human feedback to \nbetter reﬂect human values, and direct preference optimization (DPO) [111], focusing on \ntraining with pairs of human preferences th at usually include an input prompt and the \npreferred and non-preferred responses. \nFor both instruction tuning and alignment tuning, which involve training LLMs with \nextensively large model para meters, substantial GPU memo ry and computational re-\nsources are required, with high costs typi cally incurred when utilizing cloud-based \nFigure 2.Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx Decoder\n(middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectangles\nrepresent attention between preﬁx tokens, attention between preﬁx and target tokens, attention\nbetween target tokens, and masked attention [5].\nIn terms of the tuning of LLMs, these models are essentially pre-trained on mas-\nsive datasets and require ﬁne-tuning for different application domains. However, the\nconsiderable model size and number of parameters pose challenges for ﬁne-tuning on\nstandard computers and GPUs. The subsequent sections will discuss methods to address\nthese challenges.\nLLM tuning is broadly divided into two categories based on the training objective.\nInstruction tuning is a form of supervised learning where the training data typically include\ndescriptions of tasks, inputs, and corresponding outputs. This type of tuning is designed\n(1) to enhance the functional capabilities of LLMs, (2) to specialize them by training with"
  },
  {
    "question": "What are the emergent abilities of large language models?",
    "chunk": "(RNNs) to model the probability of word sequences [24–26]. A key element of this stage\nis the development of word vectors, also known as word embeddings, which form word\nprediction models based on vectors that use a distributed representation of words [24,27].\nWord2vec, a simpliﬁed shallow neural network approach, was introduced to learn these\ndistributed word representations [28,29]. It proved highly effective across various NLP\ntasks by calculating meaningful similarities between word vectors. NLMs progressed\nfrom basic word sequence modeling to sophisticated techniques for representing language\nthrough word2vec.\nFollowing the NLM phase, the ﬁeld advanced to pre-trained language models (PLMs),\nwhich encompass models such as ELMO [30] and BERT [31]. PLMs, utilizing large-scale\ntext data, learn text patterns, structures, and meanings to develop pre-trained context-\nsensitive word representations. They have successfully executed a variety of language\nunderstanding and generation tasks using this acquired knowledge. ELMo [30] introduced\na pre-training method employing bidirectional LSTM (biLSTM) networks for modeling\ndeep contextualized word representations, optimizing performance through speciﬁc ﬁne-\ntuning of the trained biLSTM network for downstream tasks. ELMo is also characterized\nas a bidirectional language model for its dual-directional use of language models.\nAnother PLM model, BERT [31], leverages the transformer architecture [32], exhibiting\nremarkable effectiveness with self-attention mechanisms and parallel processing. BERT,\na pre-trained bidirectional language model, utilizes extensive unlabeled text data. The\nmethod of unsupervised learning-based pre-training in BERT comprises two primary tasks:\nmasked language models and next sentence prediction. PLMs that provide pre-trained\ncontext-aware word representations are profoundly effective in general-purpose semantic\nfeature extraction, facilitating enhancements in NLP task performance. Owing to these\ncharacteristics, numerous subsequent studies employing pre-training and ﬁne-tuning have\nbeen introduced, featuring varied structures [33,34] (e.g., BART [33] and GPT-2 [35]) and\nenhancing pre-training strategies [36–38].\nBased on subsequent studies, it has been found that increasing the model size or data\nsize of PLMs typically enhances the performance of LM models [39]. This has prompted\nresearch into training large-scale PLMs, such as GPT-3 with 175B parameters and PaLM\nwith 540B parameters. The focus of this research, grounded in scaling laws, primarily\ncenters on augmenting model sizes and exploring the capabilities of larger models. These\ncapabilities, known as the emergent abilities of LLMs, have sparked signiﬁcant interest. For\nexample, GPT-3 can address problems it has not been trained on with minimal examples\nthrough in-context learning, a feat GPT-2 ﬁnds challenging. Due to these characteristics,\nthe academic community commonly designates these large PLMs as LLMs [40–43]. Conse-\nquently, research in this area is highly active. Notably, since the introduction of OpenAI’s\nChatGPT, there has been a surge in the number of arXiv papers on LLMs. Following\nMicrosoft’s announcement [2] about integrating ChatGPT into robotics, a variety of studies\nhave explored the application of LLMs across different areas of robotics research. The\navailable LLM models are presented in chronological order in Table2. Additionally, Table3\nincludes the VLM models.Appl. Sci.2024, 14, 8868 7 of 39\nTable 2.Chronicle of LLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref."
  },
  {
    "question": "How do LLMs and VLMs work together in enhancing robotics?",
    "chunk": "• It reviews and encapsulates how LLMs and VLMs have been employed to augment\nrobot intelligence across ﬁve topics as shown in Figure1: (1) reward design for\nreinforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation,\nand (5) scene understanding.\nAppl. Sci. 2024 , 14 , x FOR PEER REVIEW 3 of 39 \n \nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which \nenable low-level actuator control using LLM s and VLMs, Google has introduced AutoRT \n[10]. AutoRT is a system where robots interact  with real-world objects to collect motion \ndata. It begins by exploring th e surrounding space to identify feasible tasks, then uses a \nVLM to understand the situation and an LLM to propose possible tasks. By inpu tting the \nrobot’s operational guidelines and safety co nstraints into the LLM as prompts, AutoRT \nassesses the validity of the proposed tasks and the necessity for human intervention. \nThroughout this process, AutoRT safely selects and executes feasible tasks while collecting \nrelevant data. \nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for \nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical cau-\nsality in the real world, problem-solving through trial-and-error feedback, and code gen-\neration abilities. Eureka can autonomously generate reward functions for a variety of tasks \nand robots without needing speci ﬁc templates for each. This allows for the generation of \nhuman-level reward functions for diverse robo ts and tasks without human input. Further-\nmore, Eureka has demonstrated the ability to solve complex problems that were previ-\nously unsolved by expert-designed reward functions. \nGiven these research outcomes, integrating language models into robotic intelligence \npresents signi ﬁcant potential to enhance robot capabilities and applications dramatically, \nthereby rede ﬁning their roles in diverse industries and everyday life. Therefore, this sur-\nvey paper explores recent research trends in LLM- and VLM-based robot intelligence, \naiming to provide a comprehensive understanding of future development possibilities by \nexamining the application of language models in various robotic research ﬁelds. It also \nseeks to highlight research cases, identify cu rrent limitations, and suggest future research \ndirections. \nTo chronicle this advancement in robotics research ﬁelds, this review paper presents \nthe following contributions: \n• This paper summarizes and introduces the foundational elements and tuning meth-\nods of LLM architecture. \n• It explores and arranges prompt techniques  to enhance the problem-solving abilities \nof LLMs. \n• It  reviews and encapsulates how LLMs and VLMs have been employed to augment \nrobot intelligence across ﬁve topics as shown in Figure 1: (1) reward design for rein-\nforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation, \nand (5) scene understanding. \n \nFigure 1. Five categories for robot intelligence with large language models in this study. \nFigure 1.Five categories for robot intelligence with large language models in this study.Appl. Sci.2024, 14, 8868 4 of 39\nThe reward design in RLcategory represents a research ﬁeld in which an LLM devel-\nops and enhances reward functions employed in reinforcement learning via code-based"
  },
  {
    "question": "In what ways does LLM-Planner demonstrate generalization across tasks?",
    "chunk": "robots to carry out instructions: (a) mobile manipulation and (b,c) tabletop manipulation, in both\nsimulated and real-world environments [153].Appl. Sci.2024, 14, 8868 20 of 39\nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn,\na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot\nbehavior trees (BTs) from textual descriptions. The developed model was compact enough\nto operate on a robot’s onboard microcomputer, while adept at constructing complex robot\nbehaviors. It provided structurally and logically correct BTs and demonstrated the ability\nto handle instructions that were not included in the training set.\nSong [155], as shown in Figure9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions\nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via\na low-level planner. It continuously updated environmental information as new objects\nwere detected during action implementation and revisited the LLM to adjust the plan if\nsubgoals failed or were delayed based on updated observations. This iterative process was\nrepeated until the subgoal was achieved, after which the system moved to the next goal.\nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated\ncompetitive performance with signiﬁcantly reduced training data and proved its ability to\ngeneralize in various tasks (e.g., ALFRED) with minimal examples.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 9.LLM-Planner is a system that creates high-level plans based on natural language commands,\nsets subgoals to determine actions, and continuously updates the plan to reﬂect environmental\nchanges [155].\nSingh [156], as shown in Figure10, introduced ProgPrompt, a programmatic LLM\nprompt structure designed for generating plans across diverse situated environments,\nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that\nleveraged LLMs and included a Python programming structure to facilitate information\nabout the environment and executable actions. It featured a feedback mechanism, using\nexecutable program plan examples and assertion statements to mitigate errors, enhancing"
  },
  {
    "question": "What techniques do LLMs use for high-level planning in robotics?",
    "chunk": "reward-labeled trajectories as context and incorporating online interaction, LLM-based\nagents could explore small grids and reﬁne simple trajectories using human-in-the-loop\nmethods, such as optimizing a CartPole controller.Appl. Sci.2024, 14, 8868 17 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 17 of 39 \n \nsequence transformation, the research demonstrated that LLMs could generalize speci ﬁc \nsequence transformations using benchmarks such as ARC (abstraction and reasoning \ncorpus) and PCFG (probabilistic context-free grammar), thereby proving their utility in \nspatial reasoning tasks for robotics. In sequence completion, the study examined whether \nLLMs could ﬁnish pa tterns in elementary functions (e.g., sinusoids), illustrating their \nutility in robotic tasks such as extending a wiping motion from kinesthetic demonstrations \nor creating drawings on a whiteboard. Finally, in sequence improvement, the research \nrevealed that by utilizing reward-labeled trajectories as context and incorporating online \ninteraction, LLM-based agents could explore small grids and re ﬁne simple trajectories \nusing human-in-the-loop methods, such as optimizing a CartPole controller.  \n \nFigure 6. Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed \nin sequence transformation, completion, and improvement [148]. \n4.3. High-Level Planning (Including Decision-Making and Reasoning) \nThe abstraction and generalization capabilities of LLMs oﬀer eﬀective methodologies \nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various \nresearch outcomes have been realized in the ﬁelds of planning, decision-making, \nreasoning, and behavior trees within robotics. \nYoneda [149] introduced Statler, a framew ork designed to provide LLMs with an \nexplicit world state representation through a continuously maintained ‘memory’. The core \nof Statler consisted of two components: the world model reader and the world model \nwriter. These components interacted with and sustained the world state. The world model \nreader interpreted user commands and generated executable code based on the current \nstate representation, while the world model wr iter updated the system’s state according \nto execution outcomes. By facilitating access to the world state ‘memory’, Statler improved \nLLMs’ ability to reason about planning task s with extended time horizons, overcoming \nlimitations imposed by context length. \nMu [150], shown in Figure 7, introduced EmbodiedGPT, a model speci ﬁcally \ndesigned for Embodied AI, which leverages LLMs. This framework processes visual \nobservations and natural language to establish long-term plans and execute tasks in real-\nFigure 6.Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed in\nsequence transformation, completion, and improvement [148].\n4.3. High-Level Planning (Including Decision-Making and Reasoning)\nThe abstraction and generalization capabilities of LLMs offer effective methodologies\nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various\nresearch outcomes have been realized in the ﬁelds of planning, decision-making, reasoning,\nand behavior trees within robotics.\nYoneda [149] introduced Statler, a framework designed to provide LLMs with an\nexplicit world state representation through a continuously maintained ‘memory’. The core\nof Statler consisted of two components: the world model reader and the world model\nwriter. These components interacted with and sustained the world state. The world model"
  },
  {
    "question": "What are the benefits of using a modular approach in robotic tasks?",
    "chunk": "sequential manipulation tasks that require long-horizon reasoning. Text2Motion interpreted\nnatural language instructions to formulate task plans and generated multiple candidate\nskill sequences, evaluating the geometric feasibility of each sequence. By employing a\ngreedy search strategy, it selected the optimal skill sequence to verify and execute the ﬁnal\nplan. This method enabled Text2Motion to perform complex sequential manipulation tasks\nwith a higher success rate compared to existing language-based planning methods, such\nas Saycan-gs and Innermono-gs, and provided semantically generalized characteristics\namong skills with geometric relationships.\nWu [160] investigated personalization in-home cleaning robots that organize and tidy\nspaces, using an LLM to convert user-provided object placement locations into generalized\nrules. By using a camera to identify objects and CLIP to categorize them, TidyBot efﬁciently\nrelocated objects according to these rules. This method attained an impressive accuracy\nof 91.2% for unseen objects in a benchmark dataset, which encompassed a variety of\nobjects, receptacles, and example placements of both “seen” and “unseen” objects across\n96 scenarios. Additionally, it achieved an 85% success rate in removing objects during\nreal-world tests.\n4.4. Manipulation by LLMs\nIn robotics research, the manipulation domain, which includes robotic arms and\nend effectors, encompasses various areas that beneﬁt from foundation models such as\nLLMs for language-based interactions and VLMs for object handling. Among the studies\nintegrating manipulation with foundation models, Stone [161] introduced an approach\ncalled manipulation of open-world objects (MOO). This approach determined whether a\nrobot could follow instructions involving unseen object categories by linking pre-trained\nmodels to robotic policies. MOO utilized pre-trained vision-language models to derive\nobject information from language commands and images, guiding the robot’s actions based\non the current image, command, and identiﬁed object data. Experimental use of real mobile\nmanipulation robots showed that MOO could adapt to new object types and environments\nin a zero-shot fashion. Moreover, MOO responded to non-verbal cues such as pointing at\nspeciﬁc objects, extending its scope to open-world exploration and manipulation.Appl. Sci.2024, 14, 8868 23 of 39\nExisting VLMs often lack a comprehensive understanding of physical concepts such\nas material and fragility, which limits their effectiveness in robotic manipulation tasks. To\naddress this issue, Gao [162] introduced PhysObjects, an object-centric dataset featuring\n39.6K crowd-sourced annotations and 417K automated annotations of physical concepts.\nThe automated annotations involved assigning speciﬁc concept values to predeﬁned object\ncategories or continuous concepts such as material and fragility. Fine-tuning a VLM on\nPhysObjects enhanced comprehension of physical concepts by capturing human biases\nrelated to the visual appearance of objects. Integrating this physically grounded VLM\nwith an LLM-based robotic planner framework improved performance in tasks requiring\nreasoning about physical concepts.\nThe traditional pre-training and ﬁne-tuning pipeline often suffers from decreased\nlearning efﬁciency and challenges in generalizing to unseen objects and tasks due to its\nreliance on domain-speciﬁc action information and domain-general visual information. To\naddress these limitations, Wang [163] proposed a modular approach named ProgramPort,\nwhich utilizes the syntactic and semantic structure of language instructions. Wang’s\nframework incorporated a semantic parser to reconstruct executable programs, composed of\nfunctional modules based on vision and action across multiple modalities. Each functional\nmodule combined deterministic computation with learnable neural networks. Program\nexecution involved generating parameters for general manipulation primitives used by the"
  },
  {
    "question": "What is the significance of instructions in prompt engineering?",
    "chunk": "neering (or prompting) entails supplying inputs  to the model to perform a distinct task, \ndesigning the input format to encapsulate the task’s purpose and context, and generating \nthe desired output. The four components of pr ompt engineering can be analyzed as fol-\nlows: within the prompt, “ Instructions” delineate the speci ﬁc tasks or directives for the \nmodel and “Context” provides external or additional contextual information that can tune \nthe model. Furthermore, “Input data” refers to the type of input or questions seeking an-\nswers, and “ Output data” deﬁnes the output type or format within the prompt, thereby \noptimizing the LLM’s performance for particular tasks. Various methodologies for creat-\ning prompts have been introduced, as described below. \nFigure 3. An overview of four strategies for parameter-efﬁcient ﬁne-tuning: (a) Adapter Tuning,\n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5].\nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a\nlow-rank constraint on transformer layers to approximate the update matrices through\ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates\nthe parameter updates using low-rank decomposition matrices. The primary beneﬁt of\nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning,\nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory\nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning.\nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119].\n3.3. Prompt Techniques for Increasing LLM Performance\nTo enhance the performance of LLMs, the most straightforward approach involves\ntraining with additional data via ﬁne-tuning techniques, which mirrors supervised learning\nin conventional machine learning. Another method for improving performance involves the\nuse of in-context learning, which capitalizes on prompts for zero-shot learning, a capability\nﬁrst observed in LLMs with the advent of GPT-3. The adaptation of these prompts for\nspeciﬁc tasks is known as prompt engineering. Fundamentally, prompt engineering (or\nprompting) entails supplying inputs to the model to perform a distinct task, designing the\ninput format to encapsulate the task’s purpose and context, and generating the desired\noutput. The four components of prompt engineering can be analyzed as follows: within\nthe prompt, “Instructions” delineate the speciﬁc tasks or directives for the model andAppl. Sci.2024, 14, 8868 10 of 39\n“Context” provides external or additional contextual information that can tune the model.\nFurthermore, “Input data” refers to the type of input or questions seeking answers, and\n“Output data” deﬁnes the output type or format within the prompt, thereby optimizing the\nLLM’s performance for particular tasks. Various methodologies for creating prompts have\nbeen introduced, as described below.\nZero-shot prompting[53] is a technique that allows the model to take on new tasks\nwith no prior examples. The model relies solely on the task description or instructions with-\nout additional training. Likewise,few-shot prompting[49] introduces a small number of\nexamples to aid the model in learning new tasks. This approach does not require extensive\ndatasets and can improve the model’s performance through a limited set of examples.\nChain-of-thought (CoT) [41] is a technique that explicitly describes intermediate"
  },
  {
    "question": "What is PhysObjects and what does it aim to improve?",
    "chunk": "on the current image, command, and identiﬁed object data. Experimental use of real mobile\nmanipulation robots showed that MOO could adapt to new object types and environments\nin a zero-shot fashion. Moreover, MOO responded to non-verbal cues such as pointing at\nspeciﬁc objects, extending its scope to open-world exploration and manipulation.Appl. Sci.2024, 14, 8868 23 of 39\nExisting VLMs often lack a comprehensive understanding of physical concepts such\nas material and fragility, which limits their effectiveness in robotic manipulation tasks. To\naddress this issue, Gao [162] introduced PhysObjects, an object-centric dataset featuring\n39.6K crowd-sourced annotations and 417K automated annotations of physical concepts.\nThe automated annotations involved assigning speciﬁc concept values to predeﬁned object\ncategories or continuous concepts such as material and fragility. Fine-tuning a VLM on\nPhysObjects enhanced comprehension of physical concepts by capturing human biases\nrelated to the visual appearance of objects. Integrating this physically grounded VLM\nwith an LLM-based robotic planner framework improved performance in tasks requiring\nreasoning about physical concepts.\nThe traditional pre-training and ﬁne-tuning pipeline often suffers from decreased\nlearning efﬁciency and challenges in generalizing to unseen objects and tasks due to its\nreliance on domain-speciﬁc action information and domain-general visual information. To\naddress these limitations, Wang [163] proposed a modular approach named ProgramPort,\nwhich utilizes the syntactic and semantic structure of language instructions. Wang’s\nframework incorporated a semantic parser to reconstruct executable programs, composed of\nfunctional modules based on vision and action across multiple modalities. Each functional\nmodule combined deterministic computation with learnable neural networks. Program\nexecution involved generating parameters for general manipulation primitives used by the\nrobot’s end effector. The entire module network was trainable with an end-to-end imitation\nlearning objective. Experimental results demonstrated that the model effectively separated\naction and perception, achieving enhanced zero-shot and compositional generalization\nacross various manipulation tasks, speciﬁcally 16 tasks related to robot manipulation.\nHa [164] proposed a framework aimed at robot skill acquisition. This framework\nprovided a comprehensive solution by utilizing language guidance, without necessitating\nexpert demonstrations or reward speciﬁcation/engineering. It consisted of two main\ncomponents. The ﬁrst component, scaling up language-guided data generation, employed\nLLMs to break down tasks into subtasks and generate a hierarchical plan or task tree. This\nplan was materialized into various robot trajectories using 6-DoF exploration primitives.\nThese trajectories were subsequently veriﬁed and retries were performed as needed until\nsuccess was achieved. This approach enhanced the success rate of data collection and\nmore effectively mitigated the low-level understanding gap in LLMs by incorporating retry\nprocesses as part of the robot’s experiences. The second component, distilling down to\nlanguage-conditioned visuomotor policy, transformed robot experiences into a policy that\ndeduced control sequences from visual observations and natural language task descriptions.\nBy extending diffusion policies, this component handled language-based conditioning for\nmulti-task learning. To assess long-horizon behavior, commonsense reasoning, tool use,\nand intuitive physics, a new multi-task benchmark comprising 18 tasks related to robot\nmanipulation across ﬁve domains (mailbox, transport, drawer, catapult, and bus balance)\nwas developed. This benchmark effectively supported the learning of retry behaviors in\nthe data collection process and enhanced success rates.\nHuang [165], as shown in Figure12, aimed to synthesize dense robot trajectories,\nincluding 6-DoF end-effector waypoints, for various manipulation tasks using an open set"
  },
  {
    "question": "What is the role of environmental feedback in robot trajectory planning?",
    "chunk": "dite trajectory planning. Additionally, environmental feedback, such as collision detection,\nprompts the LLM agent to reﬁne plans and waypoints contextually. This method achieved\na high success rate across all tasks in the RoCoBench (including duties such as sweeping\nthe ﬂoor), effectively adapting to variations in task semantics. In real-world experiments,\nspeciﬁcally the block-sorting task, RoCo demonstrated its ability to communicate and\ncollaborate with other robot agents to successfully complete the tasks.\nWang [146] proposed a novel paradigm for utilizing few-shot prompts in physical\nenvironments. This method involved gathering observation and action pairs from existing\nmodel-based or learning-based controllers to form the initial text prompts. Data included\nsensor readings, such as IMU and joint encoders, coupled with target joint positions.\nThese data formed the starting input for LLM inference. As the robot interacted with its\nenvironment and collected new observational data, these initial data were updated with\noutputs from the LLM. In the subsequent prompt engineering phase, observation and action\npairs, along with explanatory prompts, were crafted to enable the LLM to function as a\nfeedback policy. The explanatory prompts provided clear descriptions of the robot walking\ntask and control design details, while the observation and action prompts delineated the\nformat and signiﬁcance of each observation and action. This method allowed the LLM\nto directly output low-level target joint positions for robot walking. The approach was\ntested using the ANYmal robot in MuJoCo and Isaac Gym simulators for robot walking,\nindicating that the LLM could act as a low-level feedback controller for dynamic motion\ncontrol within sophisticated robot systems.\nLiang [147] introduced a new framework namedCode as Policies(CaP) that directly\nconstructs robot policies from executable code generated by a code LLM. This framework\nenabled the interpretation and execution of natural language instructions through an LLM,\nsupporting the creation of high-level policies for robots and accommodating a variety of\nrobotic tasks. Speciﬁcally, CaP interpreted natural language instructions through descrip-\ntions and formulated an action plan for the robot. Moreover, it utilized VLMs such as ViLD\nand MDETR to identify objects and ascertain their locations. Based on this information,\nthe framework controlled the robot’s movements to carry out speciﬁed tasks. The paper\ndemonstrated the CaP framework across diverse domains, including whiteboard drawing,\ntabletop manipulation, and mobile robot navigation and manipulation. Experimental\nresults showed that CaP achieved similar or better success rates than existing systems\nsuch as CLIPort, displaying notably strong generalization capabilities for new tasks. These\nﬁndings underscored the ﬂexibility and efﬁcacy of the CaP framework, establishing its\neffectiveness across various robotic systems.\nMirchandani [148], shown in Figure6, suggested that pre-trained LLMs could autore-\ngressively complete complex token sequences and function as general sequence modelers\nthrough in-context learning without needing additional training. Expanding on this con-\ncept, the study evaluated LLMs’ ability to operate as pattern machines in three domains:\nsequence transformation, sequence completion, and sequence improvement. In sequence\ntransformation, the research demonstrated that LLMs could generalize speciﬁc sequence\ntransformations using benchmarks such as ARC (abstraction and reasoning corpus) and\nPCFG (probabilistic context-free grammar), thereby proving their utility in spatial reasoning\ntasks for robotics. In sequence completion, the study examined whether LLMs could ﬁnish\npatterns in elementary functions (e.g., sinusoids), illustrating their utility in robotic tasks\nsuch as extending a wiping motion from kinesthetic demonstrations or creating drawings"
  },
  {
    "question": "What limitations were identified in the use of LLMs for robotics?",
    "chunk": "language processing but also extended to broader research areas. This study explored\nextensive LLM applications in the robotics literature, such as planning, manipulation, and\nscene understanding, as well as reinforcement learning automation frameworks such as\nEureka, and included robot actions in language models such as AutoRT. Moreover, the\nresearch direction of current generative AI models is transitioning towards multimodalAppl. Sci.2024, 14, 8868 30 of 39\nlanguage models, moving beyond information acquisition and cognition aspects such as\ntext, images, and videos to include actuator actions within large models in therobotics ﬁeld.\nWhile the surveyed studies indicated that LLMs play a promising role in the future\nof robotics, certain limitations were also identiﬁed. First, the increased computational\nresources and energy consumption associated with embedding LLMs into robotic systems\nmust be addressed. Second, biases in language models and ethical considerations are\nsigniﬁcant issues that need to be tackled in robotics. Therefore, continual efforts will be\nnecessary in future research to resolve these challenges.\nOverall, LLMs are valuable tools that can signiﬁcantly advance robotics. This review\nhas revealed that innovative robot applications are possible through the integration of\nLLMs and VLMs. Moreover, these foundation models are expected to serve as critical\nelements for future robot research and practical applications in the real world.\nAuthor Contributions:Conceptualization, S.S. and C.K.; methodology, S.S.; formal analysis, H.J.,\nH.L. and S.S.; investigation, H.J., H.L. and S.S.; resources, H.J., H.L., S.S. and C.K.; writing—original\ndraft preparation, H.J., H.L., S.S. and C.K.; writing—review and editing, H.J., H.L., S.S. and C.K.; vi-\nsualization, H.J. and H.L.; supervision, S.S. and C.K.; project administration, S.S.; funding acquisition,\nS.S. All authors have read and agreed to the published version of the manuscript.\nFunding: This work was supported by the Technology Innovation Program (RS-2024-00423702,\nA Meta-Humanoid with Hypermodal Cognitivity and Role Dexterity: Adroid4X) funded by the\nMinistry of Trade, Industry, and Energy (MOTIE, Korea) and Regional Innovation Strategy (RIS)\nthrough the National Research Foundation of Korea (NRF) funded by the Ministry of Education\n(MOE) (2023RIS-007).\nInstitutional Review Board Statement:Not applicable.\nInformed Consent Statement:Not applicable.\nData Availability Statement:No new data were created or analyzed in this study. Data sharing is\nnot applicable to this article.\nConﬂicts of Interest:The authors declare no conﬂicts of interest.\nReferences\n1. Hello GPT-4o. Available online:https://openai.com/index/hello-gpt-4o/ (accessed on 13 August 2024).\n2. Vemprala, S.H.; Bonatti, R.; Bucker, A.; Kapoor, A. ChatGPT for Robotics: Design Principles and Model Abilities.IEEE Access\n2024, 12, 55682–55696. [CrossRef]\n3. Hu, Y.; Xie, Q.; Jain, V.; Francis, J.; Patrikar, J.; Keetha, N.; Kim, S.; Xie, Y.; Zhang, T.; Zhao, S.; et al. Toward General-Purpose\nRobots via Foundation Models: A Survey and Meta-Analysis.arXiv 2023, arXiv:2312.08782."
  },
  {
    "question": "What methodologies are used to enhance the performance of 3D-related tasks?",
    "chunk": "grounding, 3D-assisted dialogue, and navigation. The model used a 3D feature extractor to\nalign 3D features from multi-view images with language features, facilitating more precise\ntext generation and question answering based on spatial understanding. To train 3D-LLM,\na pre-trained 2D VLM formed the backbone, enhanced by the addition of 3D positional\nembeddings to better capture 3D spatial information. The model generated location tokens\nthrough linguistic descriptions of speciﬁc objects and was trained using 3D features as input.\nExperimental results showed that 3D-LLM excelled in various 3D-related tasks, achieving\napproximately a 9% higher BLEU-1 score compared to previous models on the ScanQA\ndataset. It demonstrated superior performance in 3D captioning, task composition, andAppl. Sci.2024, 14, 8868 26 of 39\n3D-assisted dialogue, outperforming 2D VLMs and displaying an improved understanding\nof object locations, shapes, and interactions.\nIn the extension of scene understanding using VLMs, the keyword VLN (vision-\nand-language navigation) is widely used in navigation-related research, where language\nfoundation models are increasingly utilized.\nShah [173], as shown in Figure13, introduced a robotic navigation system named\nLM-Nav, which capitalized on the advantages of training with large, unlabeled trajectory\ndatasets while providing a high-level interface for users. LM-Nav utilized three large-scale\npre-trained models: ViNG, CLIP, and GPT-3. Initially, the LLM translated natural language\ninstructions into a sequence of textual landmarks. The VLM integrated these textual\nlandmarks with images to identify the relevant images through probabilistic distribution.\nSubsequently, the VNM utilized these landmarks to plan and execute robot trajectories\nwithin the environment. During this process, the robot utilized a graph search algorithm to\ndetermine optimal trajectories and to navigate along these paths in the real world. This\nmethod demonstrated LM-Nav’s ability to perform long-horizon navigation in complex\noutdoor environments using natural language instructions.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 27 of 39 \n \n \nFigure 13. LM-Nav uses three pre-trained models: ( a) VNM builds a topological graph from \nobservations, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, \n(d) A graph search algorithm then ﬁnds the best robot trajectory, and ( e) the robot executes the \nplanned path [173]. \nZhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow \ninstructions. NavGPT is a vision-language na vigation system that employs an LLM to \ntranslate visual inputs from a visual foundation model (VFM) into natural language. The \nLLM then interprets the current state and makes informed decisions to reach the intended \ngoal, based on these converted visuals, naviga tion history, and potential future routes. \nNavGPT conducts various functions, incl uding high-level planning, decomposing \ninstructions into sub-goals, identifying landmarks in observed scenes, monitoring \nnavigation progress, and modifying plans as necessary. Although NavGPT’s performance \non zero-shot tasks from the R2R dataset has not yet matched that of trained models, it \nunderscored the potential of utilizing multi-modality inputs with LLMs for visual \nnavigation and tapping into the explicit reasoning capabilities of LLMs to enhance learned \nmodels. \nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-"
  },
  {
    "question": "How does RT-1 utilize LLMs for robot control?",
    "chunk": "Du [139] introduced theELLM (exploring with LLMs) framework, which provided\nguidelines for pre-training reinforcement learning using LLMs. ELLM utilized the natural\nlanguage processing capabilities of LLMs to deﬁne goals and furnish reward functions for\nreinforcement learning agents. This strategy enabled agents to undertake meaningful explo-\nration and learning within their environments. The paper assessed ELLM’s performance in\ntwo settings: Crafter, a 2D version of Minecraft, and Housekeep, involving the task of rear-\nranging household objects. Experimental results demonstrated that ELLM surpassed other\nmethods in both settings. In the Crafter setting, ELLM attained high performance through\ngoal-oriented learning, proving especially effective in scenarios with sparse reward signals.\nIn the Housekeep setting, the agent conducted sensible exploration by adhering to goals\nset by the LLM, achieving a high success rate. While the accuracy of goal setting by the\nLLM varied with the objects and locations, it generally showed high performance. These\nexperimental ﬁndings suggested that ELLM was successful in enhancing reinforcement\nlearning performance across diverse environments, highlighting the vital role of providing\nreward signals based on human commonsense.\n4.2. Low-Level Control\nResearch is also being conducted on generating commands that directly control a\nrobot’s actuators (i.e., enabling low-level control) through various applications of LLM\nmodels. Among these projects, the Google research team developed RT-1 [8], which consists\nof ﬁlm-conditioned EfﬁcientNet-B3, TokenLearner, and Transformer. RT-1 is a model that\nreceives images and natural language instructions at a rate of 3Hz and outputs discretized\nrobot actions. RT-1 was trained on a vast demo dataset with over 130k episodes from more\nthan 700 tests collected over 17 months using 13 robots.\nA notable feature of RT-1 is its ability to enhance performance by learning from\ndata gathered from heterogeneous robots or simulations. In the study [8], the authors\nevaluated the performance of a model trained exclusively on data from the EveryDay\nRobot (EDR) against a model trained using data from both EDR and Kuka IIWA robots.\nThey recorded a 12% improvement in the bin-picking test. Another experiment compared\nmodel performance using data from real environments and simulations for items not\nencountered in actual settings. The ﬁndings indicated that incorporating simulation data in\nRT-1 training enhances performance over using purely real environment data, suggesting\nthat RT-1 can substantially improve model performance by integrating diverse data from\nrobots of varied morphologies or simulations while sustaining existing task capabilities.Appl. Sci.2024, 14, 8868 15 of 39\nRT-2 [9] is deﬁned as a vision-language-action (VLA) model that facilitates ﬁne-grained\ncontrol of robots through vision and language commands. RT-2 is ﬁne-tuned with robotic\ntrajectory data based on VLM models such as PaLM-E [140], which has 12 billion parameters\nand is trained on VQA data, alongside PaLI-X [141], which has parameter sizes ranging\nfrom 5 billion to 55 billion. The RT-2 system operates as an integrated closed-loop robotic\nsystem that combines low-level and high-level control policies. Despite not explicitly\nlearning certain capabilities during pre-training, RT-2 exhibits improved task performance\nvia real-world generalization involving diverse objects, visual scenes, and instructional\ncontexts. The paper [9] quantitatively assesses RT-2’s emergent capabilities in areas such as\nreasoning, symbol understanding, and human recognition. Furthermore, applying chain-"
  },
  {
    "question": "How does the new multi-task benchmark support robot manipulation learning?",
    "chunk": "on the current image, command, and identiﬁed object data. Experimental use of real mobile\nmanipulation robots showed that MOO could adapt to new object types and environments\nin a zero-shot fashion. Moreover, MOO responded to non-verbal cues such as pointing at\nspeciﬁc objects, extending its scope to open-world exploration and manipulation.Appl. Sci.2024, 14, 8868 23 of 39\nExisting VLMs often lack a comprehensive understanding of physical concepts such\nas material and fragility, which limits their effectiveness in robotic manipulation tasks. To\naddress this issue, Gao [162] introduced PhysObjects, an object-centric dataset featuring\n39.6K crowd-sourced annotations and 417K automated annotations of physical concepts.\nThe automated annotations involved assigning speciﬁc concept values to predeﬁned object\ncategories or continuous concepts such as material and fragility. Fine-tuning a VLM on\nPhysObjects enhanced comprehension of physical concepts by capturing human biases\nrelated to the visual appearance of objects. Integrating this physically grounded VLM\nwith an LLM-based robotic planner framework improved performance in tasks requiring\nreasoning about physical concepts.\nThe traditional pre-training and ﬁne-tuning pipeline often suffers from decreased\nlearning efﬁciency and challenges in generalizing to unseen objects and tasks due to its\nreliance on domain-speciﬁc action information and domain-general visual information. To\naddress these limitations, Wang [163] proposed a modular approach named ProgramPort,\nwhich utilizes the syntactic and semantic structure of language instructions. Wang’s\nframework incorporated a semantic parser to reconstruct executable programs, composed of\nfunctional modules based on vision and action across multiple modalities. Each functional\nmodule combined deterministic computation with learnable neural networks. Program\nexecution involved generating parameters for general manipulation primitives used by the\nrobot’s end effector. The entire module network was trainable with an end-to-end imitation\nlearning objective. Experimental results demonstrated that the model effectively separated\naction and perception, achieving enhanced zero-shot and compositional generalization\nacross various manipulation tasks, speciﬁcally 16 tasks related to robot manipulation.\nHa [164] proposed a framework aimed at robot skill acquisition. This framework\nprovided a comprehensive solution by utilizing language guidance, without necessitating\nexpert demonstrations or reward speciﬁcation/engineering. It consisted of two main\ncomponents. The ﬁrst component, scaling up language-guided data generation, employed\nLLMs to break down tasks into subtasks and generate a hierarchical plan or task tree. This\nplan was materialized into various robot trajectories using 6-DoF exploration primitives.\nThese trajectories were subsequently veriﬁed and retries were performed as needed until\nsuccess was achieved. This approach enhanced the success rate of data collection and\nmore effectively mitigated the low-level understanding gap in LLMs by incorporating retry\nprocesses as part of the robot’s experiences. The second component, distilling down to\nlanguage-conditioned visuomotor policy, transformed robot experiences into a policy that\ndeduced control sequences from visual observations and natural language task descriptions.\nBy extending diffusion policies, this component handled language-based conditioning for\nmulti-task learning. To assess long-horizon behavior, commonsense reasoning, tool use,\nand intuitive physics, a new multi-task benchmark comprising 18 tasks related to robot\nmanipulation across ﬁve domains (mailbox, transport, drawer, catapult, and bus balance)\nwas developed. This benchmark effectively supported the learning of retry behaviors in\nthe data collection process and enhanced success rates.\nHuang [165], as shown in Figure12, aimed to synthesize dense robot trajectories,\nincluding 6-DoF end-effector waypoints, for various manipulation tasks using an open set"
  },
  {
    "question": "How does the robot gather sensor readings for task execution?",
    "chunk": "dite trajectory planning. Additionally, environmental feedback, such as collision detection,\nprompts the LLM agent to reﬁne plans and waypoints contextually. This method achieved\na high success rate across all tasks in the RoCoBench (including duties such as sweeping\nthe ﬂoor), effectively adapting to variations in task semantics. In real-world experiments,\nspeciﬁcally the block-sorting task, RoCo demonstrated its ability to communicate and\ncollaborate with other robot agents to successfully complete the tasks.\nWang [146] proposed a novel paradigm for utilizing few-shot prompts in physical\nenvironments. This method involved gathering observation and action pairs from existing\nmodel-based or learning-based controllers to form the initial text prompts. Data included\nsensor readings, such as IMU and joint encoders, coupled with target joint positions.\nThese data formed the starting input for LLM inference. As the robot interacted with its\nenvironment and collected new observational data, these initial data were updated with\noutputs from the LLM. In the subsequent prompt engineering phase, observation and action\npairs, along with explanatory prompts, were crafted to enable the LLM to function as a\nfeedback policy. The explanatory prompts provided clear descriptions of the robot walking\ntask and control design details, while the observation and action prompts delineated the\nformat and signiﬁcance of each observation and action. This method allowed the LLM\nto directly output low-level target joint positions for robot walking. The approach was\ntested using the ANYmal robot in MuJoCo and Isaac Gym simulators for robot walking,\nindicating that the LLM could act as a low-level feedback controller for dynamic motion\ncontrol within sophisticated robot systems.\nLiang [147] introduced a new framework namedCode as Policies(CaP) that directly\nconstructs robot policies from executable code generated by a code LLM. This framework\nenabled the interpretation and execution of natural language instructions through an LLM,\nsupporting the creation of high-level policies for robots and accommodating a variety of\nrobotic tasks. Speciﬁcally, CaP interpreted natural language instructions through descrip-\ntions and formulated an action plan for the robot. Moreover, it utilized VLMs such as ViLD\nand MDETR to identify objects and ascertain their locations. Based on this information,\nthe framework controlled the robot’s movements to carry out speciﬁed tasks. The paper\ndemonstrated the CaP framework across diverse domains, including whiteboard drawing,\ntabletop manipulation, and mobile robot navigation and manipulation. Experimental\nresults showed that CaP achieved similar or better success rates than existing systems\nsuch as CLIPort, displaying notably strong generalization capabilities for new tasks. These\nﬁndings underscored the ﬂexibility and efﬁcacy of the CaP framework, establishing its\neffectiveness across various robotic systems.\nMirchandani [148], shown in Figure6, suggested that pre-trained LLMs could autore-\ngressively complete complex token sequences and function as general sequence modelers\nthrough in-context learning without needing additional training. Expanding on this con-\ncept, the study evaluated LLMs’ ability to operate as pattern machines in three domains:\nsequence transformation, sequence completion, and sequence improvement. In sequence\ntransformation, the research demonstrated that LLMs could generalize speciﬁc sequence\ntransformations using benchmarks such as ARC (abstraction and reasoning corpus) and\nPCFG (probabilistic context-free grammar), thereby proving their utility in spatial reasoning\ntasks for robotics. In sequence completion, the study examined whether LLMs could ﬁnish\npatterns in elementary functions (e.g., sinusoids), illustrating their utility in robotic tasks\nsuch as extending a wiping motion from kinesthetic demonstrations or creating drawings"
  },
  {
    "question": "Why is it challenging to fine-tune large models on standard hardware?",
    "chunk": "to attend only to past and present tokens, processing input and output tokens similarly\nthrough the decoder. This method underpins the development of the GPT series. Lastly, the\npreﬁx decoder, resembling the causal decoder’s masking mechanism, allows bidirectional\nattention on preﬁx tokens [107] and unidirectional attention on generated tokens. Similar\nto the encoder–decoder, the preﬁx decoder bidirectionally encodes the preﬁx sequence and\nsequentially predicts output tokens individually. Examples of preﬁx decoder-based LLMs\ninclude GLM-130B [108] and U-PaLM [109]. Additionally, various architectures have been\nproposed to address efﬁciency challenges during training or inference with long inputs,\ndue to the quadratic computational complexity of the traditional transformer architecture.\nFor instance, the Mixture-of-Experts (MoE) scaling method [34] sparsely activates a subset\nof the neural network for each input.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 8 of 39 \n \n[47] and BART [33], with Flan-T5 [106] be ing an encoder–decoder-based LLM. Secondly, \nthe causal decoder employs a unidirectional attention mask to restrict each input token to \nattend only to past and present tokens, proc essing input and output tokens similarly \nthrough the decoder. This method underpins the development of the GPT series. Lastly, \nthe preﬁx decoder, resembling the causal decoder’s masking mechanism, allows bidirec-\ntional attention on preﬁx tokens [107] and unidirectional attention on generated tokens. \nSimilar to the encoder–decoder, the pre ﬁx decoder bidirectionally encodes the pre ﬁx se-\nquence and sequentially predicts output tokens individually. Examples of preﬁx decoder-\nbased LLMs include GLM-130B [108] and U-PaLM [109]. Additionally, various architec-\ntures have been proposed to address e ﬃciency challenges during training or inference \nwith long inputs, due to the quadratic computational complexity of the traditional trans-\nformer architecture. For instance, the Mixture-of-Experts (MoE) scaling method [34] \nsparsely activates a subset of the neural network for each input. \n \nFigure 2. Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx De-\ncoder (middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectan-\ngles represent attention between preﬁx tokens, attention between preﬁx and target tokens, atten-\ntion between target tokens, and masked attention [5]. \nIn terms of the tuning of LLMs, these models are essentially pre-trained on massive \ndatasets and require ﬁne-tuning for diﬀerent application domains. However, the consid-\nerable model size and number of parameters pose challenges for ﬁne-tuning on standard \ncomputers and GPUs. The subsequent sections will discuss methods to address these chal-\nlenges. \nLLM tuning is broadly divided into two cate gories based on the training objective. \nInstruction tuning is a form of supervised learning where the training data typically in-\nclude descriptions of tasks, inputs, and corresponding outputs. This type of tuning is de-\nsigned (1) to enhance the functional capabilities of LLMs, (2) to specialize them by training \nwith discipline-speci ﬁc information, and (3) to improve task generalization and con-"
  },
  {
    "question": "What types of tasks can ProgPrompt be applied to?",
    "chunk": "Figure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 9.LLM-Planner is a system that creates high-level plans based on natural language commands,\nsets subgoals to determine actions, and continuously updates the plan to reﬂect environmental\nchanges [155].\nSingh [156], as shown in Figure10, introduced ProgPrompt, a programmatic LLM\nprompt structure designed for generating plans across diverse situated environments,\nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that\nleveraged LLMs and included a Python programming structure to facilitate information\nabout the environment and executable actions. It featured a feedback mechanism, using\nexecutable program plan examples and assertion statements to mitigate errors, enhancing\ntask success rates. Additionally, ProgPrompt veriﬁed the current state through environ-\nmental feedback during plan execution and revised the plan accordingly. The results\nindicated that the integration of programming language features substantially improved\ntask performance in contexts such as VirtualHome and real-world manipulation tasks in\nterms of success rate, goal conditions recall, and executability.Appl. Sci.2024, 14, 8868 21 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 10.ProgPrompt is a system that uses Python programming structures to provide environ-\nmental information and actions, enhancing the success rate of robot task planning through an error"
  },
  {
    "question": "How does the LLM-Planner adapt its plans during execution?",
    "chunk": "Figure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable\nrobots to carry out instructions: (a) mobile manipulation and (b,c) tabletop manipulation, in both\nsimulated and real-world environments [153].Appl. Sci.2024, 14, 8868 20 of 39\nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn,\na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot\nbehavior trees (BTs) from textual descriptions. The developed model was compact enough\nto operate on a robot’s onboard microcomputer, while adept at constructing complex robot\nbehaviors. It provided structurally and logically correct BTs and demonstrated the ability\nto handle instructions that were not included in the training set.\nSong [155], as shown in Figure9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions\nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via\na low-level planner. It continuously updated environmental information as new objects\nwere detected during action implementation and revisited the LLM to adjust the plan if\nsubgoals failed or were delayed based on updated observations. This iterative process was\nrepeated until the subgoal was achieved, after which the system moved to the next goal.\nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated\ncompetitive performance with signiﬁcantly reduced training data and proved its ability to\ngeneralize in various tasks (e.g., ALFRED) with minimal examples.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39"
  },
  {
    "question": "How do quantization methods assist in fine-tuning LLMs?",
    "chunk": "model focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM \nmodel inference. \n \nFigure 3. An overview of four strategies for parameter-eﬃcient ﬁne-tuning: (a) Adapter Tuning, \n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5]. \nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a \nlow-rank constraint on transformer layers to approximate the update matrices through \ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates \nthe parameter updates using low-rank dec omposition matrices. The primary bene ﬁt of \nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning, \nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory \nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning. \nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119]. \n3.3. Prompt Techniques for Increasing LLM Performance \nTo enhance the performance of LLMs, the most straightforward approach involves \ntraining with additional data via ﬁne-tuning techniques, which mirrors supervised learn-\ning in conventional machine learning. Another method for improving performance in-\nvolves the use of in-context learning, which capitalizes on prompts for zero-shot learning, \na capability ﬁrst observed in LLMs with the advent of GPT-3. The adaptation of these \nprompts for speciﬁc tasks is known as prompt engineering. Fundamentally, prompt engi-\nneering (or prompting) entails supplying inputs  to the model to perform a distinct task, \ndesigning the input format to encapsulate the task’s purpose and context, and generating \nthe desired output. The four components of pr ompt engineering can be analyzed as fol-\nlows: within the prompt, “ Instructions” delineate the speci ﬁc tasks or directives for the \nmodel and “Context” provides external or additional contextual information that can tune \nthe model. Furthermore, “Input data” refers to the type of input or questions seeking an-\nswers, and “ Output data” deﬁnes the output type or format within the prompt, thereby \noptimizing the LLM’s performance for particular tasks. Various methodologies for creat-\ning prompts have been introduced, as described below. \nFigure 3. An overview of four strategies for parameter-efﬁcient ﬁne-tuning: (a) Adapter Tuning,\n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5].\nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a\nlow-rank constraint on transformer layers to approximate the update matrices through\ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates\nthe parameter updates using low-rank decomposition matrices. The primary beneﬁt of\nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning,\nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory\nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning.\nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119].\n3.3. Prompt Techniques for Increasing LLM Performance"
  },
  {
    "question": "How do automated systems address the limitations of traditional reinforcement learning methods?",
    "chunk": "4. Language Models for Robotic Intelligence\n4.1. Reward Design in Reinforcement Learning\nResearch in reinforcement learning, closely associated with the ﬁeld of robotics, has\nactively incorporated studies using LLM models. Speciﬁcally, Nvidia has developed a\nGPU-based multi-environment reinforcement learning platform. Utilizing its Omniverse\n3D virtual environment platform, Nvidia created Isaac Sim, which is dedicated to robot\nsimulation. Isaac Sim published research ﬁndings on Isaac Gym (Preview), which achieved\nsigniﬁcant reductions in reinforcement learning training times through GPU-based multi-\nenvironment approaches. Subsequently, Isaac Gym (Preview)’s features were integrated\ninto Isaac Sim and released as Omni Isaac Gym. Later, Nvidia introduced Orbit [133],\nfacilitating the simulation of PhysX 5.1-based cloth, soft-body, ﬂuid, and rigid-body dy-\nnamics, along with RGBD, LiDAR, and contact sensor simulation. Orbit also incorporates\nvarious robot platforms into the simulation environment. Recently, Orbit was updated\nto Isaac Lab and integrated into Isaac Sim 4.0. Nvidia has continuously advanced dy-\nnamic simulation environment technologies for reinforcement learning using GPU parallel\ncomputation. Leveraging this GPU reinforcement learning, they launched Eureka [11],\nwhich automates the design of reward functions for reinforcement learning using LLMs.\nFollowing this, Nvidia introduced DrEureka [134], an automated platform addressing the\nSim2Real problem [135] in reinforcement learning based on Eureka.\nEureka (Evolution-driven Universal REward Kit for Agent) [11], shown in Figure4,\nautomatically generates reward functions for various tasks using different robots, elimi-\nnating the need for speciﬁc templates or tailored reward functions for the robot’s form or\nexplanations for reinforcement learning tasks. Eureka consists of three main components:\nenvironment-as-context, evolutionary search, andreward reﬂection. Environment-as-context\ngenerates executable reward functions in a zero-shot manner by utilizing virtual environ-\nment source (Python) code as context. Evolutionary search iteratively generates reward\nfunction candidates and proposes enhanced functions based on previously generated and\nbest-performing ones, while also creating new functions through mutation. Reward reﬂec-\ntion offers a text summary of reward function quality based on training statistics recorded\nduring reinforcement learning, which assists in generating subsequent reward functions\nas feedback for the performance of previous functions. The reward functions generated\noutperformed expert-generated functions in 83% of benchmark tests. Moreover, Eureka\nsolved the pen spinning problem where a robot hand must spin a pen as much as possi-\nble according to predeﬁned rotations, a task previously considered unsolvable through\nmanual reward engineering. Eureka introduces a universal reward function design algo-\nrithm based on a code LLM and in-context evolutionary search, facilitating human-level\nreward generation for various robots and tasks without the need for prompt engineering or\nhuman intervention.\nFollowing Eureka, DrEureka [134], shown in Figure5, was developed to address\nthe sim-to-real problem by automatically conﬁguring appropriate reward functions and\ndomain randomization for physical environments. DrEureka’s reward-aware physics pri-\nors mechanism deﬁnes the lower and upper bounds of physical environment parameters\nbased on policies trained through initial reinforcement learning, facilitating reinforcement\nlearning across various physical environment domains. This randomization enables the\ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation"
  },
  {
    "question": "What kind of environments were used to test the navigation models?",
    "chunk": "trained vision-language features with a 3D reconstruction of the physical world. VLMaps, \nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary \nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands. \nThese commands can be directly localized on a map and generate new obstacle maps in \nreal-time, facilitated by sharing among various robot types. Extensive experiments \nconducted in both simulated environments (using the Habitat simulator with the \nMatterport3D dataset and the AI2THOR simulator) and real-world settings (with the HSR \nmobile robot for indoor navigation) demonstrated that VLMs can navigate based on more \ncomplex language instructions than previous methods. The reviewed papers in this study \nare summarized in Table 5. \nTable 5. Summary of the reviewed papers in this study. \nName Explanation Ref. \nReward Design in \nRL \n• Eureka automatically generates and im proves reward functions based on the \nvirtual environment source code.$• Dr Eureka builds reward-aware physics \npriors using Eureka and supports eﬀective operation in the real world through \ndomain randomization.$• LLMs design and re ﬁne reward functions based on \nnatural language input.$• LLMs and VLMs integrate multimodal data to \ngenerate reward functions. \n[11,134,136–139,176–\n180] \nFigure 13.LM-Nav uses three pre-trained models: (a) VNM builds a topological graph from observa-\ntions, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, (d)A\ngraph search algorithm then ﬁnds the best robot trajectory, and (e) the robot executes the planned\npath [173].\nZhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow\ninstructions. NavGPT is a vision-language navigation system that employs an LLM to trans-\nlate visual inputs from a visual foundation model (VFM) into natural language. The LLM\nthen interprets the current state and makes informed decisions to reach the intended goal,\nbased on these converted visuals, navigation history, and potential future routes. NavGPT\nconducts various functions, including high-level planning, decomposing instructions into\nsub-goals, identifying landmarks in observed scenes, monitoring navigation progress, and\nmodifying plans as necessary. Although NavGPT’s performance on zero-shot tasks from\nthe R2R dataset has not yet matched that of trained models, it underscored the potential\nof utilizing multi-modality inputs with LLMs for visual navigation and tapping into the\nexplicit reasoning capabilities of LLMs to enhance learned models.\nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-\ntrained vision-language features with a 3D reconstruction of the physical world. VLMaps,\nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary\nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands.\nThese commands can be directly localized on a map and generate new obstacle maps in real-Appl. Sci.2024, 14, 8868 27 of 39\ntime, facilitated by sharing among various robot types. Extensive experiments conducted in\nboth simulated environments (using the Habitat simulator with the Matterport3D dataset\nand the AI2THOR simulator) and real-world settings (with the HSR mobile robot for\nindoor navigation) demonstrated that VLMs can navigate based on more complex language\ninstructions than previous methods. The reviewed papers in this study are summarized in\nTable 5."
  },
  {
    "question": "What is the significance of integrating high-level reasoning with low-level control in robotics?",
    "chunk": "parsing them into verb frames to convert them into executable structures. During the\ncommonsense reasoning phase, the robot analyzed surrounding objects and employed a\nlanguage model trained on large-scale unstructured text materials to ﬁll in the missing\ndetails from the instructions. This model identiﬁed the most suitable verb frame to com-\nplete the gaps. Subsequently, based on the completed verb frame, the robot formulated its\nactions using predeﬁned action plans for each verb to guide the movements of the robot\narm and execute the assignment. Experimental results showed that LMCR demonstrated\nsuperior generalization performance for novel concepts not presented in the training set\nand surpassed GCNGrasp, which depends on a predeﬁned graph structure for all concepts\nand their relationships. This indicated that LMCR was an effective tool, combining the se-\nmantic reasoning capabilities of language models with planning that adapted to the robot’s\nspeciﬁc environment and context, effectively managing complex and prolonged tasks.\nHuang [152] introduced a methodology named grounded decoding (GD), which offers\na method for generating LLM-based robot action plans. These plans enable robots to execute\nlong-term tasks across diverse physical environments. The methodology encompasses two\nprimary elements: linking the text generated by the language model to actionable taskAppl. Sci.2024, 14, 8868 19 of 39\ncommands in the physical world via GD and adjusting the tokens generated by the LLM\nto real-world conditions to formulate feasible commands. This approach synergizes the\nhigh-level semantic reasoning of LLMs with plans that are aligned with the robot’s physical\nenvironment and capabilities, thus facilitating the execution of complex and long-term\ntasks. The method addresses several limitations robots face in performing complex, long-\nterm tasks, such as a lack of physical world experience, an inability to process non-verbal\ncues, and a disregard for necessary robotic constraints such as safety and rewards. The\npaper details experiments in a simulated tabletop rearrangement, a mini-grid 2D maze,\nand real-world kitchen mobile manipulation settings to evaluate long-horizon reasoning\nperformance. Comparative experiments with SayCan revealed that while SayCan limits\nthe range of robot actions, GD can represent a wider array of actions. In contrast to\nCLIPort, which executes high-level language instructions directly, GD achieves enhanced\nperformance through detailed, step-by-step planning.\nHuang [153], as shown in Figure8, proposed the inner monologue method, which\nallowed LLMs to plan and adjust based on feedback from the environment. This approach\nenabled robots to formulate plans in dynamic environments, retry upon facing failure,\nor seek human feedback to reﬁne their strategies. The author clariﬁed that this method\nemerged from integrating the LLM’s high-level planning capabilities with perceptual feed-\nback and low-level control, thereby facilitating more adaptable and intelligent interactions.\nInner monologue integrated various feedback sources into the language model to assist the\nrobot in executing given instructions, including text-based indicators of the robot’s action\nsuccess or failure, object recognition and descriptions within the scene, the robot’s ability to\nask questions to gather additional information, breaking down instructions into multiple\nsteps to establish an execution plan, and enabling the robot to interact with humans to\nexecute and reﬁne the instructions. The inner monologue method was evaluated in both\nsimulated and real-world environments, such as tabletop rearrangement tasks and manip-\nulation tasks in a real kitchen. The results showed that inner monologue was an effective\nframework, enabling robots to act intelligently in complex interactive settings by effectively\nintegrating environmental feedback to plan and execute tasks.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 20 of 39"
  },
  {
    "question": "In what ways does RT-2 enhance robot control capabilities?",
    "chunk": "Du [139] introduced theELLM (exploring with LLMs) framework, which provided\nguidelines for pre-training reinforcement learning using LLMs. ELLM utilized the natural\nlanguage processing capabilities of LLMs to deﬁne goals and furnish reward functions for\nreinforcement learning agents. This strategy enabled agents to undertake meaningful explo-\nration and learning within their environments. The paper assessed ELLM’s performance in\ntwo settings: Crafter, a 2D version of Minecraft, and Housekeep, involving the task of rear-\nranging household objects. Experimental results demonstrated that ELLM surpassed other\nmethods in both settings. In the Crafter setting, ELLM attained high performance through\ngoal-oriented learning, proving especially effective in scenarios with sparse reward signals.\nIn the Housekeep setting, the agent conducted sensible exploration by adhering to goals\nset by the LLM, achieving a high success rate. While the accuracy of goal setting by the\nLLM varied with the objects and locations, it generally showed high performance. These\nexperimental ﬁndings suggested that ELLM was successful in enhancing reinforcement\nlearning performance across diverse environments, highlighting the vital role of providing\nreward signals based on human commonsense.\n4.2. Low-Level Control\nResearch is also being conducted on generating commands that directly control a\nrobot’s actuators (i.e., enabling low-level control) through various applications of LLM\nmodels. Among these projects, the Google research team developed RT-1 [8], which consists\nof ﬁlm-conditioned EfﬁcientNet-B3, TokenLearner, and Transformer. RT-1 is a model that\nreceives images and natural language instructions at a rate of 3Hz and outputs discretized\nrobot actions. RT-1 was trained on a vast demo dataset with over 130k episodes from more\nthan 700 tests collected over 17 months using 13 robots.\nA notable feature of RT-1 is its ability to enhance performance by learning from\ndata gathered from heterogeneous robots or simulations. In the study [8], the authors\nevaluated the performance of a model trained exclusively on data from the EveryDay\nRobot (EDR) against a model trained using data from both EDR and Kuka IIWA robots.\nThey recorded a 12% improvement in the bin-picking test. Another experiment compared\nmodel performance using data from real environments and simulations for items not\nencountered in actual settings. The ﬁndings indicated that incorporating simulation data in\nRT-1 training enhances performance over using purely real environment data, suggesting\nthat RT-1 can substantially improve model performance by integrating diverse data from\nrobots of varied morphologies or simulations while sustaining existing task capabilities.Appl. Sci.2024, 14, 8868 15 of 39\nRT-2 [9] is deﬁned as a vision-language-action (VLA) model that facilitates ﬁne-grained\ncontrol of robots through vision and language commands. RT-2 is ﬁne-tuned with robotic\ntrajectory data based on VLM models such as PaLM-E [140], which has 12 billion parameters\nand is trained on VQA data, alongside PaLI-X [141], which has parameter sizes ranging\nfrom 5 billion to 55 billion. The RT-2 system operates as an integrated closed-loop robotic\nsystem that combines low-level and high-level control policies. Despite not explicitly\nlearning certain capabilities during pre-training, RT-2 exhibits improved task performance\nvia real-world generalization involving diverse objects, visual scenes, and instructional\ncontexts. The paper [9] quantitatively assesses RT-2’s emergent capabilities in areas such as\nreasoning, symbol understanding, and human recognition. Furthermore, applying chain-"
  },
  {
    "question": "What are the main features of the HSR mobile robot used in experiments?",
    "chunk": "trained vision-language features with a 3D reconstruction of the physical world. VLMaps, \nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary \nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands. \nThese commands can be directly localized on a map and generate new obstacle maps in \nreal-time, facilitated by sharing among various robot types. Extensive experiments \nconducted in both simulated environments (using the Habitat simulator with the \nMatterport3D dataset and the AI2THOR simulator) and real-world settings (with the HSR \nmobile robot for indoor navigation) demonstrated that VLMs can navigate based on more \ncomplex language instructions than previous methods. The reviewed papers in this study \nare summarized in Table 5. \nTable 5. Summary of the reviewed papers in this study. \nName Explanation Ref. \nReward Design in \nRL \n• Eureka automatically generates and im proves reward functions based on the \nvirtual environment source code.$• Dr Eureka builds reward-aware physics \npriors using Eureka and supports eﬀective operation in the real world through \ndomain randomization.$• LLMs design and re ﬁne reward functions based on \nnatural language input.$• LLMs and VLMs integrate multimodal data to \ngenerate reward functions. \n[11,134,136–139,176–\n180] \nFigure 13.LM-Nav uses three pre-trained models: (a) VNM builds a topological graph from observa-\ntions, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, (d)A\ngraph search algorithm then ﬁnds the best robot trajectory, and (e) the robot executes the planned\npath [173].\nZhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow\ninstructions. NavGPT is a vision-language navigation system that employs an LLM to trans-\nlate visual inputs from a visual foundation model (VFM) into natural language. The LLM\nthen interprets the current state and makes informed decisions to reach the intended goal,\nbased on these converted visuals, navigation history, and potential future routes. NavGPT\nconducts various functions, including high-level planning, decomposing instructions into\nsub-goals, identifying landmarks in observed scenes, monitoring navigation progress, and\nmodifying plans as necessary. Although NavGPT’s performance on zero-shot tasks from\nthe R2R dataset has not yet matched that of trained models, it underscored the potential\nof utilizing multi-modality inputs with LLMs for visual navigation and tapping into the\nexplicit reasoning capabilities of LLMs to enhance learned models.\nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-\ntrained vision-language features with a 3D reconstruction of the physical world. VLMaps,\nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary\nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands.\nThese commands can be directly localized on a map and generate new obstacle maps in real-Appl. Sci.2024, 14, 8868 27 of 39\ntime, facilitated by sharing among various robot types. Extensive experiments conducted in\nboth simulated environments (using the Habitat simulator with the Matterport3D dataset\nand the AI2THOR simulator) and real-world settings (with the HSR mobile robot for\nindoor navigation) demonstrated that VLMs can navigate based on more complex language\ninstructions than previous methods. The reviewed papers in this study are summarized in\nTable 5."
  },
  {
    "question": "In what ways does adapter tuning reduce the number of trainable parameters?",
    "chunk": "through a better understanding of natural language commands. Conversely, alignment\ntuning (or preference alignment) seeks to align the behavior of LLMs with human values\nand preferences. Prominent methods include reinforcement learning from human feedbackAppl. Sci.2024, 14, 8868 9 of 39\n(RLHF) [110], which involves ﬁne-tuning LLMs using human feedback to better reﬂect\nhuman values, and direct preference optimization (DPO) [111], focusing on training with\npairs of human preferences that usually include an input prompt and the preferred and\nnon-preferred responses.\nFor both instruction tuning and alignment tuning, which involve training LLMs\nwith extensively large model parameters, substantial GPU memory and computational\nresources are required, with high costs typically incurred when utilizing cloud-based\nresources. Under these conditions, parameter-efﬁcient ﬁne-tuning (PEFT) offers a method\ndesigned to efﬁciently conduct ﬁne-tuning of such LLMs [112].\nAmong the methods of PEFT, there are four major approaches as shown in Figure3:\nadapter tuning, prompt tuning, preﬁx tuning, and low-rank adaptation (LoRA). Adapter\ntuning [113,114] involves integrating small neural network modules, known as adapters,\ninto the core components of a transformer model, speciﬁcally into the attention and feed-\nforward layers. These adapters are inserted serially following these layers, allowing ﬁne-\ntuning of only the adapter modules according to speciﬁc task goals, while the parameters\nof the original language model remain unchanged. Consequently, adapter tuning effec-\ntively reduces the number of trainable parameters. Additionally, prompt tuning [115,116]\ndiverges from adapter tuning by adding trainable prompt vectors to the input layer. Preﬁx\ntuning [117] entails appending a sequence of preﬁxes to each transformer layer of the\nlanguage model, which consists of trainable continuous vectors. During ﬁne-tuning, the\nmodel focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM\nmodel inference.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 9 of 39 \n \nresources. Under these conditions, parameter-eﬃcient ﬁne-tuning (PEFT) oﬀers a method \ndesigned to eﬃciently conduct ﬁne-tuning of such LLMs [112]. \nAmong the methods of PEFT, there are four major approaches as shown in Figure 3: \nadapter tuning, prompt tuning, preﬁx tuning, and low-rank adaptation (LoRA). Adapter \ntuning [113,114] involves integrating small neural network modules, known as adapters, \ninto the core components of a transformer model, speciﬁcally into the attention and feed-\nforward layers. These adapters are inserted serially following these layers, allowing ﬁne-\ntuning of only the adapter modules according to speciﬁc task goals, while the parameters \nof the original language model remain unchanged. Consequently, adapter tuning e ﬀec-\ntively reduces the number of trainable parameters. Additionally, prompt tuning [115,116] \ndiverges from adapter tuning by adding trainable prompt vectors to the input layer. Preﬁx \ntuning [117] entails appending a sequence of preﬁxes to each transformer layer of the lan-\nguage model, which consists of trainable continuous vectors. During ﬁne-tuning, the"
  },
  {
    "question": "What is LLM-Grounder and how does it work for 3D scene understanding?",
    "chunk": "Chen [168] explored methods to integrate commonsense into scene understanding\nusing LLMs and introduced three paradigms for classifying room types within indoor\nenvironments based on included objects. The zero-shot approach utilized a pre-trained\nlanguage model to identify the objects in a room and estimate their types. The feed-\nforward classiﬁer approach involved inputting sentences that listed a room’s objects into\nthe language model to generate embedding vectors, which were subsequently input into a\npre-trained shallow multilayer perceptron to predict each room type. Lastly, the classiﬁer\napproach embedded images of rooms alongside textual descriptions to identify the best-\nmatching description, thereby determining the room type. These paradigms demonstrated\nthe capacity to generalize to objects not presented in the training set and to make inferences\nwithin a space larger than that deﬁned by the trained object labels.\nYang [169] introduced the innovative zero-shot, open-vocabulary, LLM-based 3D\nvisual grounding pipeline called LLM-Grounder. This method breaks down complex natu-\nral language queries into semantic components and uses visual grounding tools such as\nOpenScene or LERF to locate objects within 3D scenes. Subsequently, the LLM evaluates\nspatial and commonsense relationships among these objects to achieve the ﬁnal grounding.\nRemarkably, LLM-Grounder operates without labeled training data and has proven its ca-\npacity to adapt to new 3D scenes and diverse text queries, enhancing grounding capabilities\nfor complex language queries and establishing itself as an effective solution.\nChen [170] developed NLMap, an open-vocabulary, queryable scene representation\nsystem. Designed to accumulate and incorporate contextual data within a scene repre-\nsentation for natural language queries, this system allows an LLM planner to visualize\nand query objects, thereby generating contextual plans. Initially, a VLM sets up a scene\nrepresentation for natural language queries; then, an LLM-based object suggestion module\nreviews instructions, suggests relevant objects, and queries the scene for object availability\nand location. Using this information, the LLM planner devises plans uniquely tailored to\nthe scene’s context. NLMap equips robots with the ability to function without a predeﬁned\ncatalog of objects or actions, overcoming the constraints of earlier methods and enabling\nmore adaptable operations in environments with novel or absent objects.\nElhafsi [171] introduced a monitoring framework that employed an LLM with superior\ncontextual understanding and reasoning capabilities to detect edge cases and anomalies\nwithin vision-based policies. This framework monitored the robot’s perception stream\nthrough an LLM-based module, designed to detect semantic anomalies that might occur\nduring operations. By converting the robot’s visual observations into textual descriptions\nat regular intervals and integrating these into LLM prompts, it could pinpoint factors\nleading to policy errors, unsafe behavior, or task confusion. The conversion of visual\ninformation into natural language descriptions used various techniques, without restriction\nto any speciﬁc method. This ﬂexibility enabled both fully end-to-end policies and classical\nautonomy stacks using learned perception to align more closely with human intuition.\nThe ﬁndings indicated that semantic anomalies did not always correspond to semantically\nexplainable failures, and end-to-end policies could sometimes behave unpredictably.\nHon [172] introduced a new model family named 3D-LLM, which incorporated 3D\nworld information into LLMs. The 3D-LLM model utilized 3D point clouds and their\nfeatures as input, enabling it to handle a variety of spatially aware 3D tasks. These tasks\nincluded 3D captioning, dense captioning, 3D question answering, task decomposition, 3D"
  },
  {
    "question": "What are the implications of sparse reward signals in learning environments?",
    "chunk": "agents’ understanding of human intentions. They developed a framework that utilized\nlanguage as a primary inference tool, investigating how it could address key challengesAppl. Sci.2024, 14, 8868 14 of 39\nin reinforcement learning, such as efﬁcient exploration, data reuse in experience, skill\nscheduling, and observational learning. This framework employed LLMs and VLMs to\naddress these reinforcement learning challenges by (1) efﬁciently exploring environments\nwith sparse rewards, (2) reusing collected data to sequentially bootstrap the learning of\nnew tasks, (3) scheduling learned skills for novel tasks, and (4) acquiring knowledge from\nobserving expert agents.\nDu [138] developed success detectors that identiﬁed whether actions or tasks were\nsuccessfully completed, utilizing the large multimodal language model Flamingo and\nhuman reward annotations. The study on success detection spanned three distinct do-\nmains: (1) interactive language-conditioned agents in simulated households, (2) real-world\nrobotic manipulation tasks (inserting and removing small, medium, and large gears), and\n(3) “in-the-wild” human egocentric videos. These success detectors adapted to new lan-\nguage instructions and visual changes using VLMs such as Flamingo, which were trained\non a broad range of language and visual data. Furthermore, success detection was reframed\nas a VQA problem, enabling the tracking of task progress through multiple frames to ascer-\ntain whether tasks had been successfully completed. The proposed method proved to be\nmore accurate in detecting success compared to custom reward models in the ﬁrst two do-\nmains, even with new language instructions or visual changes. However, success detection\nin unseen real-world videos in the third domain posed a more challenging generalization\ntask, underscoring the need for additional research.\nDu [139] introduced theELLM (exploring with LLMs) framework, which provided\nguidelines for pre-training reinforcement learning using LLMs. ELLM utilized the natural\nlanguage processing capabilities of LLMs to deﬁne goals and furnish reward functions for\nreinforcement learning agents. This strategy enabled agents to undertake meaningful explo-\nration and learning within their environments. The paper assessed ELLM’s performance in\ntwo settings: Crafter, a 2D version of Minecraft, and Housekeep, involving the task of rear-\nranging household objects. Experimental results demonstrated that ELLM surpassed other\nmethods in both settings. In the Crafter setting, ELLM attained high performance through\ngoal-oriented learning, proving especially effective in scenarios with sparse reward signals.\nIn the Housekeep setting, the agent conducted sensible exploration by adhering to goals\nset by the LLM, achieving a high success rate. While the accuracy of goal setting by the\nLLM varied with the objects and locations, it generally showed high performance. These\nexperimental ﬁndings suggested that ELLM was successful in enhancing reinforcement\nlearning performance across diverse environments, highlighting the vital role of providing\nreward signals based on human commonsense.\n4.2. Low-Level Control\nResearch is also being conducted on generating commands that directly control a\nrobot’s actuators (i.e., enabling low-level control) through various applications of LLM\nmodels. Among these projects, the Google research team developed RT-1 [8], which consists\nof ﬁlm-conditioned EfﬁcientNet-B3, TokenLearner, and Transformer. RT-1 is a model that\nreceives images and natural language instructions at a rate of 3Hz and outputs discretized\nrobot actions. RT-1 was trained on a vast demo dataset with over 130k episodes from more"
  },
  {
    "question": "What is the significance of using geometric feasibility in Text2Motion?",
    "chunk": "other novel applications.Appl. Sci.2024, 14, 8868 22 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 22 of 39 \n \nFigure 10. ProgPrompt is a system that uses Pyth on programming structures to provide \nenvironmental information and actions, enhancing the success rate of robot task planning through \nan error recovery feedback mechanism and environmental state feedback [156]. \nRana [157] introduced SayPlan, a scalable method for large-scale task planning using \nLLMs and based on a 3D Scene Graph (3DSG) representation. SayPlan involved the LLM \nsearching a collapsed 3D scene graph and task instructions to identify all relevant items \nand then locating the subgraph that contained the necessary items to complete the task. \nThe identiﬁed subgraph was subsequently used by the LLM to generate a high-level plan \nthat addressed the navigational aspect of the task. This plan was formatted as a JSON 3D \nscene graph and subjected to a repetitive replanning process through feedback from the \nscene graph simulator and a set of API calls for manipulation and operation until an \nexecutable plan was determined. SayPlan wa s tested in two large-scale environments, \nfeaturing up to three ﬂoors, 36 rooms, and 140 assets and objects, proving its capability to \nground large-scale and long-horizon task plans from abstract and natural language \ninstructions, thereby enabling a mobile manipulator robot to execute these tasks. \nZeng [158], as shown in Figure 11, proposed the Socratic model (SM), a modular \nframework that synergistically utilizes various forms of knowledge and employs multiple \npre-trained models to exchange information and leverage new multimodal capabilities. \nSM operates without ﬁne-tuning by integrating diverse pre-trained models and functions \nin a zero-shot approach (e.g., using multimodal prompts), which enables it to harness new \nmultimodal capabilities. SM demonstrated state-of-the-art performance in zero-shot \nimage captioning and video-to-text retrieval, and it e ﬀectively answered free-form \nquestions about egocentric vi deo. Additionally, it supported interactions with external \nAPIs and databases (e.g., web search) for multimodal assistive dialogue, robot perception, \nand planning, among other novel applications. \n \nFigure 11. SM integrates various types of knowledge by using multiple pre-trained models and \nprovides meaningful results even in complex computer vision tasks such as image captioning, \ncontext inference, and activity prediction [158]. \nLin [159] introduced Text2Motion, a language-based framework designed to handle \nsequential manipulation tasks that requ ire long-horizon reasoning. Text2Motion \ninterpreted natural language instructions to formulate task plans and generated multiple \ncandidate skill sequences, evaluating the ge ometric feasibility of each sequence. By \nemploying a greedy search strategy, it selected  the optimal skill sequence to verify and \nexecute the ﬁnal plan. This method enabled Text2Motion to perform complex sequential \nmanipulation tasks with a higher success rate compared to existing language-based \nplanning methods, such as Saycan-gs an d Innermono-gs, and provided semantically \ngeneralized characteristics among skills with geometric relationships. \nFigure 11. SM integrates various types of knowledge by using multiple pre-trained models and\nprovides meaningful results even in complex computer vision tasks such as image captioning, context\ninference, and activity prediction [158].\nLin [159] introduced Text2Motion, a language-based framework designed to handle"
  },
  {
    "question": "What research has been done on the reasoning capabilities of LLMs?",
    "chunk": "reward-labeled trajectories as context and incorporating online interaction, LLM-based\nagents could explore small grids and reﬁne simple trajectories using human-in-the-loop\nmethods, such as optimizing a CartPole controller.Appl. Sci.2024, 14, 8868 17 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 17 of 39 \n \nsequence transformation, the research demonstrated that LLMs could generalize speci ﬁc \nsequence transformations using benchmarks such as ARC (abstraction and reasoning \ncorpus) and PCFG (probabilistic context-free grammar), thereby proving their utility in \nspatial reasoning tasks for robotics. In sequence completion, the study examined whether \nLLMs could ﬁnish pa tterns in elementary functions (e.g., sinusoids), illustrating their \nutility in robotic tasks such as extending a wiping motion from kinesthetic demonstrations \nor creating drawings on a whiteboard. Finally, in sequence improvement, the research \nrevealed that by utilizing reward-labeled trajectories as context and incorporating online \ninteraction, LLM-based agents could explore small grids and re ﬁne simple trajectories \nusing human-in-the-loop methods, such as optimizing a CartPole controller.  \n \nFigure 6. Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed \nin sequence transformation, completion, and improvement [148]. \n4.3. High-Level Planning (Including Decision-Making and Reasoning) \nThe abstraction and generalization capabilities of LLMs oﬀer eﬀective methodologies \nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various \nresearch outcomes have been realized in the ﬁelds of planning, decision-making, \nreasoning, and behavior trees within robotics. \nYoneda [149] introduced Statler, a framew ork designed to provide LLMs with an \nexplicit world state representation through a continuously maintained ‘memory’. The core \nof Statler consisted of two components: the world model reader and the world model \nwriter. These components interacted with and sustained the world state. The world model \nreader interpreted user commands and generated executable code based on the current \nstate representation, while the world model wr iter updated the system’s state according \nto execution outcomes. By facilitating access to the world state ‘memory’, Statler improved \nLLMs’ ability to reason about planning task s with extended time horizons, overcoming \nlimitations imposed by context length. \nMu [150], shown in Figure 7, introduced EmbodiedGPT, a model speci ﬁcally \ndesigned for Embodied AI, which leverages LLMs. This framework processes visual \nobservations and natural language to establish long-term plans and execute tasks in real-\nFigure 6.Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed in\nsequence transformation, completion, and improvement [148].\n4.3. High-Level Planning (Including Decision-Making and Reasoning)\nThe abstraction and generalization capabilities of LLMs offer effective methodologies\nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various\nresearch outcomes have been realized in the ﬁelds of planning, decision-making, reasoning,\nand behavior trees within robotics.\nYoneda [149] introduced Statler, a framework designed to provide LLMs with an\nexplicit world state representation through a continuously maintained ‘memory’. The core\nof Statler consisted of two components: the world model reader and the world model\nwriter. These components interacted with and sustained the world state. The world model"
  },
  {
    "question": "How do language models improve robot intelligence?",
    "chunk": "Citation: Jeong, H.; Lee, H.; Kim, C.;\nShin, S. A Survey of Robot\nIntelligence with Large Language\nModels. Appl. Sci.2024, 14, 8868.\nhttps://doi.org/10.3390/app14198868\nAcademic Editors: Luis Gracia and J.\nErnesto Solanes\nReceived: 6 September 2024\nRevised: 24 September 2024\nAccepted: 25 September 2024\nPublished: 2 October 2024\nCopyright: © 2024 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\napplied  sciences\nReview\nA Survey of Robot Intelligence with Large Language Models\nHyeongyo Jeong1,† , Haechan Lee1,† , Changwon Kim2,* and Sungtae Shin1,*\n1 Department of Mechanical Engineering, Dong-A University, Busan 49315, Republic of Korea\n2 School of Mechanical Engineering, Pukyong National University, Busan 48513, Republic of Korea\n* Correspondence: ckim@pknu.ac.kr (C.K.); stshin@dau.ac.kr (S.S.)\n† These authors contributed equally to this work.\nAbstract: Since the emergence of ChatGPT, research on large language models (LLMs) has actively\nprogressed across various ﬁelds. LLMs, pre-trained on vast text datasets, have exhibited exceptional\nabilities in understanding natural language and planning tasks. These abilities of LLMs are promis-\ning in robotics. In general, traditional supervised learning-based robot intelligence systems have a\nsigniﬁcant lack of adaptability to dynamically changing environments. However, LLMs help a robot\nintelligence system to improve its generalization ability in dynamic and complex real-world environ-\nments. Indeed, ﬁndings from ongoing robotics studies indicate that LLMs can signiﬁcantly improve\nrobots’ behavior planning and execution capabilities. Additionally, vision-language models (VLMs),\ntrained on extensive visual and linguistic data for the vision question answering (VQA) problem,\nexcel at integrating computer vision with natural language processing. VLMs can comprehend visual\ncontexts and execute actions through natural language. They also provide descriptions of scenes\nin natural language. Several studies have explored the enhancement of robot intelligence using\nmultimodal data, including object recognition and description by VLMs, along with the execution\nof language-driven commands integrated with visual information. This review paper thoroughly\ninvestigates how foundation models such as LLMs and VLMs have been employed to boost robot\nintelligence. For clarity, the research areas are categorized into ﬁve topics: reward design in rein-\nforcement learning, low-level control, high-level planning, manipulation, and scene understanding.\nThis review also summarizes studies that show how foundation models, such as the Eureka model\nfor automating reward function design in reinforcement learning, RT-2 for integrating visual data,\nlanguage, and robot actions in vision-language-action models, and AutoRT for generating feasible\ntasks and executing robot behavior policies via LLMs, have improved robot intelligence.\nKeywords: embodied intelligence; foundation model; large language model (LLM); vision-language\nmodel (VLM); vision-language-action (VLA) model; robotics\n1. Introduction\nTo enhance the intelligence of robots in real-world environments that interact with\nhumans, developing robots capable of perceiving, acting, and interacting like humans is"
  },
  {
    "question": "What kind of information do robotic systems need to process for effective functioning?",
    "chunk": "and multiple months of training. Developing SLMs to excel within speciﬁc domains for\nrobotic systems and ensuring real-time performance with minimal computational resources\nare essential research directions for advancing robot intelligence with SLMs.\nThe third implication is that LLMs, based on text-centered natural language process-\ning, are limited as single-modality models when applied to real-world robotic systems\nwhere information often blends in diverse ways. Research on LLMs is transitioning from\nsingle-modality to multimodality models, as evidenced by VLMs and OpenAI Sora [228],\nwith increasing demand for such models. Currently, to address the limitations of LLMs’\nsingle-modality, robotic systems are being developed with multimodality models that\nintegrate vision, such as VLMs. However, relying solely on text and images falls short\nof the diverse information range required in the real world, including images, sounds,\nvideos, and proprioceptive sensory information (such as the position, orientation, balance,\nmovement degree, and direction of various parts of the robot). Proprioceptive sensory\ninformation related to actions and movements is particularly vital for enhancing dynamic\nhuman interaction, information processing, and manipulation and planning skills based on\ndynamic movements. For instance, the integrated VLA model, which facilitates low-level\ncontrol based on LLMs and VLMs as shown by Google’s RT-2 model, highlights the neces-\nsity for models capable of integrating information from a broader range of modalities to\nenhance robot intelligence.Appl. Sci.2024, 14, 8868 29 of 39\nFinally, the fourth area to consider is how to address safety and ethical issues when\nLLM is applied to robotic intelligence systems. Studies were conducted to address the\nissue of discriminatory and unsafe behaviors that may be generated by robot applications\npowered by LLMs [229]. The outputs of LLMs have the potential to generate content\nthat is biased based on personal characteristics (such as race, nationality, religion, gender,\ndisability, and so forth). In addition, they can also be used to instruct robotic systems to\nengage in violent or illegal behaviors such as misstatements, sexual predation, etc. Notable\nexamples include discriminatory behaviors such as inadequate recognition of children\nor individuals with speciﬁc skin tones in human detection systems, and the exclusion\nof individuals with disabilities from task assignments. It is imperative to consider the\npotential social biases of LLM when integrating with robotic systems. Although this kind of\nconsideration was secondary in traditional robotic systems because of the limitation of their\nlanguage capability, it is a necessary consideration for LLMs to be able to generate human-\nlike language. To address this issue, previous studies have attempted to resolve it in various\nways, such as AutoRT’s constitutional rules [10], DrEureka’s safety instructions [134], and\nNeMo’s guardrails [230]. The guideline-based output control of LLMs can represent an\naccessible method to ensure safety.\nAs an extension of this point, safety issues can be identiﬁed when integrating LLMs\nand VLMs into robotic intelligence systems [231]. Typically, in robotic intelligence systems,\nLLM models generate high-level action plans in various forms, such as programming\ncodes and behavior trees based on natural language or vector prompts. At this point, a\nprompt attack has the potential to disrupt the inference of the LLMs, thereby threatening\nthe reliability and safety of the robotic system. Prompt injection is one of the prompt attacks,\nwhereby the inference of LLMs is subtly altered through speciﬁc inputs. Jailbreak, another\nprompt attack, bypasses safety rules and causes LLMs to generate abnormal behaviors to"
  },
  {
    "question": "How does GPT-3 differ from GPT-2 in terms of capabilities?",
    "chunk": "(RNNs) to model the probability of word sequences [24–26]. A key element of this stage\nis the development of word vectors, also known as word embeddings, which form word\nprediction models based on vectors that use a distributed representation of words [24,27].\nWord2vec, a simpliﬁed shallow neural network approach, was introduced to learn these\ndistributed word representations [28,29]. It proved highly effective across various NLP\ntasks by calculating meaningful similarities between word vectors. NLMs progressed\nfrom basic word sequence modeling to sophisticated techniques for representing language\nthrough word2vec.\nFollowing the NLM phase, the ﬁeld advanced to pre-trained language models (PLMs),\nwhich encompass models such as ELMO [30] and BERT [31]. PLMs, utilizing large-scale\ntext data, learn text patterns, structures, and meanings to develop pre-trained context-\nsensitive word representations. They have successfully executed a variety of language\nunderstanding and generation tasks using this acquired knowledge. ELMo [30] introduced\na pre-training method employing bidirectional LSTM (biLSTM) networks for modeling\ndeep contextualized word representations, optimizing performance through speciﬁc ﬁne-\ntuning of the trained biLSTM network for downstream tasks. ELMo is also characterized\nas a bidirectional language model for its dual-directional use of language models.\nAnother PLM model, BERT [31], leverages the transformer architecture [32], exhibiting\nremarkable effectiveness with self-attention mechanisms and parallel processing. BERT,\na pre-trained bidirectional language model, utilizes extensive unlabeled text data. The\nmethod of unsupervised learning-based pre-training in BERT comprises two primary tasks:\nmasked language models and next sentence prediction. PLMs that provide pre-trained\ncontext-aware word representations are profoundly effective in general-purpose semantic\nfeature extraction, facilitating enhancements in NLP task performance. Owing to these\ncharacteristics, numerous subsequent studies employing pre-training and ﬁne-tuning have\nbeen introduced, featuring varied structures [33,34] (e.g., BART [33] and GPT-2 [35]) and\nenhancing pre-training strategies [36–38].\nBased on subsequent studies, it has been found that increasing the model size or data\nsize of PLMs typically enhances the performance of LM models [39]. This has prompted\nresearch into training large-scale PLMs, such as GPT-3 with 175B parameters and PaLM\nwith 540B parameters. The focus of this research, grounded in scaling laws, primarily\ncenters on augmenting model sizes and exploring the capabilities of larger models. These\ncapabilities, known as the emergent abilities of LLMs, have sparked signiﬁcant interest. For\nexample, GPT-3 can address problems it has not been trained on with minimal examples\nthrough in-context learning, a feat GPT-2 ﬁnds challenging. Due to these characteristics,\nthe academic community commonly designates these large PLMs as LLMs [40–43]. Conse-\nquently, research in this area is highly active. Notably, since the introduction of OpenAI’s\nChatGPT, there has been a surge in the number of arXiv papers on LLMs. Following\nMicrosoft’s announcement [2] about integrating ChatGPT into robotics, a variety of studies\nhave explored the application of LLMs across different areas of robotics research. The\navailable LLM models are presented in chronological order in Table2. Additionally, Table3\nincludes the VLM models.Appl. Sci.2024, 14, 8868 7 of 39\nTable 2.Chronicle of LLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref."
  },
  {
    "question": "Can you describe the closed-loop planning approach used in robot trajectories?",
    "chunk": "including 6-DoF end-eﬀector waypoints, for various manipulation tasks using an open set \nof instructions and objects. Huang note d that LLMs were skilled at deriving a ﬀordances \nand constraints from free-form language in structions. Further, by harnessing code \ngeneration capabilities, Huang developed 3D value maps for the agent’s observation \nspace through interactions with VLMs. Thes e 3D value maps were integrated into a \nmodel-based planning framework to generate closed-loop robot trajectories robust to \ndynamic perturbations in a zero-shot approach. The proposed framework demonstrated \neﬃcient learning of the dynamics model for sc enes with contact-rich interactions and \nprovided advantages in these complex scenarios. \n \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM to \ngenerate 3D a ﬀordance and constraint maps and design robot trajectories without additional \ntraining [165]. \nAhn [166] introduced a framework named SayCan, which integrates LLMs with \nreinforcement learning value functions, enabling robots to follow high-level text \ninstructions. SayCan comprises two primary components: Say, which uses an LLM for \ntask-based decision-making, and Can, which evaluates the feasibility of these decisions \nvia reinforcement learning. Say leverages task-based knowledge from the LLM and \nreinforcement learning functionality to assess the feasibility of task execution by robots in \nreal-world scenarios. The LLM determines th e actions necessary to achieve high-level \ngoals and evaluates the eﬀectiveness of each action in fulﬁlling the instructions. Learned \nthrough reinforcement learning, the a ﬀordance function estimate s each action’s success \nprobability in the current state, con ﬁrming the executability of actions proposed by the \nLLM. This process allows the LLM to assess the robot’s current state and capabilities, \nultimately generating an interpretable action plan. SayCan was evaluated across 101 robot \ntasks, achieving an 84% plan success rate and a 74% execution success rate in a simulated \nkitchen environment. In a real kitchen setting, the plan success rate decreased slightly to \n81% and the execution success rate fell to 60%, demonstrating that the policy and value \nfunctions generalize well to real-world settings. \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM\nto generate 3D affordance and constraint maps and design robot trajectories without additional\ntraining [165].\nAhn [166] introduced a framework named SayCan, which integrates LLMs with rein-\nforcement learning value functions, enabling robots to follow high-level text instructions.\nSayCan comprises two primary components: Say, which uses an LLM for task-based\ndecision-making, and Can, which evaluates the feasibility of these decisions via reinforce-\nment learning. Say leverages task-based knowledge from the LLM and reinforcement\nlearning functionality to assess the feasibility of task execution by robots in real-world\nscenarios. The LLM determines the actions necessary to achieve high-level goals and\nevaluates the effectiveness of each action in fulﬁlling the instructions. Learned through re-\ninforcement learning, the affordance function estimates each action’s success probability in\nthe current state, conﬁrming the executability of actions proposed by the LLM. This process\nallows the LLM to assess the robot’s current state and capabilities, ultimately generating\nan interpretable action plan. SayCan was evaluated across 101 robot tasks, achieving an\n84% plan success rate and a 74% execution success rate in a simulated kitchen environment."
  },
  {
    "question": "What are the benefits of using multimodal data in robotics?",
    "chunk": "Citation: Jeong, H.; Lee, H.; Kim, C.;\nShin, S. A Survey of Robot\nIntelligence with Large Language\nModels. Appl. Sci.2024, 14, 8868.\nhttps://doi.org/10.3390/app14198868\nAcademic Editors: Luis Gracia and J.\nErnesto Solanes\nReceived: 6 September 2024\nRevised: 24 September 2024\nAccepted: 25 September 2024\nPublished: 2 October 2024\nCopyright: © 2024 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\napplied  sciences\nReview\nA Survey of Robot Intelligence with Large Language Models\nHyeongyo Jeong1,† , Haechan Lee1,† , Changwon Kim2,* and Sungtae Shin1,*\n1 Department of Mechanical Engineering, Dong-A University, Busan 49315, Republic of Korea\n2 School of Mechanical Engineering, Pukyong National University, Busan 48513, Republic of Korea\n* Correspondence: ckim@pknu.ac.kr (C.K.); stshin@dau.ac.kr (S.S.)\n† These authors contributed equally to this work.\nAbstract: Since the emergence of ChatGPT, research on large language models (LLMs) has actively\nprogressed across various ﬁelds. LLMs, pre-trained on vast text datasets, have exhibited exceptional\nabilities in understanding natural language and planning tasks. These abilities of LLMs are promis-\ning in robotics. In general, traditional supervised learning-based robot intelligence systems have a\nsigniﬁcant lack of adaptability to dynamically changing environments. However, LLMs help a robot\nintelligence system to improve its generalization ability in dynamic and complex real-world environ-\nments. Indeed, ﬁndings from ongoing robotics studies indicate that LLMs can signiﬁcantly improve\nrobots’ behavior planning and execution capabilities. Additionally, vision-language models (VLMs),\ntrained on extensive visual and linguistic data for the vision question answering (VQA) problem,\nexcel at integrating computer vision with natural language processing. VLMs can comprehend visual\ncontexts and execute actions through natural language. They also provide descriptions of scenes\nin natural language. Several studies have explored the enhancement of robot intelligence using\nmultimodal data, including object recognition and description by VLMs, along with the execution\nof language-driven commands integrated with visual information. This review paper thoroughly\ninvestigates how foundation models such as LLMs and VLMs have been employed to boost robot\nintelligence. For clarity, the research areas are categorized into ﬁve topics: reward design in rein-\nforcement learning, low-level control, high-level planning, manipulation, and scene understanding.\nThis review also summarizes studies that show how foundation models, such as the Eureka model\nfor automating reward function design in reinforcement learning, RT-2 for integrating visual data,\nlanguage, and robot actions in vision-language-action models, and AutoRT for generating feasible\ntasks and executing robot behavior policies via LLMs, have improved robot intelligence.\nKeywords: embodied intelligence; foundation model; large language model (LLM); vision-language\nmodel (VLM); vision-language-action (VLA) model; robotics\n1. Introduction\nTo enhance the intelligence of robots in real-world environments that interact with\nhumans, developing robots capable of perceiving, acting, and interacting like humans is"
  },
  {
    "question": "What makes VLMs like CLIP important for robotic systems?",
    "chunk": "of labeled data. This process is inherently resource-intensive. Moreover, these models\nare designed for a speciﬁc environment and require reconﬁguration whenever the task or\nenvironment changes. This renders the robots challenging to adapt and scale to disparate\nenvironments [3]. For practical robot systems, it is essential that they are able to ﬂexibly\nrespond to the ever-changing physical environment. From this perspective, the gener-\nalization of affordable tasks, environmental adaptability, and the accuracy of execution,\nplanning, and reasoning capabilities remain signiﬁcant challenges for traditional robotic\nintelligence systems [4].\nHowever, LLMs and VLMs help a robot intelligence system to enhance its general-\nization capability in dynamic and complex real-world environments. LLMs can leverage\npre-trained knowledge from extensive datasets to augment their ability to generalize to\neveryday tasks that are typically expected of robots. Unlike the conventional supervised\nmodels, LLMs can utilize zero-shot and few-shot learning to help robots quickly adapt\nto new environments without additional training [5]. This has the advantage of signiﬁ-\ncantly reducing the need for costly data collection and labeling. In addition, robot systems\nequipped with LLMs can process complex instructions based on their ability to understand\nand generate natural language, which can improve human–robot interactions. Furthermore,\nLLMs can be integrated with multimodal sensors such as LiDAR, depth, voice, tactile, pro-\nprioception, and visual information, which enables robots to comprehensively understand\nand adapt to their environment [6].\nLLMs have demonstrated exceptional capabilities in processing and understanding\ntext-based information, signiﬁcantly enhancing robotic communication abilities. For in-\nstance, robots can accurately comprehend and execute natural language commands via\nLLMs, providing scalability and ﬂexibility beyond traditional word-based robotic com-\nmand systems. Consequently, robots can respond more adaptably and intelligently in\ninteractions with human users, allowing them to engage in complex problem-solving and\ndecision-making processes beyond simple mechanical tasks.\nAdditionally, LLMs not only enhance a robot’s communication skills to improve HRI\nusability but also boost the robot’s planning abilities. Planning involves setting goals\nand devising a sequence of actions to achieve them, which are essential in determining a\nrobot’s autonomy and efﬁciency. LLMs interpret natural language from users and complex\ncommands, enabling robots to establish and execute suitable plans in various situations.\nMoreover, LLMs adapt ﬂexibly to new situations through a zero-shot approach and utilize\npast data for learning. These capabilities indicate that robots can play a vital role in\nautonomously navigating changing environments and resolving unexpected issues.\nMoreover, VLMs such as CLIP [7], which are trained to solve vision question answering\n(VQA) tasks, have the ability to process visual and linguistic information simultaneously.\nThis ability allows robots to visually perceive their surroundings and integrate this infor-\nmation into linguistic descriptions, enabling more sophisticated situational awareness. For\ninstance, using VLMs, a robot can recognize objects and provide descriptions, as well as\nunderstand and execute user commands based on visual cues. This integrated approach\nsigniﬁcantly enhances a robot’s autonomy and interaction capabilities.\nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which en-\nable low-level actuator control using LLMs and VLMs, Google has introduced AutoRT [10].\nAutoRT is a system where robots interact with real-world objects to collect motion data. ItAppl. Sci.2024, 14, 8868 3 of 39"
  },
  {
    "question": "What technologies are involved in the development of low-level control for robots?",
    "chunk": "agents’ understanding of human intentions. They developed a framework that utilized\nlanguage as a primary inference tool, investigating how it could address key challengesAppl. Sci.2024, 14, 8868 14 of 39\nin reinforcement learning, such as efﬁcient exploration, data reuse in experience, skill\nscheduling, and observational learning. This framework employed LLMs and VLMs to\naddress these reinforcement learning challenges by (1) efﬁciently exploring environments\nwith sparse rewards, (2) reusing collected data to sequentially bootstrap the learning of\nnew tasks, (3) scheduling learned skills for novel tasks, and (4) acquiring knowledge from\nobserving expert agents.\nDu [138] developed success detectors that identiﬁed whether actions or tasks were\nsuccessfully completed, utilizing the large multimodal language model Flamingo and\nhuman reward annotations. The study on success detection spanned three distinct do-\nmains: (1) interactive language-conditioned agents in simulated households, (2) real-world\nrobotic manipulation tasks (inserting and removing small, medium, and large gears), and\n(3) “in-the-wild” human egocentric videos. These success detectors adapted to new lan-\nguage instructions and visual changes using VLMs such as Flamingo, which were trained\non a broad range of language and visual data. Furthermore, success detection was reframed\nas a VQA problem, enabling the tracking of task progress through multiple frames to ascer-\ntain whether tasks had been successfully completed. The proposed method proved to be\nmore accurate in detecting success compared to custom reward models in the ﬁrst two do-\nmains, even with new language instructions or visual changes. However, success detection\nin unseen real-world videos in the third domain posed a more challenging generalization\ntask, underscoring the need for additional research.\nDu [139] introduced theELLM (exploring with LLMs) framework, which provided\nguidelines for pre-training reinforcement learning using LLMs. ELLM utilized the natural\nlanguage processing capabilities of LLMs to deﬁne goals and furnish reward functions for\nreinforcement learning agents. This strategy enabled agents to undertake meaningful explo-\nration and learning within their environments. The paper assessed ELLM’s performance in\ntwo settings: Crafter, a 2D version of Minecraft, and Housekeep, involving the task of rear-\nranging household objects. Experimental results demonstrated that ELLM surpassed other\nmethods in both settings. In the Crafter setting, ELLM attained high performance through\ngoal-oriented learning, proving especially effective in scenarios with sparse reward signals.\nIn the Housekeep setting, the agent conducted sensible exploration by adhering to goals\nset by the LLM, achieving a high success rate. While the accuracy of goal setting by the\nLLM varied with the objects and locations, it generally showed high performance. These\nexperimental ﬁndings suggested that ELLM was successful in enhancing reinforcement\nlearning performance across diverse environments, highlighting the vital role of providing\nreward signals based on human commonsense.\n4.2. Low-Level Control\nResearch is also being conducted on generating commands that directly control a\nrobot’s actuators (i.e., enabling low-level control) through various applications of LLM\nmodels. Among these projects, the Google research team developed RT-1 [8], which consists\nof ﬁlm-conditioned EfﬁcientNet-B3, TokenLearner, and Transformer. RT-1 is a model that\nreceives images and natural language instructions at a rate of 3Hz and outputs discretized\nrobot actions. RT-1 was trained on a vast demo dataset with over 130k episodes from more"
  },
  {
    "question": "How do large language models enhance robot intelligence?",
    "chunk": "signiﬁcant lack of adaptability to dynamically changing environments. However, LLMs help a robot\nintelligence system to improve its generalization ability in dynamic and complex real-world environ-\nments. Indeed, ﬁndings from ongoing robotics studies indicate that LLMs can signiﬁcantly improve\nrobots’ behavior planning and execution capabilities. Additionally, vision-language models (VLMs),\ntrained on extensive visual and linguistic data for the vision question answering (VQA) problem,\nexcel at integrating computer vision with natural language processing. VLMs can comprehend visual\ncontexts and execute actions through natural language. They also provide descriptions of scenes\nin natural language. Several studies have explored the enhancement of robot intelligence using\nmultimodal data, including object recognition and description by VLMs, along with the execution\nof language-driven commands integrated with visual information. This review paper thoroughly\ninvestigates how foundation models such as LLMs and VLMs have been employed to boost robot\nintelligence. For clarity, the research areas are categorized into ﬁve topics: reward design in rein-\nforcement learning, low-level control, high-level planning, manipulation, and scene understanding.\nThis review also summarizes studies that show how foundation models, such as the Eureka model\nfor automating reward function design in reinforcement learning, RT-2 for integrating visual data,\nlanguage, and robot actions in vision-language-action models, and AutoRT for generating feasible\ntasks and executing robot behavior policies via LLMs, have improved robot intelligence.\nKeywords: embodied intelligence; foundation model; large language model (LLM); vision-language\nmodel (VLM); vision-language-action (VLA) model; robotics\n1. Introduction\nTo enhance the intelligence of robots in real-world environments that interact with\nhumans, developing robots capable of perceiving, acting, and interacting like humans is\na crucial goal. The recent advancements in large language models (LLMs) such as GPT-\n4o [1] have signiﬁcantly altered the ﬁeld of robotic AI research. These LLMs, trained on\nvast amounts of textual data, have shown excellent performance in enabling robots to\ncommunicate with humans more naturally and efﬁciently. Moreover, beyond the impacts\non human–robot interaction (HRI), there is ongoing research aimed at surpassing the\nlimitations of traditional low-level robot control techniques and planning algorithms by\nutilizing the high-level situational awareness and knowledge-based planning capabilities\nof LLMs. Notably, the programming capabilities of ChatGPT in the research presented by\nMicrosoft’s ChatGPT for Robotics [2] have introduced a new paradigm for applying LLMs\nin the robotics ﬁeld.\nThe goal of robot intelligence is to enable robots to operate autonomously in complex\nenvironments, interact naturally with humans, and make high-level decisions. To promote\nAppl. Sci.2024, 14, 8868.https://doi.org/10.3390/app14198868 https://www.mdpi.com/journal/applsciAppl. Sci.2024, 14, 8868 2 of 39\nadvancements in robot intelligence, the adoption of foundation models, such as LLMs and\nvision-language models (VLMs), which boast large parameter scales and pre-training on\nmassive datasets, is accelerating. These foundation models can perform various tasks, such\nas complex language understanding and generation and visual perception, enabling robots\nto engage with their environment in a more human-like manner.\nWhile traditional robot intelligence systems are highly effective in structured and pre-\ndictable environments, they are signiﬁcantly limited in their ability to adapt to dynamically\nchanging and complex real-world scenarios. In general, the intelligence models used in"
  },
  {
    "question": "How do LLMs assess action feasibility in robotic systems?",
    "chunk": "Zhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow\ninstructions. NavGPT is a vision-language navigation system that employs an LLM to trans-\nlate visual inputs from a visual foundation model (VFM) into natural language. The LLM\nthen interprets the current state and makes informed decisions to reach the intended goal,\nbased on these converted visuals, navigation history, and potential future routes. NavGPT\nconducts various functions, including high-level planning, decomposing instructions into\nsub-goals, identifying landmarks in observed scenes, monitoring navigation progress, and\nmodifying plans as necessary. Although NavGPT’s performance on zero-shot tasks from\nthe R2R dataset has not yet matched that of trained models, it underscored the potential\nof utilizing multi-modality inputs with LLMs for visual navigation and tapping into the\nexplicit reasoning capabilities of LLMs to enhance learned models.\nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-\ntrained vision-language features with a 3D reconstruction of the physical world. VLMaps,\nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary\nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands.\nThese commands can be directly localized on a map and generate new obstacle maps in real-Appl. Sci.2024, 14, 8868 27 of 39\ntime, facilitated by sharing among various robot types. Extensive experiments conducted in\nboth simulated environments (using the Habitat simulator with the Matterport3D dataset\nand the AI2THOR simulator) and real-world settings (with the HSR mobile robot for\nindoor navigation) demonstrated that VLMs can navigate based on more complex language\ninstructions than previous methods. The reviewed papers in this study are summarized in\nTable 5.\nTable 5.Summary of the reviewed papers in this study.\nName Explanation Ref.\nReward Design in RL\n• Eureka automatically generates and improves reward\nfunctions based on the virtual environment source\ncode.\n• DrEureka builds reward-aware physics priors using\nEureka and supports effective operation in the real\nworld through domain randomization.\n• LLMs design and reﬁne reward functions based on\nnatural language input.\n• LLMs and VLMs integrate multimodal data to\ngenerate reward functions.\n[11,134,136–139,176–180]\nLow-level\nControl\n• Generating commands to control actuators capable of\nlow-level control.\n• RT-1 and RT-2 enable robots to perform complex tasks\nbased on language-vision data.\n• AutoRT establishes a system where robots can\nautonomously collect and utilize data.\n[8–10,144–148,181–183]\nHigh-level\nPlanning\n• LLMs provide an effective methodology for tasks\nrelated to high-level planning within robotic systems.\n• By using natural language, LLMs can formulate plans\nto solve tasks that require long-horizon reasoning.\n• LLMs assess the feasibility of actions to determine and\nexecute the optimal robotic behavior.\n• LLMs generate behavior trees to structure complex\nrobotic actions accurately.\n[149–160,184–207]\nManipulation\n• Using LLMs and VLMs to integrate language and\nvision data allows various manipulations.\n• LLMs interpret high-level instructions to generate the\nnecessary robot actions and assess their feasibility.\n• VLMs extract object information from images to assist\nin performing manipulations.\n[161–167,208–215]\nScene\nUnderstanding\n• To solve VQA problems, use VLMs to extract\nhigh-level information from vision data."
  },
  {
    "question": "Why is proprioceptive sensory information important for robot interactions?",
    "chunk": "While foundation models offer considerable potential for advancing robotics intelli-\ngence, several limitations and future considerations remain. These include (1) the speed\nof inference required for real-time applications, (2) the computational efﬁciency necessary\nfor embedded systems, (3) the ability to handle multi-modality information, and (4) the\nnecessity of addressing safety and ethical considerations.\nFirst of all, LLMs and VLMs hold considerable potential for enhancing robot intelli-\ngence. Nonetheless, several critical issues remain to be addressed. Foundation models,\ncharacterized as large-scale models pre-trained on extensive datasets, face challenges re-\nlated to real-time requirements and limited computational resources in robotic applications.\nMoreover, concerns such as personal information protection, privacy, and security from\nexternal attacks need resolution to enable cloud-based LLMs for robotics.\nSecondly, to enhance the computational efﬁciency and usability of LMs, there is\nongoing research into small language models (SLMs). Despite having fewer parameters,\nSLMs can achieve performance comparable to LLMs in speciﬁc applications. Several\nSLMs have been introduced, including DistilBERT [224], which is a compact version of\nGoogle’s BERT; Phi-3 [61], another SLM; Florence-2 [84], a small VLM model from Microsoft;\nMobileBERT [225], which is optimized for mobile platforms; and compact open-source\nversions of OpenAI’s GPT models such as GPT-Neo [226] and GPT-J [227]. Generally, SLMs\nare streamlined models with fewer parameters compared to LLMs, which can number in the\nbillions. SLMs utilize smaller, domain-speciﬁc datasets and require shorter training periods,\ntypically just a few weeks, unlike LLMs, which demand vast datasets for broad learning\nand multiple months of training. Developing SLMs to excel within speciﬁc domains for\nrobotic systems and ensuring real-time performance with minimal computational resources\nare essential research directions for advancing robot intelligence with SLMs.\nThe third implication is that LLMs, based on text-centered natural language process-\ning, are limited as single-modality models when applied to real-world robotic systems\nwhere information often blends in diverse ways. Research on LLMs is transitioning from\nsingle-modality to multimodality models, as evidenced by VLMs and OpenAI Sora [228],\nwith increasing demand for such models. Currently, to address the limitations of LLMs’\nsingle-modality, robotic systems are being developed with multimodality models that\nintegrate vision, such as VLMs. However, relying solely on text and images falls short\nof the diverse information range required in the real world, including images, sounds,\nvideos, and proprioceptive sensory information (such as the position, orientation, balance,\nmovement degree, and direction of various parts of the robot). Proprioceptive sensory\ninformation related to actions and movements is particularly vital for enhancing dynamic\nhuman interaction, information processing, and manipulation and planning skills based on\ndynamic movements. For instance, the integrated VLA model, which facilitates low-level\ncontrol based on LLMs and VLMs as shown by Google’s RT-2 model, highlights the neces-\nsity for models capable of integrating information from a broader range of modalities to\nenhance robot intelligence.Appl. Sci.2024, 14, 8868 29 of 39\nFinally, the fourth area to consider is how to address safety and ethical issues when\nLLM is applied to robotic intelligence systems. Studies were conducted to address the\nissue of discriminatory and unsafe behaviors that may be generated by robot applications"
  },
  {
    "question": "How does automated reward design compare to manual reward engineering?",
    "chunk": "4. Language Models for Robotic Intelligence\n4.1. Reward Design in Reinforcement Learning\nResearch in reinforcement learning, closely associated with the ﬁeld of robotics, has\nactively incorporated studies using LLM models. Speciﬁcally, Nvidia has developed a\nGPU-based multi-environment reinforcement learning platform. Utilizing its Omniverse\n3D virtual environment platform, Nvidia created Isaac Sim, which is dedicated to robot\nsimulation. Isaac Sim published research ﬁndings on Isaac Gym (Preview), which achieved\nsigniﬁcant reductions in reinforcement learning training times through GPU-based multi-\nenvironment approaches. Subsequently, Isaac Gym (Preview)’s features were integrated\ninto Isaac Sim and released as Omni Isaac Gym. Later, Nvidia introduced Orbit [133],\nfacilitating the simulation of PhysX 5.1-based cloth, soft-body, ﬂuid, and rigid-body dy-\nnamics, along with RGBD, LiDAR, and contact sensor simulation. Orbit also incorporates\nvarious robot platforms into the simulation environment. Recently, Orbit was updated\nto Isaac Lab and integrated into Isaac Sim 4.0. Nvidia has continuously advanced dy-\nnamic simulation environment technologies for reinforcement learning using GPU parallel\ncomputation. Leveraging this GPU reinforcement learning, they launched Eureka [11],\nwhich automates the design of reward functions for reinforcement learning using LLMs.\nFollowing this, Nvidia introduced DrEureka [134], an automated platform addressing the\nSim2Real problem [135] in reinforcement learning based on Eureka.\nEureka (Evolution-driven Universal REward Kit for Agent) [11], shown in Figure4,\nautomatically generates reward functions for various tasks using different robots, elimi-\nnating the need for speciﬁc templates or tailored reward functions for the robot’s form or\nexplanations for reinforcement learning tasks. Eureka consists of three main components:\nenvironment-as-context, evolutionary search, andreward reﬂection. Environment-as-context\ngenerates executable reward functions in a zero-shot manner by utilizing virtual environ-\nment source (Python) code as context. Evolutionary search iteratively generates reward\nfunction candidates and proposes enhanced functions based on previously generated and\nbest-performing ones, while also creating new functions through mutation. Reward reﬂec-\ntion offers a text summary of reward function quality based on training statistics recorded\nduring reinforcement learning, which assists in generating subsequent reward functions\nas feedback for the performance of previous functions. The reward functions generated\noutperformed expert-generated functions in 83% of benchmark tests. Moreover, Eureka\nsolved the pen spinning problem where a robot hand must spin a pen as much as possi-\nble according to predeﬁned rotations, a task previously considered unsolvable through\nmanual reward engineering. Eureka introduces a universal reward function design algo-\nrithm based on a code LLM and in-context evolutionary search, facilitating human-level\nreward generation for various robots and tasks without the need for prompt engineering or\nhuman intervention.\nFollowing Eureka, DrEureka [134], shown in Figure5, was developed to address\nthe sim-to-real problem by automatically conﬁguring appropriate reward functions and\ndomain randomization for physical environments. DrEureka’s reward-aware physics pri-\nors mechanism deﬁnes the lower and upper bounds of physical environment parameters\nbased on policies trained through initial reinforcement learning, facilitating reinforcement\nlearning across various physical environment domains. This randomization enables the\ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation"
  },
  {
    "question": "What are the advantages of step-by-step planning in robot tasks?",
    "chunk": "paper details experiments in a simulated tabletop rearrangement, a mini-grid 2D maze,\nand real-world kitchen mobile manipulation settings to evaluate long-horizon reasoning\nperformance. Comparative experiments with SayCan revealed that while SayCan limits\nthe range of robot actions, GD can represent a wider array of actions. In contrast to\nCLIPort, which executes high-level language instructions directly, GD achieves enhanced\nperformance through detailed, step-by-step planning.\nHuang [153], as shown in Figure8, proposed the inner monologue method, which\nallowed LLMs to plan and adjust based on feedback from the environment. This approach\nenabled robots to formulate plans in dynamic environments, retry upon facing failure,\nor seek human feedback to reﬁne their strategies. The author clariﬁed that this method\nemerged from integrating the LLM’s high-level planning capabilities with perceptual feed-\nback and low-level control, thereby facilitating more adaptable and intelligent interactions.\nInner monologue integrated various feedback sources into the language model to assist the\nrobot in executing given instructions, including text-based indicators of the robot’s action\nsuccess or failure, object recognition and descriptions within the scene, the robot’s ability to\nask questions to gather additional information, breaking down instructions into multiple\nsteps to establish an execution plan, and enabling the robot to interact with humans to\nexecute and reﬁne the instructions. The inner monologue method was evaluated in both\nsimulated and real-world environments, such as tabletop rearrangement tasks and manip-\nulation tasks in a real kitchen. The results showed that inner monologue was an effective\nframework, enabling robots to act intelligently in complex interactive settings by effectively\nintegrating environmental feedback to plan and execute tasks.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 20 of 39 \n \n \nFigure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable"
  },
  {
    "question": "How does LLM-Grounder break down natural language queries?",
    "chunk": "In a real kitchen setting, the plan success rate decreased slightly to 81% and the execution\nsuccess rate fell to 60%, demonstrating that the policy and value functions generalize well\nto real-world settings.\nHuang [167] introduced the Instruct2Act framework, which employs LLMs to se-\nquentially map multi-modality instructions to robot actions. The previous method, CaP,\ngenerated robot policy program code directly from in-context examples based on language\ninstructions. However, this approach was constrained by the capabilities of the generated\ncode and encountered difﬁculties with longer, more complex commands due to the required\nhigh precision of code. To overcome these limitations, Instruct2Act introduced a novel\nstrategy that used multi-modality models and LLMs to simultaneously address recognition,\ntask planning, and low-level control modules. Instruct2Act utilized the segment anything\nmodel for identifying potential objects in input images for multi-modality recognition\nand the CLIP model for object classiﬁcation. As a result, Instruct2Act developed an inte-\ngrated search system capable of managing various input modalities and instruction types,\nincluding both pure language instructions and combined language-visual instructions,\nfacilitating the integration of diverse instruction types into a uniﬁed architecture. Moreover,\nfor pointer-language instructions, the framework supported task segmentation based on\nthe user’s clicks.\n4.5. Scene Understanding in LLMs and VLMs\nTo address the VQA problem, robotics research increasingly uses pre-trained VLMs\nto derive high-level information from visual data. This method is advantageous for scene\nunderstanding as it helps determine affordances that describe the relationship between the\ncurrent state and the next action based on images from cameras. Related studies focus on\naspects of scene understanding.Appl. Sci.2024, 14, 8868 25 of 39\nChen [168] explored methods to integrate commonsense into scene understanding\nusing LLMs and introduced three paradigms for classifying room types within indoor\nenvironments based on included objects. The zero-shot approach utilized a pre-trained\nlanguage model to identify the objects in a room and estimate their types. The feed-\nforward classiﬁer approach involved inputting sentences that listed a room’s objects into\nthe language model to generate embedding vectors, which were subsequently input into a\npre-trained shallow multilayer perceptron to predict each room type. Lastly, the classiﬁer\napproach embedded images of rooms alongside textual descriptions to identify the best-\nmatching description, thereby determining the room type. These paradigms demonstrated\nthe capacity to generalize to objects not presented in the training set and to make inferences\nwithin a space larger than that deﬁned by the trained object labels.\nYang [169] introduced the innovative zero-shot, open-vocabulary, LLM-based 3D\nvisual grounding pipeline called LLM-Grounder. This method breaks down complex natu-\nral language queries into semantic components and uses visual grounding tools such as\nOpenScene or LERF to locate objects within 3D scenes. Subsequently, the LLM evaluates\nspatial and commonsense relationships among these objects to achieve the ﬁnal grounding.\nRemarkably, LLM-Grounder operates without labeled training data and has proven its ca-\npacity to adapt to new 3D scenes and diverse text queries, enhancing grounding capabilities\nfor complex language queries and establishing itself as an effective solution.\nChen [170] developed NLMap, an open-vocabulary, queryable scene representation\nsystem. Designed to accumulate and incorporate contextual data within a scene repre-\nsentation for natural language queries, this system allows an LLM planner to visualize"
  },
  {
    "question": "What types of manipulation can robots perform using these models?",
    "chunk": "Figure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable\nrobots to carry out instructions: (a) mobile manipulation and (b,c) tabletop manipulation, in both\nsimulated and real-world environments [153].Appl. Sci.2024, 14, 8868 20 of 39\nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn,\na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot\nbehavior trees (BTs) from textual descriptions. The developed model was compact enough\nto operate on a robot’s onboard microcomputer, while adept at constructing complex robot\nbehaviors. It provided structurally and logically correct BTs and demonstrated the ability\nto handle instructions that were not included in the training set.\nSong [155], as shown in Figure9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions\nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via\na low-level planner. It continuously updated environmental information as new objects\nwere detected during action implementation and revisited the LLM to adjust the plan if\nsubgoals failed or were delayed based on updated observations. This iterative process was\nrepeated until the subgoal was achieved, after which the system moved to the next goal.\nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated\ncompetitive performance with signiﬁcantly reduced training data and proved its ability to\ngeneralize in various tasks (e.g., ALFRED) with minimal examples.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39"
  },
  {
    "question": "How do LLMs and VLMs enhance robot intelligence?",
    "chunk": "While foundation models offer considerable potential for advancing robotics intelli-\ngence, several limitations and future considerations remain. These include (1) the speed\nof inference required for real-time applications, (2) the computational efﬁciency necessary\nfor embedded systems, (3) the ability to handle multi-modality information, and (4) the\nnecessity of addressing safety and ethical considerations.\nFirst of all, LLMs and VLMs hold considerable potential for enhancing robot intelli-\ngence. Nonetheless, several critical issues remain to be addressed. Foundation models,\ncharacterized as large-scale models pre-trained on extensive datasets, face challenges re-\nlated to real-time requirements and limited computational resources in robotic applications.\nMoreover, concerns such as personal information protection, privacy, and security from\nexternal attacks need resolution to enable cloud-based LLMs for robotics.\nSecondly, to enhance the computational efﬁciency and usability of LMs, there is\nongoing research into small language models (SLMs). Despite having fewer parameters,\nSLMs can achieve performance comparable to LLMs in speciﬁc applications. Several\nSLMs have been introduced, including DistilBERT [224], which is a compact version of\nGoogle’s BERT; Phi-3 [61], another SLM; Florence-2 [84], a small VLM model from Microsoft;\nMobileBERT [225], which is optimized for mobile platforms; and compact open-source\nversions of OpenAI’s GPT models such as GPT-Neo [226] and GPT-J [227]. Generally, SLMs\nare streamlined models with fewer parameters compared to LLMs, which can number in the\nbillions. SLMs utilize smaller, domain-speciﬁc datasets and require shorter training periods,\ntypically just a few weeks, unlike LLMs, which demand vast datasets for broad learning\nand multiple months of training. Developing SLMs to excel within speciﬁc domains for\nrobotic systems and ensuring real-time performance with minimal computational resources\nare essential research directions for advancing robot intelligence with SLMs.\nThe third implication is that LLMs, based on text-centered natural language process-\ning, are limited as single-modality models when applied to real-world robotic systems\nwhere information often blends in diverse ways. Research on LLMs is transitioning from\nsingle-modality to multimodality models, as evidenced by VLMs and OpenAI Sora [228],\nwith increasing demand for such models. Currently, to address the limitations of LLMs’\nsingle-modality, robotic systems are being developed with multimodality models that\nintegrate vision, such as VLMs. However, relying solely on text and images falls short\nof the diverse information range required in the real world, including images, sounds,\nvideos, and proprioceptive sensory information (such as the position, orientation, balance,\nmovement degree, and direction of various parts of the robot). Proprioceptive sensory\ninformation related to actions and movements is particularly vital for enhancing dynamic\nhuman interaction, information processing, and manipulation and planning skills based on\ndynamic movements. For instance, the integrated VLA model, which facilitates low-level\ncontrol based on LLMs and VLMs as shown by Google’s RT-2 model, highlights the neces-\nsity for models capable of integrating information from a broader range of modalities to\nenhance robot intelligence.Appl. Sci.2024, 14, 8868 29 of 39\nFinally, the fourth area to consider is how to address safety and ethical issues when\nLLM is applied to robotic intelligence systems. Studies were conducted to address the\nissue of discriminatory and unsafe behaviors that may be generated by robot applications"
  },
  {
    "question": "How does language guidance play a role in robot task execution?",
    "chunk": "on the current image, command, and identiﬁed object data. Experimental use of real mobile\nmanipulation robots showed that MOO could adapt to new object types and environments\nin a zero-shot fashion. Moreover, MOO responded to non-verbal cues such as pointing at\nspeciﬁc objects, extending its scope to open-world exploration and manipulation.Appl. Sci.2024, 14, 8868 23 of 39\nExisting VLMs often lack a comprehensive understanding of physical concepts such\nas material and fragility, which limits their effectiveness in robotic manipulation tasks. To\naddress this issue, Gao [162] introduced PhysObjects, an object-centric dataset featuring\n39.6K crowd-sourced annotations and 417K automated annotations of physical concepts.\nThe automated annotations involved assigning speciﬁc concept values to predeﬁned object\ncategories or continuous concepts such as material and fragility. Fine-tuning a VLM on\nPhysObjects enhanced comprehension of physical concepts by capturing human biases\nrelated to the visual appearance of objects. Integrating this physically grounded VLM\nwith an LLM-based robotic planner framework improved performance in tasks requiring\nreasoning about physical concepts.\nThe traditional pre-training and ﬁne-tuning pipeline often suffers from decreased\nlearning efﬁciency and challenges in generalizing to unseen objects and tasks due to its\nreliance on domain-speciﬁc action information and domain-general visual information. To\naddress these limitations, Wang [163] proposed a modular approach named ProgramPort,\nwhich utilizes the syntactic and semantic structure of language instructions. Wang’s\nframework incorporated a semantic parser to reconstruct executable programs, composed of\nfunctional modules based on vision and action across multiple modalities. Each functional\nmodule combined deterministic computation with learnable neural networks. Program\nexecution involved generating parameters for general manipulation primitives used by the\nrobot’s end effector. The entire module network was trainable with an end-to-end imitation\nlearning objective. Experimental results demonstrated that the model effectively separated\naction and perception, achieving enhanced zero-shot and compositional generalization\nacross various manipulation tasks, speciﬁcally 16 tasks related to robot manipulation.\nHa [164] proposed a framework aimed at robot skill acquisition. This framework\nprovided a comprehensive solution by utilizing language guidance, without necessitating\nexpert demonstrations or reward speciﬁcation/engineering. It consisted of two main\ncomponents. The ﬁrst component, scaling up language-guided data generation, employed\nLLMs to break down tasks into subtasks and generate a hierarchical plan or task tree. This\nplan was materialized into various robot trajectories using 6-DoF exploration primitives.\nThese trajectories were subsequently veriﬁed and retries were performed as needed until\nsuccess was achieved. This approach enhanced the success rate of data collection and\nmore effectively mitigated the low-level understanding gap in LLMs by incorporating retry\nprocesses as part of the robot’s experiences. The second component, distilling down to\nlanguage-conditioned visuomotor policy, transformed robot experiences into a policy that\ndeduced control sequences from visual observations and natural language task descriptions.\nBy extending diffusion policies, this component handled language-based conditioning for\nmulti-task learning. To assess long-horizon behavior, commonsense reasoning, tool use,\nand intuitive physics, a new multi-task benchmark comprising 18 tasks related to robot\nmanipulation across ﬁve domains (mailbox, transport, drawer, catapult, and bus balance)\nwas developed. This benchmark effectively supported the learning of retry behaviors in\nthe data collection process and enhanced success rates.\nHuang [165], as shown in Figure12, aimed to synthesize dense robot trajectories,\nincluding 6-DoF end-effector waypoints, for various manipulation tasks using an open set"
  },
  {
    "question": "How does LLM-Planner select subgoals from its plans?",
    "chunk": "Figure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable\nrobots to carry out instructions: (a) mobile manipulation and (b,c) tabletop manipulation, in both\nsimulated and real-world environments [153].Appl. Sci.2024, 14, 8868 20 of 39\nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn,\na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot\nbehavior trees (BTs) from textual descriptions. The developed model was compact enough\nto operate on a robot’s onboard microcomputer, while adept at constructing complex robot\nbehaviors. It provided structurally and logically correct BTs and demonstrated the ability\nto handle instructions that were not included in the training set.\nSong [155], as shown in Figure9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions\nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via\na low-level planner. It continuously updated environmental information as new objects\nwere detected during action implementation and revisited the LLM to adjust the plan if\nsubgoals failed or were delayed based on updated observations. This iterative process was\nrepeated until the subgoal was achieved, after which the system moved to the next goal.\nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated\ncompetitive performance with signiﬁcantly reduced training data and proved its ability to\ngeneralize in various tasks (e.g., ALFRED) with minimal examples.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39"
  },
  {
    "question": "How are LLMs and VLMs used together in robotic applications?",
    "chunk": "Table 5.Summary of the reviewed papers in this study.\nName Explanation Ref.\nReward Design in RL\n• Eureka automatically generates and improves reward\nfunctions based on the virtual environment source\ncode.\n• DrEureka builds reward-aware physics priors using\nEureka and supports effective operation in the real\nworld through domain randomization.\n• LLMs design and reﬁne reward functions based on\nnatural language input.\n• LLMs and VLMs integrate multimodal data to\ngenerate reward functions.\n[11,134,136–139,176–180]\nLow-level\nControl\n• Generating commands to control actuators capable of\nlow-level control.\n• RT-1 and RT-2 enable robots to perform complex tasks\nbased on language-vision data.\n• AutoRT establishes a system where robots can\nautonomously collect and utilize data.\n[8–10,144–148,181–183]\nHigh-level\nPlanning\n• LLMs provide an effective methodology for tasks\nrelated to high-level planning within robotic systems.\n• By using natural language, LLMs can formulate plans\nto solve tasks that require long-horizon reasoning.\n• LLMs assess the feasibility of actions to determine and\nexecute the optimal robotic behavior.\n• LLMs generate behavior trees to structure complex\nrobotic actions accurately.\n[149–160,184–207]\nManipulation\n• Using LLMs and VLMs to integrate language and\nvision data allows various manipulations.\n• LLMs interpret high-level instructions to generate the\nnecessary robot actions and assess their feasibility.\n• VLMs extract object information from images to assist\nin performing manipulations.\n[161–167,208–215]\nScene\nUnderstanding\n• To solve VQA problems, use VLMs to extract\nhigh-level information from vision data.\n• For scene understanding, estimate and identify objects\nand evaluate relationships between objects.\n• For navigation, convert natural language instructions\nand combine them with vision data to identify the\nimage through probability distributions.\n[168–175,216–223]\n5. Discussion and Future Directions\nThe review revealed two potentials of foundation models: (1) commonsense reasoning\nfor planning and (2) the ability to generate code.\nThe ﬁrst ﬁnding from this review study is the potential to enhance robot intelligence\nthrough foundation models. Beyond the studies mentioned here, numerous recent stud-\nies have shown that pre-trained models such as LLMs and VLMs can enhance various\naspects of robot intelligence, such as situational awareness, high-level task planning, and\nhuman interaction. LLMs allow communication with humans in natural languages, objectAppl. Sci.2024, 14, 8868 28 of 39\nutilization based on extensive information, and high-level planning using that information.\nVLMs can describe tasks in text and understand visual information. Furthermore, the\ninformation from VLMs can be supplemented by connecting to knowledge databases via\nLLMs. These capabilities are crucial for enhancing robot intelligence, broadening the scope\nof robot applications, and maximizing robot utility.\nThe second ﬁnding is the code generation capability of LLMs, which has the potential\nto automate the robot development process traditionally performed by humans. Addi-\ntionally, robots that can autonomously update their own algorithms are no longer just\nscience ﬁction. Although limitations exist for robots to self-update, frameworks such as\nEureka and DrEureka, which automatically enhanced reinforcement learning performance\nfor robot motion control, demonstrate the potential for future advancements. This suggests\nthat LLMs may not only enhance human interactions but could also pave the way for\nself-improvement without human intervention."
  },
  {
    "question": "What does high-level planning mean in the context of robot navigation?",
    "chunk": "within the environment. During this process, the robot utilized a graph search algorithm to\ndetermine optimal trajectories and to navigate along these paths in the real world. This\nmethod demonstrated LM-Nav’s ability to perform long-horizon navigation in complex\noutdoor environments using natural language instructions.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 27 of 39 \n \n \nFigure 13. LM-Nav uses three pre-trained models: ( a) VNM builds a topological graph from \nobservations, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, \n(d) A graph search algorithm then ﬁnds the best robot trajectory, and ( e) the robot executes the \nplanned path [173]. \nZhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow \ninstructions. NavGPT is a vision-language na vigation system that employs an LLM to \ntranslate visual inputs from a visual foundation model (VFM) into natural language. The \nLLM then interprets the current state and makes informed decisions to reach the intended \ngoal, based on these converted visuals, naviga tion history, and potential future routes. \nNavGPT conducts various functions, incl uding high-level planning, decomposing \ninstructions into sub-goals, identifying landmarks in observed scenes, monitoring \nnavigation progress, and modifying plans as necessary. Although NavGPT’s performance \non zero-shot tasks from the R2R dataset has not yet matched that of trained models, it \nunderscored the potential of utilizing multi-modality inputs with LLMs for visual \nnavigation and tapping into the explicit reasoning capabilities of LLMs to enhance learned \nmodels. \nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-\ntrained vision-language features with a 3D reconstruction of the physical world. VLMaps, \nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary \nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands. \nThese commands can be directly localized on a map and generate new obstacle maps in \nreal-time, facilitated by sharing among various robot types. Extensive experiments \nconducted in both simulated environments (using the Habitat simulator with the \nMatterport3D dataset and the AI2THOR simulator) and real-world settings (with the HSR \nmobile robot for indoor navigation) demonstrated that VLMs can navigate based on more \ncomplex language instructions than previous methods. The reviewed papers in this study \nare summarized in Table 5. \nTable 5. Summary of the reviewed papers in this study. \nName Explanation Ref. \nReward Design in \nRL \n• Eureka automatically generates and im proves reward functions based on the \nvirtual environment source code.$• Dr Eureka builds reward-aware physics \npriors using Eureka and supports eﬀective operation in the real world through \ndomain randomization.$• LLMs design and re ﬁne reward functions based on \nnatural language input.$• LLMs and VLMs integrate multimodal data to \ngenerate reward functions. \n[11,134,136–139,176–\n180] \nFigure 13.LM-Nav uses three pre-trained models: (a) VNM builds a topological graph from observa-\ntions, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, (d)A\ngraph search algorithm then ﬁnds the best robot trajectory, and (e) the robot executes the planned\npath [173]."
  },
  {
    "question": "Can you explain what parameter-efficient fine-tuning is?",
    "chunk": "diverges from adapter tuning by adding trainable prompt vectors to the input layer. Preﬁx\ntuning [117] entails appending a sequence of preﬁxes to each transformer layer of the\nlanguage model, which consists of trainable continuous vectors. During ﬁne-tuning, the\nmodel focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM\nmodel inference.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 9 of 39 \n \nresources. Under these conditions, parameter-eﬃcient ﬁne-tuning (PEFT) oﬀers a method \ndesigned to eﬃciently conduct ﬁne-tuning of such LLMs [112]. \nAmong the methods of PEFT, there are four major approaches as shown in Figure 3: \nadapter tuning, prompt tuning, preﬁx tuning, and low-rank adaptation (LoRA). Adapter \ntuning [113,114] involves integrating small neural network modules, known as adapters, \ninto the core components of a transformer model, speciﬁcally into the attention and feed-\nforward layers. These adapters are inserted serially following these layers, allowing ﬁne-\ntuning of only the adapter modules according to speciﬁc task goals, while the parameters \nof the original language model remain unchanged. Consequently, adapter tuning e ﬀec-\ntively reduces the number of trainable parameters. Additionally, prompt tuning [115,116] \ndiverges from adapter tuning by adding trainable prompt vectors to the input layer. Preﬁx \ntuning [117] entails appending a sequence of preﬁxes to each transformer layer of the lan-\nguage model, which consists of trainable continuous vectors. During ﬁne-tuning, the \nmodel focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM \nmodel inference. \n \nFigure 3. An overview of four strategies for parameter-eﬃcient ﬁne-tuning: (a) Adapter Tuning, \n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5]. \nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a \nlow-rank constraint on transformer layers to approximate the update matrices through \ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates \nthe parameter updates using low-rank dec omposition matrices. The primary bene ﬁt of \nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning, \nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory \nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning. \nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119]. \n3.3. Prompt Techniques for Increasing LLM Performance \nTo enhance the performance of LLMs, the most straightforward approach involves \ntraining with additional data via ﬁne-tuning techniques, which mirrors supervised learn-\ning in conventional machine learning. Another method for improving performance in-\nvolves the use of in-context learning, which capitalizes on prompts for zero-shot learning, \na capability ﬁrst observed in LLMs with the advent of GPT-3. The adaptation of these \nprompts for speciﬁc tasks is known as prompt engineering. Fundamentally, prompt engi-"
  },
  {
    "question": "What is the benefit of using low-rank adaptation in LLMs?",
    "chunk": "model focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM \nmodel inference. \n \nFigure 3. An overview of four strategies for parameter-eﬃcient ﬁne-tuning: (a) Adapter Tuning, \n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5]. \nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a \nlow-rank constraint on transformer layers to approximate the update matrices through \ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates \nthe parameter updates using low-rank dec omposition matrices. The primary bene ﬁt of \nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning, \nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory \nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning. \nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119]. \n3.3. Prompt Techniques for Increasing LLM Performance \nTo enhance the performance of LLMs, the most straightforward approach involves \ntraining with additional data via ﬁne-tuning techniques, which mirrors supervised learn-\ning in conventional machine learning. Another method for improving performance in-\nvolves the use of in-context learning, which capitalizes on prompts for zero-shot learning, \na capability ﬁrst observed in LLMs with the advent of GPT-3. The adaptation of these \nprompts for speciﬁc tasks is known as prompt engineering. Fundamentally, prompt engi-\nneering (or prompting) entails supplying inputs  to the model to perform a distinct task, \ndesigning the input format to encapsulate the task’s purpose and context, and generating \nthe desired output. The four components of pr ompt engineering can be analyzed as fol-\nlows: within the prompt, “ Instructions” delineate the speci ﬁc tasks or directives for the \nmodel and “Context” provides external or additional contextual information that can tune \nthe model. Furthermore, “Input data” refers to the type of input or questions seeking an-\nswers, and “ Output data” deﬁnes the output type or format within the prompt, thereby \noptimizing the LLM’s performance for particular tasks. Various methodologies for creat-\ning prompts have been introduced, as described below. \nFigure 3. An overview of four strategies for parameter-efﬁcient ﬁne-tuning: (a) Adapter Tuning,\n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5].\nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a\nlow-rank constraint on transformer layers to approximate the update matrices through\ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates\nthe parameter updates using low-rank decomposition matrices. The primary beneﬁt of\nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning,\nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory\nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning.\nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119].\n3.3. Prompt Techniques for Increasing LLM Performance"
  },
  {
    "question": "How did the introduction of LLMs change the design of reward functions?",
    "chunk": "explanations for reinforcement learning tasks. Eureka consists of three main components:\nenvironment-as-context, evolutionary search, andreward reﬂection. Environment-as-context\ngenerates executable reward functions in a zero-shot manner by utilizing virtual environ-\nment source (Python) code as context. Evolutionary search iteratively generates reward\nfunction candidates and proposes enhanced functions based on previously generated and\nbest-performing ones, while also creating new functions through mutation. Reward reﬂec-\ntion offers a text summary of reward function quality based on training statistics recorded\nduring reinforcement learning, which assists in generating subsequent reward functions\nas feedback for the performance of previous functions. The reward functions generated\noutperformed expert-generated functions in 83% of benchmark tests. Moreover, Eureka\nsolved the pen spinning problem where a robot hand must spin a pen as much as possi-\nble according to predeﬁned rotations, a task previously considered unsolvable through\nmanual reward engineering. Eureka introduces a universal reward function design algo-\nrithm based on a code LLM and in-context evolutionary search, facilitating human-level\nreward generation for various robots and tasks without the need for prompt engineering or\nhuman intervention.\nFollowing Eureka, DrEureka [134], shown in Figure5, was developed to address\nthe sim-to-real problem by automatically conﬁguring appropriate reward functions and\ndomain randomization for physical environments. DrEureka’s reward-aware physics pri-\nors mechanism deﬁnes the lower and upper bounds of physical environment parameters\nbased on policies trained through initial reinforcement learning, facilitating reinforcement\nlearning across various physical environment domains. This randomization enables the\ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation\nmanipulation using real robots, all without human supervision.Appl. Sci.2024, 14, 8868 13 of 39Appl. Sci. 2024, 14, x FOR PEER REVIEW 13 of 39 \n \n \nFigure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses expert-\ndesigned functions through iterative improvements [11]. \nFollowing Eureka, DrEureka [134], shown in Figure 5, was developed to address the \nsim-to-real problem by automatically con ﬁguring appropriate reward functions and do-\nmain randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses\nexpert-designed functions through iterative improvements [11].\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 13 of 39 \n \n \nFigure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses expert-"
  },
  {
    "question": "What are the benefits of using LLMs in robotic planning?",
    "chunk": "mand systems. Consequently, robots can respond more adaptably and intelligently in\ninteractions with human users, allowing them to engage in complex problem-solving and\ndecision-making processes beyond simple mechanical tasks.\nAdditionally, LLMs not only enhance a robot’s communication skills to improve HRI\nusability but also boost the robot’s planning abilities. Planning involves setting goals\nand devising a sequence of actions to achieve them, which are essential in determining a\nrobot’s autonomy and efﬁciency. LLMs interpret natural language from users and complex\ncommands, enabling robots to establish and execute suitable plans in various situations.\nMoreover, LLMs adapt ﬂexibly to new situations through a zero-shot approach and utilize\npast data for learning. These capabilities indicate that robots can play a vital role in\nautonomously navigating changing environments and resolving unexpected issues.\nMoreover, VLMs such as CLIP [7], which are trained to solve vision question answering\n(VQA) tasks, have the ability to process visual and linguistic information simultaneously.\nThis ability allows robots to visually perceive their surroundings and integrate this infor-\nmation into linguistic descriptions, enabling more sophisticated situational awareness. For\ninstance, using VLMs, a robot can recognize objects and provide descriptions, as well as\nunderstand and execute user commands based on visual cues. This integrated approach\nsigniﬁcantly enhances a robot’s autonomy and interaction capabilities.\nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which en-\nable low-level actuator control using LLMs and VLMs, Google has introduced AutoRT [10].\nAutoRT is a system where robots interact with real-world objects to collect motion data. ItAppl. Sci.2024, 14, 8868 3 of 39\nbegins by exploring the surrounding space to identify feasible tasks, then uses a VLM to\nunderstand the situation and an LLM to propose possible tasks. By inputting the robot’s\noperational guidelines and safety constraints into the LLM as prompts, AutoRT assesses\nthe validity of the proposed tasks and the necessity for human intervention. Throughout\nthis process, AutoRT safely selects and executes feasible tasks while collecting relevant\ndata.\nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for\nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical causal-\nity in the real world, problem-solving through trial-and-error feedback, and code generation\nabilities. Eureka can autonomously generate reward functions for a variety of tasks and\nrobots without needing speciﬁc templates for each. This allows for the generation of\nhuman-level reward functions for diverse robots and tasks without human input. Further-\nmore, Eureka has demonstrated the ability to solve complex problems that were previously\nunsolved by expert-designed reward functions.\nGiven these research outcomes, integrating language models into robotic intelligence\npresents signiﬁcant potential to enhance robot capabilities and applications dramatically,\nthereby redeﬁning their roles in diverse industries and everyday life. Therefore, this survey\npaper explores recent research trends in LLM- and VLM-based robot intelligence, aiming to\nprovide a comprehensive understanding of future development possibilities by examining\nthe application of language models in various robotic research ﬁelds. It also seeks to\nhighlight research cases, identify current limitations, and suggest future research directions.\nTo chronicle this advancement in robotics research ﬁelds, this review paper presents\nthe following contributions:\n• This paper summarizes and introduces the foundational elements and tuning methods\nof LLM architecture.\n• It explores and arranges prompt techniques to enhance the problem-solving abilities\nof LLMs."
  },
  {
    "question": "What are the implications of using natural language instructions in outdoor navigation?",
    "chunk": "within the environment. During this process, the robot utilized a graph search algorithm to\ndetermine optimal trajectories and to navigate along these paths in the real world. This\nmethod demonstrated LM-Nav’s ability to perform long-horizon navigation in complex\noutdoor environments using natural language instructions.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 27 of 39 \n \n \nFigure 13. LM-Nav uses three pre-trained models: ( a) VNM builds a topological graph from \nobservations, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, \n(d) A graph search algorithm then ﬁnds the best robot trajectory, and ( e) the robot executes the \nplanned path [173]. \nZhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow \ninstructions. NavGPT is a vision-language na vigation system that employs an LLM to \ntranslate visual inputs from a visual foundation model (VFM) into natural language. The \nLLM then interprets the current state and makes informed decisions to reach the intended \ngoal, based on these converted visuals, naviga tion history, and potential future routes. \nNavGPT conducts various functions, incl uding high-level planning, decomposing \ninstructions into sub-goals, identifying landmarks in observed scenes, monitoring \nnavigation progress, and modifying plans as necessary. Although NavGPT’s performance \non zero-shot tasks from the R2R dataset has not yet matched that of trained models, it \nunderscored the potential of utilizing multi-modality inputs with LLMs for visual \nnavigation and tapping into the explicit reasoning capabilities of LLMs to enhance learned \nmodels. \nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-\ntrained vision-language features with a 3D reconstruction of the physical world. VLMaps, \nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary \nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands. \nThese commands can be directly localized on a map and generate new obstacle maps in \nreal-time, facilitated by sharing among various robot types. Extensive experiments \nconducted in both simulated environments (using the Habitat simulator with the \nMatterport3D dataset and the AI2THOR simulator) and real-world settings (with the HSR \nmobile robot for indoor navigation) demonstrated that VLMs can navigate based on more \ncomplex language instructions than previous methods. The reviewed papers in this study \nare summarized in Table 5. \nTable 5. Summary of the reviewed papers in this study. \nName Explanation Ref. \nReward Design in \nRL \n• Eureka automatically generates and im proves reward functions based on the \nvirtual environment source code.$• Dr Eureka builds reward-aware physics \npriors using Eureka and supports eﬀective operation in the real world through \ndomain randomization.$• LLMs design and re ﬁne reward functions based on \nnatural language input.$• LLMs and VLMs integrate multimodal data to \ngenerate reward functions. \n[11,134,136–139,176–\n180] \nFigure 13.LM-Nav uses three pre-trained models: (a) VNM builds a topological graph from observa-\ntions, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, (d)A\ngraph search algorithm then ﬁnds the best robot trajectory, and (e) the robot executes the planned\npath [173]."
  },
  {
    "question": "What are the differences between GPT-2 and GPT-3?",
    "chunk": "characteristics, numerous subsequent studies employing pre-training and ﬁne-tuning have\nbeen introduced, featuring varied structures [33,34] (e.g., BART [33] and GPT-2 [35]) and\nenhancing pre-training strategies [36–38].\nBased on subsequent studies, it has been found that increasing the model size or data\nsize of PLMs typically enhances the performance of LM models [39]. This has prompted\nresearch into training large-scale PLMs, such as GPT-3 with 175B parameters and PaLM\nwith 540B parameters. The focus of this research, grounded in scaling laws, primarily\ncenters on augmenting model sizes and exploring the capabilities of larger models. These\ncapabilities, known as the emergent abilities of LLMs, have sparked signiﬁcant interest. For\nexample, GPT-3 can address problems it has not been trained on with minimal examples\nthrough in-context learning, a feat GPT-2 ﬁnds challenging. Due to these characteristics,\nthe academic community commonly designates these large PLMs as LLMs [40–43]. Conse-\nquently, research in this area is highly active. Notably, since the introduction of OpenAI’s\nChatGPT, there has been a surge in the number of arXiv papers on LLMs. Following\nMicrosoft’s announcement [2] about integrating ChatGPT into robotics, a variety of studies\nhave explored the application of LLMs across different areas of robotics research. The\navailable LLM models are presented in chronological order in Table2. Additionally, Table3\nincludes the VLM models.Appl. Sci.2024, 14, 8868 7 of 39\nTable 2.Chronicle of LLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref.\n2018-06 GPT-1 OpenAI [ 44] 2024-02 OLMo Allen Institute for\nAI [45]\n2019-02 GPT-2 OpenAI [ 35] 2024-02 StarCoder2 Hugging Face [ 46]\n2019-10 T5 Google [ 47] 2024-03 Claude 3 Anthropic [ 48]\n2020-05 GPT-3 OpenAI [ 49] 2024-03 InternLM2 Shanghai AI Lab [ 50]\n2021-07 Codex OpenAI [ 51] 2024-03 Jamba AI21Labs [ 52]\n2021-09 FLAN Google [ 53] 2024-04 Stabe Code Stability AI [ 54]\n2021-10 T0 Hugging Face [ 37] 2024-04 HyperCLOVA Naver [ 55]\n2021-12 Gopher DeepMind [ 56] 2024-04 Grok-1.5 xAI [ 57]\n2022-03 InstructGPT OpenAI [ 58] 2024-04 Llama3 Meta AI Research [ 59]\n2022-04 PaLM Google [ 60] 2024-04 Phi-3 Microsoft [ 61]\n2022-05 OPT Meta AI Research [ 62] 2024-05 GPT-4o OpenAI [ 1]\n2023-02 LLaMA Meta AI Research [ 63] 2024-06 Claude 3.5 Anthropic [ 64]\n2023-03 Alpaca Stanford Univ. [ 65] 2024-07 GPT-4o mini OpenAI [ 66]\n2023-03 GPT-4 OpenAI [ 50] 2024-07 Falcon2-11B TII [ 67]"
  },
  {
    "question": "Can you give examples of applications that benefit from LLM tuning?",
    "chunk": "to attend only to past and present tokens, processing input and output tokens similarly\nthrough the decoder. This method underpins the development of the GPT series. Lastly, the\npreﬁx decoder, resembling the causal decoder’s masking mechanism, allows bidirectional\nattention on preﬁx tokens [107] and unidirectional attention on generated tokens. Similar\nto the encoder–decoder, the preﬁx decoder bidirectionally encodes the preﬁx sequence and\nsequentially predicts output tokens individually. Examples of preﬁx decoder-based LLMs\ninclude GLM-130B [108] and U-PaLM [109]. Additionally, various architectures have been\nproposed to address efﬁciency challenges during training or inference with long inputs,\ndue to the quadratic computational complexity of the traditional transformer architecture.\nFor instance, the Mixture-of-Experts (MoE) scaling method [34] sparsely activates a subset\nof the neural network for each input.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 8 of 39 \n \n[47] and BART [33], with Flan-T5 [106] be ing an encoder–decoder-based LLM. Secondly, \nthe causal decoder employs a unidirectional attention mask to restrict each input token to \nattend only to past and present tokens, proc essing input and output tokens similarly \nthrough the decoder. This method underpins the development of the GPT series. Lastly, \nthe preﬁx decoder, resembling the causal decoder’s masking mechanism, allows bidirec-\ntional attention on preﬁx tokens [107] and unidirectional attention on generated tokens. \nSimilar to the encoder–decoder, the pre ﬁx decoder bidirectionally encodes the pre ﬁx se-\nquence and sequentially predicts output tokens individually. Examples of preﬁx decoder-\nbased LLMs include GLM-130B [108] and U-PaLM [109]. Additionally, various architec-\ntures have been proposed to address e ﬃciency challenges during training or inference \nwith long inputs, due to the quadratic computational complexity of the traditional trans-\nformer architecture. For instance, the Mixture-of-Experts (MoE) scaling method [34] \nsparsely activates a subset of the neural network for each input. \n \nFigure 2. Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx De-\ncoder (middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectan-\ngles represent attention between preﬁx tokens, attention between preﬁx and target tokens, atten-\ntion between target tokens, and masked attention [5]. \nIn terms of the tuning of LLMs, these models are essentially pre-trained on massive \ndatasets and require ﬁne-tuning for diﬀerent application domains. However, the consid-\nerable model size and number of parameters pose challenges for ﬁne-tuning on standard \ncomputers and GPUs. The subsequent sections will discuss methods to address these chal-\nlenges. \nLLM tuning is broadly divided into two cate gories based on the training objective. \nInstruction tuning is a form of supervised learning where the training data typically in-\nclude descriptions of tasks, inputs, and corresponding outputs. This type of tuning is de-\nsigned (1) to enhance the functional capabilities of LLMs, (2) to specialize them by training \nwith discipline-speci ﬁc information, and (3) to improve task generalization and con-"
  },
  {
    "question": "How does the framework improve the execution of complex tasks by robots?",
    "chunk": "framework to assist robots in comprehendin g incomplete natural language instructions. \nThis framework enabled robots to receive instructions in natural language from humans, \nobserve their surroundings, and employ a commonsense reasoning method to \nautonomously infer missing information. LMCR utilized a model of commonsense \nreasoning learned from web-based text materials, allowing robots to understand \nincomplete instructions and autonomously execute tasks. The framework comprised three \nmain functions: language understanding, commonsense reasoning, and action planning. \nIn language understanding, LMCR translated human natural language instructions into a \nform interpretable by robots, parsing them into verb frames to convert them into \nexecutable structures. During the common sense reasoning phase, the robot analyzed \nsurrounding objects and employed a language model trained on large-scale unstructured \ntext materials to ﬁll in the missing details from the instructions. This model identiﬁed the \nFigure 7.After encoding visual features, they are mapped using visual tokens and text queries. A\nplan is then created with the LLaMA model and turned into task commands. The visual tokens are\nqueried and converted into low-level control commands to perform the task [150].\nChen [151] introduced the language-model-based commonsense reasoning (LMCR)\nframework to assist robots in comprehending incomplete natural language instructions.\nThis framework enabled robots to receive instructions in natural language from humans, ob-\nserve their surroundings, and employ a commonsense reasoning method to autonomously\ninfer missing information. LMCR utilized a model of commonsense reasoning learned\nfrom web-based text materials, allowing robots to understand incomplete instructions and\nautonomously execute tasks. The framework comprised three main functions: language\nunderstanding, commonsense reasoning, and action planning. In language understanding,\nLMCR translated human natural language instructions into a form interpretable by robots,\nparsing them into verb frames to convert them into executable structures. During the\ncommonsense reasoning phase, the robot analyzed surrounding objects and employed a\nlanguage model trained on large-scale unstructured text materials to ﬁll in the missing\ndetails from the instructions. This model identiﬁed the most suitable verb frame to com-\nplete the gaps. Subsequently, based on the completed verb frame, the robot formulated its\nactions using predeﬁned action plans for each verb to guide the movements of the robot\narm and execute the assignment. Experimental results showed that LMCR demonstrated\nsuperior generalization performance for novel concepts not presented in the training set\nand surpassed GCNGrasp, which depends on a predeﬁned graph structure for all concepts\nand their relationships. This indicated that LMCR was an effective tool, combining the se-\nmantic reasoning capabilities of language models with planning that adapted to the robot’s\nspeciﬁc environment and context, effectively managing complex and prolonged tasks.\nHuang [152] introduced a methodology named grounded decoding (GD), which offers\na method for generating LLM-based robot action plans. These plans enable robots to execute\nlong-term tasks across diverse physical environments. The methodology encompasses two\nprimary elements: linking the text generated by the language model to actionable taskAppl. Sci.2024, 14, 8868 19 of 39\ncommands in the physical world via GD and adjusting the tokens generated by the LLM\nto real-world conditions to formulate feasible commands. This approach synergizes the\nhigh-level semantic reasoning of LLMs with plans that are aligned with the robot’s physical\nenvironment and capabilities, thus facilitating the execution of complex and long-term\ntasks. The method addresses several limitations robots face in performing complex, long-\nterm tasks, such as a lack of physical world experience, an inability to process non-verbal"
  },
  {
    "question": "What is vision and language navigation in robotics?",
    "chunk": "for speciﬁc tasks through reinforcement learning, even in complex environments. The\nlow-level controlcategory includes a research area in which LLMs and VLMs generate\ncommand sequences that directly control the robot’s actuators through natural language\nand visual input. Thehigh-level planningcategory is a research area where the LLM\nidentiﬁes the present circumstances and objective of the tasks, subsequently developing an\nexplainable plan based on the reasoning required for problem-solving. In this research area,\nthe LLM is also tasked with developing the optimal robot behavior plan, which entails\nevaluating the feasibility of the established plan. In themanipulation category, the LLM in-\nterprets high-level instructions and the VLM (and LLM) analyzes various conditions based\non their understanding of the surroundings to assist robot arms in performing the speciﬁc\ntasks. While this category can be broadly included in the high-level planning category,\nthere are numerous studies that are speciﬁcally related to manipulation with a robot arm,\nwhich is why the manipulation category was separated. Thescene understandingcategory\nrepresents a research area that seeks to combine LLMs and VLMs with the objective of\nassisting robots in comprehending their surrounding environment. This is accomplished\nby identifying objects based on natural language instructions and visual information, as\nwell as by evaluating the relationships between them. This research area is also closely\nrelated to the ﬁeld of autonomous visual navigation. From a boarder perspective, there is an\noverlap between the scene understanding category and the perception-related components\nof the high-level planning category. However, in this review, the scene understanding\ncategory was considered a distinct category due to its prevalence as an application of\nVLM models.\nTable 1 lists resources that aid in understanding robot intelligence based on language\nmodels. The review [5] examined recent advancements in LLMs with a particular emphasis\non four key areas: pre-training, adaptation tuning, utilization, and capacity evaluation. Fur-\nthermore, it provided a summary of the resources currently available for the development\nof LLMs and discussed potential future directions for research in this ﬁeld. The survey [12]\nconducted a comprehensive and systematic review of VLMs for visual recognition tasks.\nIt addressed the evolution of the visual recognition paradigm, the principal architectures\nand datasets, and the fundamental principles of VLMs. Moreover, the paper provided\nan overview of the pre-training, transfer learning, and knowledge distillation methods\nemployed in the context of VLMs. The review [3] examined the potential for leveraging\nexisting natural language processing and computer vision foundation models in robotics.\nIn addition, it explored the possibility of developing a robot-speciﬁc foundation model. The\nreview [13] presented an analysis of recent studies on language-based approaches to robotic\nmanipulation. It comprised an analysis of learning paradigms integrated with foundation\nmodels related to manipulation tasks, including semantic information extraction, environ-\nment and evaluation, auxiliary tasks, task representation, safety issues, and other pertinent\nconsiderations. The survey paper [14] presented an analysis of recent research articles that\nemployed foundation models to address robotics challenges. It investigated the extent to\nwhich foundation models enhanced robot performance in perception, decision-making, and\ncontrol. In addition, it examined the obstacles impeding the implementation of foundation\nmodels in robot autonomy and proposed avenues for future advancements. The review\npaper [15] presented a comprehensive review of the research in the ﬁeld of vision and\nlanguage navigation (VLN), encompassing tasks, evaluation metrics, and methodologies\nrelated to autonomous navigation.Appl. Sci.2024, 14, 8868 5 of 39\nTable 1.Useful Review Papers.\nTitle Keywords Ref."
  },
  {
    "question": "How does the Reﬂexion model provide feedback in task execution?",
    "chunk": "into smaller units called thoughts, which it then assesses through a reasoning process to\ngauge its progress toward a solution. The ability of the model to generate and evaluate these\nthoughts is integrated with search algorithms such as breadth-ﬁrst and depth-ﬁrst search,\nfacilitating systematic thought exploration with lookahead and backtracking capabilities.\nIn contrast to the CoT method, which addresses problems sequentially, ToT concurrently\nexamines multiple pathways to ﬁnd a solution.Prompt chaining[125] is a strategy where\nthe model divides a task into sub-tasks, uses the outputs of each sub-task as subsequent\ninputs, and links prompts in input–output pairs. This approach improves the precision and\nconsistency of the outputs at each stage and simpliﬁes the handling of complex tasks by\nsubdividing them into manageable sub-tasks.\nGenerated knowledge prompting[126] is a technique in which the model incorpo-\nrates knowledge and information pertinent to the question and provides it alongside the\nquestion to generate more accurate answers. This method not only enhances the common-\nsense reasoning capabilities but also retains the ﬂexibility of existing models.Retrieval\naugmented generation(RAG) [127] merges external information retrieval with natural lan-\nguage generation. RAG can be ﬁne-tuned for knowledge-intensive downstream tasks and\nenables straightforward modiﬁcations or additions of knowledge within the framework.\nThis facilitates an increase in the model’s factual consistency, enhances the reliability of\ngenerated responses, and helps alleviate issues with hallucination.Automatic reasoning\nand tool-use(ART) [128] is a framework that utilizes external tools to autonomously gen-Appl. Sci.2024, 14, 8868 11 of 39\nerate intermediate reasoning steps. It chooses relevant tasks from a library that includes\ndemonstrations and calls on external tools as necessary to integrate their outputs into the\nreasoning process. The model generalizes from demonstrations using tools to decompose\nnew tasks and learns to use tools effectively. Enhancing ART’s performance is possible\nby modifying the task library or incorporating new tools.Automatic prompt engineer\n(APE) [129] is a framework designed for the automatic generation and selection of com-\nmands. The model generates command candidates for a problem and selects the most\nsuitable one based on a scoring function, such as execution accuracy or log probability.\nDirectional stimulus prompting[130] is a technique that directs the model to consider\nand generate responses in a particular direction. By deploying a tunable policy LM (e.g.,\nT5 [47]), it creates directional stimulus prompts for each input and uses these as cues to\nsteer the model toward producing the desired outcomes [131]. ReAct combines reasoning\nwith action within the model. It enables the model to perform reasoning in generating\nanswers, take actions based on external sources (e.g., documents, articles, and news), and\nreﬁne reasoning based on observations of these actions. This process facilitates the creation,\nmaintenance, and modiﬁcation of action plans while incorporating additional information\nfrom interactions with external sources.Reﬂexion [132] augments language-based agents\nwith language feedback. Reﬂexion involves three models: the actor, the evaluator, and self-\nreﬂection. The actor initiates actions within a speciﬁc environment to generate task steps,\nthe evaluator assesses these steps, and self-reﬂection provides linguistic feedback, which\nthe actor uses to formulate new steps and achieve the task’s objective. The introduced\nprompt techniques are summarized in Table4.\nTable 4.Prompt Techniques.\nName Explanation Ref."
  },
  {
    "question": "What is the role of feedback in robot task planning systems?",
    "chunk": "mental feedback during plan execution and revised the plan accordingly. The results\nindicated that the integration of programming language features substantially improved\ntask performance in contexts such as VirtualHome and real-world manipulation tasks in\nterms of success rate, goal conditions recall, and executability.Appl. Sci.2024, 14, 8868 21 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 10.ProgPrompt is a system that uses Python programming structures to provide environ-\nmental information and actions, enhancing the success rate of robot task planning through an error\nrecovery feedback mechanism and environmental state feedback [156].\nRana [157] introduced SayPlan, a scalable method for large-scale task planning using\nLLMs and based on a 3D Scene Graph (3DSG) representation. SayPlan involved the LLM\nsearching a collapsed 3D scene graph and task instructions to identify all relevant items\nand then locating the subgraph that contained the necessary items to complete the task.\nThe identiﬁed subgraph was subsequently used by the LLM to generate a high-level plan\nthat addressed the navigational aspect of the task. This plan was formatted as a JSON\n3D scene graph and subjected to a repetitive replanning process through feedback from\nthe scene graph simulator and a set of API calls for manipulation and operation until an\nexecutable plan was determined. SayPlan was tested in two large-scale environments,\nfeaturing up to three ﬂoors, 36 rooms, and 140 assets and objects, proving its capability\nto ground large-scale and long-horizon task plans from abstract and natural language\ninstructions, thereby enabling a mobile manipulator robot to execute these tasks.\nZeng [158], as shown in Figure11, proposed the Socratic model (SM), a modular\nframework that synergistically utilizes various forms of knowledge and employs multiple\npre-trained models to exchange information and leverage new multimodal capabilities. SM\noperates without ﬁne-tuning by integrating diverse pre-trained models and functions in\na zero-shot approach (e.g., using multimodal prompts), which enables it to harness new\nmultimodal capabilities. SM demonstrated state-of-the-art performance in zero-shot image\ncaptioning and video-to-text retrieval, and it effectively answered free-form questions about\negocentric video. Additionally, it supported interactions with external APIs and databases\n(e.g., web search) for multimodal assistive dialogue, robot perception, and planning, among"
  },
  {
    "question": "What improvements were observed in EmbodiedGPT's performance?",
    "chunk": "state representation, while the world model writer updated the system’s state according to\nexecution outcomes. By facilitating access to the world state ‘memory’, Statler improved\nLLMs’ ability to reason about planning tasks with extended time horizons, overcoming\nlimitations imposed by context length.\nMu [150], shown in Figure7, introduced EmbodiedGPT, a model speciﬁcally designed\nfor Embodied AI, which leverages LLMs. This framework processes visual observations\nand natural language to establish long-term plans and execute tasks in real-time. Em-\nbodiedGPT utilizes pre-trained vision transformers and the LLaMA language model to\nencode visual features and map them to the language modality. The generated plan was\nsubsequently converted into speciﬁc task commands using general visual tokens, encoded\nby the vision model. The framework’s functionality comprises (1) encoding current visual\nfeatures, (2) mapping visual features to the language modality via attention-based interac-\ntions between visual tokens and text queries or learnable embedded queries, (3) generating\nplans with the LLaMA language model and translating them into speciﬁc task commands,\nand (4) querying the encoded visual tokens from the vision model and translating them\ninto low-level control commands through a downstream policy network for task execu-\ntion. Experimental results, utilizing the MS-COCO dataset, revealed that EmbodiedGPT\nexcels in object recognition and understanding spatial relationships. Notably, implement-Appl. Sci.2024, 14, 8868 18 of 39\ning a closed-loop design and a “chain-of-thought” training mode signiﬁcantly enhanced\nEmbodiedGPT’s performance. These results demonstrate that EmbodiedGPT effectively\nhandles various autonomous tasks, exhibiting superior capability in object recognition,\nunderstanding spatial relationships, and generating logical, executable plans.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 18 of 39 \n \ntime. EmbodiedGPT utilizes pre-trained visi on transformers and the LLaMA language \nmodel to encode visual features and map them  to the language modality. The generated \nplan was subsequently converted into speci ﬁc task commands using general visual \ntokens, encoded by the vision model. The framework’s functionality comprises (1) \nencoding current visual features, (2) mapping visual features to the language modality \nvia a ttention-based interactions between visual tokens and text queries or learnable \nembedded queries, (3) generating plans with the LLaMA language model and translating \nthem into speciﬁc task commands, and (4) querying the encoded visual tokens from the \nvision model and translating them into low-level control commands through a \ndownstream policy network for task execution. Experimental results, utilizing the MS-\nCOCO dataset, revealed that EmbodiedG PT excels in object recognition and \nunderstanding spatial relationships. Notabl y, implementing a closed-loop design and a \n“chain-of-thought” training mode signi ﬁcantly enhanced EmbodiedGPT’s performance. \nThese results demonstrate that EmbodiedGPT e ﬀectively handles various autonomous \ntasks, exhibiting superior capability in object recognition, understanding spatial \nrelationships, and generating logical, executable plans. \n \nFigure 7. After encoding visual features, they are mappe d using visual tokens and text queries. A \nplan is then created with the LLaMA model and turned into task commands. The visual tokens are \nqueried and converted into low-level control commands to perform the task [150]. \nChen [151] introduced the language-model-based commonsense reasoning (LMCR)"
  },
  {
    "question": "What data is used to generate 3D affordance maps?",
    "chunk": "including 6-DoF end-eﬀector waypoints, for various manipulation tasks using an open set \nof instructions and objects. Huang note d that LLMs were skilled at deriving a ﬀordances \nand constraints from free-form language in structions. Further, by harnessing code \ngeneration capabilities, Huang developed 3D value maps for the agent’s observation \nspace through interactions with VLMs. Thes e 3D value maps were integrated into a \nmodel-based planning framework to generate closed-loop robot trajectories robust to \ndynamic perturbations in a zero-shot approach. The proposed framework demonstrated \neﬃcient learning of the dynamics model for sc enes with contact-rich interactions and \nprovided advantages in these complex scenarios. \n \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM to \ngenerate 3D a ﬀordance and constraint maps and design robot trajectories without additional \ntraining [165]. \nAhn [166] introduced a framework named SayCan, which integrates LLMs with \nreinforcement learning value functions, enabling robots to follow high-level text \ninstructions. SayCan comprises two primary components: Say, which uses an LLM for \ntask-based decision-making, and Can, which evaluates the feasibility of these decisions \nvia reinforcement learning. Say leverages task-based knowledge from the LLM and \nreinforcement learning functionality to assess the feasibility of task execution by robots in \nreal-world scenarios. The LLM determines th e actions necessary to achieve high-level \ngoals and evaluates the eﬀectiveness of each action in fulﬁlling the instructions. Learned \nthrough reinforcement learning, the a ﬀordance function estimate s each action’s success \nprobability in the current state, con ﬁrming the executability of actions proposed by the \nLLM. This process allows the LLM to assess the robot’s current state and capabilities, \nultimately generating an interpretable action plan. SayCan was evaluated across 101 robot \ntasks, achieving an 84% plan success rate and a 74% execution success rate in a simulated \nkitchen environment. In a real kitchen setting, the plan success rate decreased slightly to \n81% and the execution success rate fell to 60%, demonstrating that the policy and value \nfunctions generalize well to real-world settings. \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM\nto generate 3D affordance and constraint maps and design robot trajectories without additional\ntraining [165].\nAhn [166] introduced a framework named SayCan, which integrates LLMs with rein-\nforcement learning value functions, enabling robots to follow high-level text instructions.\nSayCan comprises two primary components: Say, which uses an LLM for task-based\ndecision-making, and Can, which evaluates the feasibility of these decisions via reinforce-\nment learning. Say leverages task-based knowledge from the LLM and reinforcement\nlearning functionality to assess the feasibility of task execution by robots in real-world\nscenarios. The LLM determines the actions necessary to achieve high-level goals and\nevaluates the effectiveness of each action in fulﬁlling the instructions. Learned through re-\ninforcement learning, the affordance function estimates each action’s success probability in\nthe current state, conﬁrming the executability of actions proposed by the LLM. This process\nallows the LLM to assess the robot’s current state and capabilities, ultimately generating\nan interpretable action plan. SayCan was evaluated across 101 robot tasks, achieving an\n84% plan success rate and a 74% execution success rate in a simulated kitchen environment."
  },
  {
    "question": "How do different surveys contribute to the understanding of robot learning techniques?",
    "chunk": "Robots via Foundation Models: A Survey and Meta-Analysis.arXiv 2023, arXiv:2312.08782.\n4. Xiao, X.; Liu, J.; Wang, Z.; Zhou, Y.; Qi, Y.; Cheng, Q.; He, B.; Jiang, S. Robot Learning in the Era of Foundation Models: A Survey.\narXiv 2023, arXiv:2311.14379.\n5. Mao, Y.; Ge, Y.; Fan, Y.; Xu, W.; Mi, Y.; Hu, Z.; Gao, Y. A Survey on LoRA of Large Language Models.arXiv 2024, arXiv:2407.11046.\n6. Hunt, W.; Ramchurn, S.D.; Soorati, M.D. A Survey of Language-Based Communication in Robotics.arXiv 2024, arXiv:2406.04086.\n7. Radford, A.; Kim, J.W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. Learning\nTransferable Visual Models From Natural Language Supervision.Proc. Mach. Learn. Res.2021, 139, 8748–8763.\n8. Brohan, A.; Brown, N.; Carbajal, J.; Chebotar, Y.; Dabis, J.; Finn, C.; Gopalakrishnan, K.; Hausman, K.; Herzog, A.; Hsu, J.; et al.\nRT-1: Robotics Transformer for Real-World Control at Scale. In Proceedings of the Robotics: Science and Systems 2023, Daegu,\nRepublic of Korea, 10–14 July 2023. [CrossRef]\n9. Brohan, A.; Brown, N.; Carbajal, J.; Chebotar, Y.; Chen, X.; Choromanski, K.; Ding, T.; Driess, D.; Dubey, A.; Finn, C.; et al. RT-2:\nVision-Language-Action Models Transfer Web Knowledge to Robotic Control.arXiv 2023, arXiv:2307.15818.\n10. Ahn, M.; Dwibedi, D.; Finn, C.; Arenas, M.G.; Gopalakrishnan, K.; Hausman, K.; Ichter, B.; Irpan, A.; Joshi, N.; Julian, R.; et al.\nAutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents.arXiv 2024, arXiv:2401.12963.\n11. Ma, Y.J.; Liang, W.; Wang, G.; Huang, D.-A.; Bastani, O.; Jayaraman, D.; Zhu, Y.; Fan, L.; Anandkumar, A. Eureka: Human-Level\nReward Design via Coding Large Language Models.arXiv 2023, arXiv:2310.12931.\n12. Ma, Y.; Song, Z.; Zhuang, Y.; Hao, J.; King, I. A Survey on Vision-Language-Action Models for Embodied AI.arXiv 2024,\narXiv:2405.14093.\n13. Zhou, H.; Yao, X.; Meng, Y.; Sun, S.; Bing, Z.; Huang, K.; Knoll, A. Language-Conditioned Learning for Robotic Manipulation: A\nSurvey. arXiv 2023, arXiv:2312.10807."
  },
  {
    "question": "Why is parameter-efficient fine-tuning important for LLMs?",
    "chunk": "through a better understanding of natural language commands. Conversely, alignment\ntuning (or preference alignment) seeks to align the behavior of LLMs with human values\nand preferences. Prominent methods include reinforcement learning from human feedbackAppl. Sci.2024, 14, 8868 9 of 39\n(RLHF) [110], which involves ﬁne-tuning LLMs using human feedback to better reﬂect\nhuman values, and direct preference optimization (DPO) [111], focusing on training with\npairs of human preferences that usually include an input prompt and the preferred and\nnon-preferred responses.\nFor both instruction tuning and alignment tuning, which involve training LLMs\nwith extensively large model parameters, substantial GPU memory and computational\nresources are required, with high costs typically incurred when utilizing cloud-based\nresources. Under these conditions, parameter-efﬁcient ﬁne-tuning (PEFT) offers a method\ndesigned to efﬁciently conduct ﬁne-tuning of such LLMs [112].\nAmong the methods of PEFT, there are four major approaches as shown in Figure3:\nadapter tuning, prompt tuning, preﬁx tuning, and low-rank adaptation (LoRA). Adapter\ntuning [113,114] involves integrating small neural network modules, known as adapters,\ninto the core components of a transformer model, speciﬁcally into the attention and feed-\nforward layers. These adapters are inserted serially following these layers, allowing ﬁne-\ntuning of only the adapter modules according to speciﬁc task goals, while the parameters\nof the original language model remain unchanged. Consequently, adapter tuning effec-\ntively reduces the number of trainable parameters. Additionally, prompt tuning [115,116]\ndiverges from adapter tuning by adding trainable prompt vectors to the input layer. Preﬁx\ntuning [117] entails appending a sequence of preﬁxes to each transformer layer of the\nlanguage model, which consists of trainable continuous vectors. During ﬁne-tuning, the\nmodel focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM\nmodel inference.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 9 of 39 \n \nresources. Under these conditions, parameter-eﬃcient ﬁne-tuning (PEFT) oﬀers a method \ndesigned to eﬃciently conduct ﬁne-tuning of such LLMs [112]. \nAmong the methods of PEFT, there are four major approaches as shown in Figure 3: \nadapter tuning, prompt tuning, preﬁx tuning, and low-rank adaptation (LoRA). Adapter \ntuning [113,114] involves integrating small neural network modules, known as adapters, \ninto the core components of a transformer model, speciﬁcally into the attention and feed-\nforward layers. These adapters are inserted serially following these layers, allowing ﬁne-\ntuning of only the adapter modules according to speciﬁc task goals, while the parameters \nof the original language model remain unchanged. Consequently, adapter tuning e ﬀec-\ntively reduces the number of trainable parameters. Additionally, prompt tuning [115,116] \ndiverges from adapter tuning by adding trainable prompt vectors to the input layer. Preﬁx \ntuning [117] entails appending a sequence of preﬁxes to each transformer layer of the lan-\nguage model, which consists of trainable continuous vectors. During ﬁne-tuning, the"
  },
  {
    "question": "Can you explain how ELMo improves contextual word representations?",
    "chunk": "(RNNs) to model the probability of word sequences [24–26]. A key element of this stage\nis the development of word vectors, also known as word embeddings, which form word\nprediction models based on vectors that use a distributed representation of words [24,27].\nWord2vec, a simpliﬁed shallow neural network approach, was introduced to learn these\ndistributed word representations [28,29]. It proved highly effective across various NLP\ntasks by calculating meaningful similarities between word vectors. NLMs progressed\nfrom basic word sequence modeling to sophisticated techniques for representing language\nthrough word2vec.\nFollowing the NLM phase, the ﬁeld advanced to pre-trained language models (PLMs),\nwhich encompass models such as ELMO [30] and BERT [31]. PLMs, utilizing large-scale\ntext data, learn text patterns, structures, and meanings to develop pre-trained context-\nsensitive word representations. They have successfully executed a variety of language\nunderstanding and generation tasks using this acquired knowledge. ELMo [30] introduced\na pre-training method employing bidirectional LSTM (biLSTM) networks for modeling\ndeep contextualized word representations, optimizing performance through speciﬁc ﬁne-\ntuning of the trained biLSTM network for downstream tasks. ELMo is also characterized\nas a bidirectional language model for its dual-directional use of language models.\nAnother PLM model, BERT [31], leverages the transformer architecture [32], exhibiting\nremarkable effectiveness with self-attention mechanisms and parallel processing. BERT,\na pre-trained bidirectional language model, utilizes extensive unlabeled text data. The\nmethod of unsupervised learning-based pre-training in BERT comprises two primary tasks:\nmasked language models and next sentence prediction. PLMs that provide pre-trained\ncontext-aware word representations are profoundly effective in general-purpose semantic\nfeature extraction, facilitating enhancements in NLP task performance. Owing to these\ncharacteristics, numerous subsequent studies employing pre-training and ﬁne-tuning have\nbeen introduced, featuring varied structures [33,34] (e.g., BART [33] and GPT-2 [35]) and\nenhancing pre-training strategies [36–38].\nBased on subsequent studies, it has been found that increasing the model size or data\nsize of PLMs typically enhances the performance of LM models [39]. This has prompted\nresearch into training large-scale PLMs, such as GPT-3 with 175B parameters and PaLM\nwith 540B parameters. The focus of this research, grounded in scaling laws, primarily\ncenters on augmenting model sizes and exploring the capabilities of larger models. These\ncapabilities, known as the emergent abilities of LLMs, have sparked signiﬁcant interest. For\nexample, GPT-3 can address problems it has not been trained on with minimal examples\nthrough in-context learning, a feat GPT-2 ﬁnds challenging. Due to these characteristics,\nthe academic community commonly designates these large PLMs as LLMs [40–43]. Conse-\nquently, research in this area is highly active. Notably, since the introduction of OpenAI’s\nChatGPT, there has been a surge in the number of arXiv papers on LLMs. Following\nMicrosoft’s announcement [2] about integrating ChatGPT into robotics, a variety of studies\nhave explored the application of LLMs across different areas of robotics research. The\navailable LLM models are presented in chronological order in Table2. Additionally, Table3\nincludes the VLM models.Appl. Sci.2024, 14, 8868 7 of 39\nTable 2.Chronicle of LLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref."
  },
  {
    "question": "How do language models enhance robots' ability to understand commands?",
    "chunk": "that is biased based on personal characteristics (such as race, nationality, religion, gender,\ndisability, and so forth). In addition, they can also be used to instruct robotic systems to\nengage in violent or illegal behaviors such as misstatements, sexual predation, etc. Notable\nexamples include discriminatory behaviors such as inadequate recognition of children\nor individuals with speciﬁc skin tones in human detection systems, and the exclusion\nof individuals with disabilities from task assignments. It is imperative to consider the\npotential social biases of LLM when integrating with robotic systems. Although this kind of\nconsideration was secondary in traditional robotic systems because of the limitation of their\nlanguage capability, it is a necessary consideration for LLMs to be able to generate human-\nlike language. To address this issue, previous studies have attempted to resolve it in various\nways, such as AutoRT’s constitutional rules [10], DrEureka’s safety instructions [134], and\nNeMo’s guardrails [230]. The guideline-based output control of LLMs can represent an\naccessible method to ensure safety.\nAs an extension of this point, safety issues can be identiﬁed when integrating LLMs\nand VLMs into robotic intelligence systems [231]. Typically, in robotic intelligence systems,\nLLM models generate high-level action plans in various forms, such as programming\ncodes and behavior trees based on natural language or vector prompts. At this point, a\nprompt attack has the potential to disrupt the inference of the LLMs, thereby threatening\nthe reliability and safety of the robotic system. Prompt injection is one of the prompt attacks,\nwhereby the inference of LLMs is subtly altered through speciﬁc inputs. Jailbreak, another\nprompt attack, bypasses safety rules and causes LLMs to generate abnormal behaviors to\nbe performed by the robotic system. Consequently, even minor disturbances in the input\nprompts have the potential to cause the entire robotic system to malfunction. To defend\nagainst this critical threat to the reliability and safety of robotic systems, various techniques\nhave been proposed, such as input validation, which ﬁlters the model’s input, and context\nlocking, which restricts access based on the history and content of the prompt. Furthermore,\nstrict guardrails that restrict harmful or unsafe outputs from models can be an alternative\nto improve the reliability of robotic systems. However, it is essential to recognize that the\nsecurity techniques may potentially lead to a decline in the performance of the robot system.\nConsequently, the trade-off between performance and safety must be carefully considered.\nSince the emergence of ChatGPT and Microsoft’s implementation of robot systems\nusing ChatGPT [2], artiﬁcial intelligence components have been applied more widely and\nintensively in robotics research. Despite existing challenges, it is expected that research\ninvolving foundation models to improve robot intelligence will persist across various\ndomains and methods, which will likely enhance the usability and market potential of\nrobot systems inﬂuenced by these advancements.\n6. Conclusions\nIn this paper, we have explored the potential impact and applicability of LLMs on\nrobotics research ﬁelds by summarizing studies that applied LLMs and VLMs to robots.\nFundamentally, LLMs can enhance the capability of robots in natural language processing\nto interact with humans and to improve the robots’ autonomy in various task scenarios.\nIn particular, the ability of LLMs to understand and generate natural language plays a\ncrucial role in enabling robots to comprehend and execute complex commands. This survey\nconﬁrmed that the scope of utilizing LLMs in robotics was not limited to simple natural\nlanguage processing but also extended to broader research areas. This study explored"
  },
  {
    "question": "What findings were reported regarding the generalization performance of robotic systems?",
    "chunk": "parsing them into verb frames to convert them into executable structures. During the\ncommonsense reasoning phase, the robot analyzed surrounding objects and employed a\nlanguage model trained on large-scale unstructured text materials to ﬁll in the missing\ndetails from the instructions. This model identiﬁed the most suitable verb frame to com-\nplete the gaps. Subsequently, based on the completed verb frame, the robot formulated its\nactions using predeﬁned action plans for each verb to guide the movements of the robot\narm and execute the assignment. Experimental results showed that LMCR demonstrated\nsuperior generalization performance for novel concepts not presented in the training set\nand surpassed GCNGrasp, which depends on a predeﬁned graph structure for all concepts\nand their relationships. This indicated that LMCR was an effective tool, combining the se-\nmantic reasoning capabilities of language models with planning that adapted to the robot’s\nspeciﬁc environment and context, effectively managing complex and prolonged tasks.\nHuang [152] introduced a methodology named grounded decoding (GD), which offers\na method for generating LLM-based robot action plans. These plans enable robots to execute\nlong-term tasks across diverse physical environments. The methodology encompasses two\nprimary elements: linking the text generated by the language model to actionable taskAppl. Sci.2024, 14, 8868 19 of 39\ncommands in the physical world via GD and adjusting the tokens generated by the LLM\nto real-world conditions to formulate feasible commands. This approach synergizes the\nhigh-level semantic reasoning of LLMs with plans that are aligned with the robot’s physical\nenvironment and capabilities, thus facilitating the execution of complex and long-term\ntasks. The method addresses several limitations robots face in performing complex, long-\nterm tasks, such as a lack of physical world experience, an inability to process non-verbal\ncues, and a disregard for necessary robotic constraints such as safety and rewards. The\npaper details experiments in a simulated tabletop rearrangement, a mini-grid 2D maze,\nand real-world kitchen mobile manipulation settings to evaluate long-horizon reasoning\nperformance. Comparative experiments with SayCan revealed that while SayCan limits\nthe range of robot actions, GD can represent a wider array of actions. In contrast to\nCLIPort, which executes high-level language instructions directly, GD achieves enhanced\nperformance through detailed, step-by-step planning.\nHuang [153], as shown in Figure8, proposed the inner monologue method, which\nallowed LLMs to plan and adjust based on feedback from the environment. This approach\nenabled robots to formulate plans in dynamic environments, retry upon facing failure,\nor seek human feedback to reﬁne their strategies. The author clariﬁed that this method\nemerged from integrating the LLM’s high-level planning capabilities with perceptual feed-\nback and low-level control, thereby facilitating more adaptable and intelligent interactions.\nInner monologue integrated various feedback sources into the language model to assist the\nrobot in executing given instructions, including text-based indicators of the robot’s action\nsuccess or failure, object recognition and descriptions within the scene, the robot’s ability to\nask questions to gather additional information, breaking down instructions into multiple\nsteps to establish an execution plan, and enabling the robot to interact with humans to\nexecute and reﬁne the instructions. The inner monologue method was evaluated in both\nsimulated and real-world environments, such as tabletop rearrangement tasks and manip-\nulation tasks in a real kitchen. The results showed that inner monologue was an effective\nframework, enabling robots to act intelligently in complex interactive settings by effectively\nintegrating environmental feedback to plan and execute tasks.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 20 of 39"
  },
  {
    "question": "What is the impact of language models on robot intelligence?",
    "chunk": "language models and their variants have been utilized to enhance robot intelligence in\nthe literature.\nAll articles that met the above criteria were included in this review. Following an\nintensive survey of the abstracts of the selected articles, we categorized the research topics\ninto ﬁve groups: reward design for reinforcement learning, low-level control, high-level\nplanning, manipulation, and scene understanding. Figure1 illustrates these ﬁve categories.\nOur categorization was based on a thorough review of the sources in the literature. Subse-\nquently, duplicate articles were removed, and those not meeting the speciﬁed eligibility\ncriteria were excluded. The exclusion criteria included: (1) articles in languages other than\nEnglish and (2) articles discussing general concepts that do not focus on deep reinforcement\nlearning-based manipulation.\n3. Related Works\n3.1. Language Model\nZhao’s LLM review paper [5] categorizes the evolution of Language Models (LMs)\ninto four phases. The initial stage, the statistical language model (SLM) [16–19], utilizes\nmethods based on statistical learning techniques and the Markov assumption to construct\nword prediction models. A notable method from this phase is the n-gram language model,\nwhich predicts words based on a ﬁxed context length of n. Although SLMs have enhanced\nperformance in various domains such as information retrieval (IR) [16,20] and naturalAppl. Sci.2024, 14, 8868 6 of 39\nlanguage processing (NLP) [21–23], higher-order language models have encountered lim-\nitations due to the curse of dimensionality, which necessitates estimating exponentially\nincreasing transition probabilities.\nThe subsequent phase of LMs, termed neural language models (NLMs), leveraged\nneural networks such as multi-layer perceptron (MLP) and recurrent neural networks\n(RNNs) to model the probability of word sequences [24–26]. A key element of this stage\nis the development of word vectors, also known as word embeddings, which form word\nprediction models based on vectors that use a distributed representation of words [24,27].\nWord2vec, a simpliﬁed shallow neural network approach, was introduced to learn these\ndistributed word representations [28,29]. It proved highly effective across various NLP\ntasks by calculating meaningful similarities between word vectors. NLMs progressed\nfrom basic word sequence modeling to sophisticated techniques for representing language\nthrough word2vec.\nFollowing the NLM phase, the ﬁeld advanced to pre-trained language models (PLMs),\nwhich encompass models such as ELMO [30] and BERT [31]. PLMs, utilizing large-scale\ntext data, learn text patterns, structures, and meanings to develop pre-trained context-\nsensitive word representations. They have successfully executed a variety of language\nunderstanding and generation tasks using this acquired knowledge. ELMo [30] introduced\na pre-training method employing bidirectional LSTM (biLSTM) networks for modeling\ndeep contextualized word representations, optimizing performance through speciﬁc ﬁne-\ntuning of the trained biLSTM network for downstream tasks. ELMo is also characterized\nas a bidirectional language model for its dual-directional use of language models.\nAnother PLM model, BERT [31], leverages the transformer architecture [32], exhibiting\nremarkable effectiveness with self-attention mechanisms and parallel processing. BERT,\na pre-trained bidirectional language model, utilizes extensive unlabeled text data. The\nmethod of unsupervised learning-based pre-training in BERT comprises two primary tasks:\nmasked language models and next sentence prediction. PLMs that provide pre-trained\ncontext-aware word representations are profoundly effective in general-purpose semantic"
  },
  {
    "question": "How can LLMs be applied to improve robotic actions like drawing?",
    "chunk": "supporting the creation of high-level policies for robots and accommodating a variety of\nrobotic tasks. Speciﬁcally, CaP interpreted natural language instructions through descrip-\ntions and formulated an action plan for the robot. Moreover, it utilized VLMs such as ViLD\nand MDETR to identify objects and ascertain their locations. Based on this information,\nthe framework controlled the robot’s movements to carry out speciﬁed tasks. The paper\ndemonstrated the CaP framework across diverse domains, including whiteboard drawing,\ntabletop manipulation, and mobile robot navigation and manipulation. Experimental\nresults showed that CaP achieved similar or better success rates than existing systems\nsuch as CLIPort, displaying notably strong generalization capabilities for new tasks. These\nﬁndings underscored the ﬂexibility and efﬁcacy of the CaP framework, establishing its\neffectiveness across various robotic systems.\nMirchandani [148], shown in Figure6, suggested that pre-trained LLMs could autore-\ngressively complete complex token sequences and function as general sequence modelers\nthrough in-context learning without needing additional training. Expanding on this con-\ncept, the study evaluated LLMs’ ability to operate as pattern machines in three domains:\nsequence transformation, sequence completion, and sequence improvement. In sequence\ntransformation, the research demonstrated that LLMs could generalize speciﬁc sequence\ntransformations using benchmarks such as ARC (abstraction and reasoning corpus) and\nPCFG (probabilistic context-free grammar), thereby proving their utility in spatial reasoning\ntasks for robotics. In sequence completion, the study examined whether LLMs could ﬁnish\npatterns in elementary functions (e.g., sinusoids), illustrating their utility in robotic tasks\nsuch as extending a wiping motion from kinesthetic demonstrations or creating drawings\non a whiteboard. Finally, in sequence improvement, the research revealed that by utilizing\nreward-labeled trajectories as context and incorporating online interaction, LLM-based\nagents could explore small grids and reﬁne simple trajectories using human-in-the-loop\nmethods, such as optimizing a CartPole controller.Appl. Sci.2024, 14, 8868 17 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 17 of 39 \n \nsequence transformation, the research demonstrated that LLMs could generalize speci ﬁc \nsequence transformations using benchmarks such as ARC (abstraction and reasoning \ncorpus) and PCFG (probabilistic context-free grammar), thereby proving their utility in \nspatial reasoning tasks for robotics. In sequence completion, the study examined whether \nLLMs could ﬁnish pa tterns in elementary functions (e.g., sinusoids), illustrating their \nutility in robotic tasks such as extending a wiping motion from kinesthetic demonstrations \nor creating drawings on a whiteboard. Finally, in sequence improvement, the research \nrevealed that by utilizing reward-labeled trajectories as context and incorporating online \ninteraction, LLM-based agents could explore small grids and re ﬁne simple trajectories \nusing human-in-the-loop methods, such as optimizing a CartPole controller.  \n \nFigure 6. Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed \nin sequence transformation, completion, and improvement [148]. \n4.3. High-Level Planning (Including Decision-Making and Reasoning) \nThe abstraction and generalization capabilities of LLMs oﬀer eﬀective methodologies \nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various \nresearch outcomes have been realized in the ﬁelds of planning, decision-making,"
  },
  {
    "question": "What are the benefits of using multi-environment approaches in training robots?",
    "chunk": "4. Language Models for Robotic Intelligence\n4.1. Reward Design in Reinforcement Learning\nResearch in reinforcement learning, closely associated with the ﬁeld of robotics, has\nactively incorporated studies using LLM models. Speciﬁcally, Nvidia has developed a\nGPU-based multi-environment reinforcement learning platform. Utilizing its Omniverse\n3D virtual environment platform, Nvidia created Isaac Sim, which is dedicated to robot\nsimulation. Isaac Sim published research ﬁndings on Isaac Gym (Preview), which achieved\nsigniﬁcant reductions in reinforcement learning training times through GPU-based multi-\nenvironment approaches. Subsequently, Isaac Gym (Preview)’s features were integrated\ninto Isaac Sim and released as Omni Isaac Gym. Later, Nvidia introduced Orbit [133],\nfacilitating the simulation of PhysX 5.1-based cloth, soft-body, ﬂuid, and rigid-body dy-\nnamics, along with RGBD, LiDAR, and contact sensor simulation. Orbit also incorporates\nvarious robot platforms into the simulation environment. Recently, Orbit was updated\nto Isaac Lab and integrated into Isaac Sim 4.0. Nvidia has continuously advanced dy-\nnamic simulation environment technologies for reinforcement learning using GPU parallel\ncomputation. Leveraging this GPU reinforcement learning, they launched Eureka [11],\nwhich automates the design of reward functions for reinforcement learning using LLMs.\nFollowing this, Nvidia introduced DrEureka [134], an automated platform addressing the\nSim2Real problem [135] in reinforcement learning based on Eureka.\nEureka (Evolution-driven Universal REward Kit for Agent) [11], shown in Figure4,\nautomatically generates reward functions for various tasks using different robots, elimi-\nnating the need for speciﬁc templates or tailored reward functions for the robot’s form or\nexplanations for reinforcement learning tasks. Eureka consists of three main components:\nenvironment-as-context, evolutionary search, andreward reﬂection. Environment-as-context\ngenerates executable reward functions in a zero-shot manner by utilizing virtual environ-\nment source (Python) code as context. Evolutionary search iteratively generates reward\nfunction candidates and proposes enhanced functions based on previously generated and\nbest-performing ones, while also creating new functions through mutation. Reward reﬂec-\ntion offers a text summary of reward function quality based on training statistics recorded\nduring reinforcement learning, which assists in generating subsequent reward functions\nas feedback for the performance of previous functions. The reward functions generated\noutperformed expert-generated functions in 83% of benchmark tests. Moreover, Eureka\nsolved the pen spinning problem where a robot hand must spin a pen as much as possi-\nble according to predeﬁned rotations, a task previously considered unsolvable through\nmanual reward engineering. Eureka introduces a universal reward function design algo-\nrithm based on a code LLM and in-context evolutionary search, facilitating human-level\nreward generation for various robots and tasks without the need for prompt engineering or\nhuman intervention.\nFollowing Eureka, DrEureka [134], shown in Figure5, was developed to address\nthe sim-to-real problem by automatically conﬁguring appropriate reward functions and\ndomain randomization for physical environments. DrEureka’s reward-aware physics pri-\nors mechanism deﬁnes the lower and upper bounds of physical environment parameters\nbased on policies trained through initial reinforcement learning, facilitating reinforcement\nlearning across various physical environment domains. This randomization enables the\ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation"
  },
  {
    "question": "Can you explain the concept of the Robot Constitution used in AutoRT?",
    "chunk": "of-thought prompting techniques to RT-2 has proven effective in solving more complex\nsemantic inference tasks, such as using a rock as an improvised hammer or offering an\nenergy drink instead of a carbonated beverage to a thirsty person. In comparison with\nthe earlier study on RT-1, RT-2 demonstrates enhanced performance in both familiar and\nnovel tasks.\nAutoRT [10] is a follow-up study based on the research results of RT-1 and RT-2,\nestablishing an orchestration of large-scale robotic agents for data collection in real-world\nscenarios. AutoRT employed 53 robots to gather 77,000 real robot episodes over seven\nmonths through both teleoperation and autonomous robot policies. At the heart of AutoRT\nis a robust foundation model that generates ‘task proposals’ based on given visual observa-\ntions. Notably, AutoRT introduces a ‘Robot Constitution’ using constitutional prompting to\nensure actions during the task proposal process do not compromise the safety of the robot\nor nearby individuals. This Robot Constitution, inspired by Asimov’s three laws [142],\ncomprises basic rules, safety rules that identify unsafe or unwanted tasks, and embodiment\nrules that clarify the robot’s operational boundaries.\nAutoRT enhances data collection by initially scanning the surroundings to identify\ninteresting scenes or tasks (exploration). It interprets the given context through a VLM\nand proposes potential tasks via an LLM (task generation). Subsequently, tasks suggested\nby the LLM are screened (affordance) to assess their feasibility and the need for human\nintervention, employing the Robot Constitution. During this procedure, viable tasks are\nchosen and performed, while pertinent data are gathered (data collection). The collected\ndata are then assessed for (diversity scoring) the visual diversity of the robot trajectories\nand the linguistic diversity of the language instructions generated by AutoRT (LLM). The\naim of this diversity evaluation is to conﬁrm that, unlike simulations, real-world data\ncollection by robots is labor-intensive, making it essential to gather data across a broad\nspectrum of tasks. Experimental outcomes illustrate that AutoRT achieves higher visual\nand linguistic diversity compared to RT-1 or BC-Z [143].\nOther researchers include Tang [144], who developed an approach that connects\nnatural language user commands with a locomotion controller using foot contact patterns as\nan interface for low-level commands. This innovative interface translates human commands\ninto the robot’s foot contact patterns, allowing the robot to move at a speciﬁed speed with\nprecise timing for each foot’s contact with the ground. To achieve this, the robot used a\ncyclic sliding window to extract foot contact ﬂags from a pattern template, thus generating\nthe required foot contact patterns. During training, a random pattern generator created\nfoot contact patterns, and during testing, an LLM translated human commands into these\npatterns. The robot then adjusted its movements based on the foot contact patterns it\nlearned through deep reinforcement learning, closely adhering to the intended foot contact\npatterns and speed commands. This approach demonstrated a 50% higher success rate in\ntask evaluation (across 30 tasks, including standing still) compared to two baselines (which\nemployed discrete gaits and sinusoidal functions as interfaces), successfully solving 10\nmore tasks than the baselines.\nMandi [145] introduced a novel method for multi-robot collaboration that utilizes\nLLMs for both high-level communication and low-level path planning. In this method, the\nrobots employ the LLM to discuss and reason about task strategies. They generate sub-taskAppl. Sci.2024, 14, 8868 16 of 39\nplans and task space waypoint paths, which a multi-arm motion planner then uses to expe-"
  },
  {
    "question": "Can you break down the process of chain-of-thought reasoning step-by-step?",
    "chunk": "3.3. Prompt Techniques for Increasing LLM Performance\nTo enhance the performance of LLMs, the most straightforward approach involves\ntraining with additional data via ﬁne-tuning techniques, which mirrors supervised learning\nin conventional machine learning. Another method for improving performance involves the\nuse of in-context learning, which capitalizes on prompts for zero-shot learning, a capability\nﬁrst observed in LLMs with the advent of GPT-3. The adaptation of these prompts for\nspeciﬁc tasks is known as prompt engineering. Fundamentally, prompt engineering (or\nprompting) entails supplying inputs to the model to perform a distinct task, designing the\ninput format to encapsulate the task’s purpose and context, and generating the desired\noutput. The four components of prompt engineering can be analyzed as follows: within\nthe prompt, “Instructions” delineate the speciﬁc tasks or directives for the model andAppl. Sci.2024, 14, 8868 10 of 39\n“Context” provides external or additional contextual information that can tune the model.\nFurthermore, “Input data” refers to the type of input or questions seeking answers, and\n“Output data” deﬁnes the output type or format within the prompt, thereby optimizing the\nLLM’s performance for particular tasks. Various methodologies for creating prompts have\nbeen introduced, as described below.\nZero-shot prompting[53] is a technique that allows the model to take on new tasks\nwith no prior examples. The model relies solely on the task description or instructions with-\nout additional training. Likewise,few-shot prompting[49] introduces a small number of\nexamples to aid the model in learning new tasks. This approach does not require extensive\ndatasets and can improve the model’s performance through a limited set of examples.\nChain-of-thought (CoT) [41] is a technique that explicitly describes intermediate\nreasoning steps, enabling the model to perform step-by-step reasoning. This approach\nallows the model to incrementally solve complex tasks. For instance, when asked, “If\nsomeone’s age will be 30 in 5 years, how old are they now?”, the model uses the information\n“age in 5 years is 30” to perform the intermediate reasoning step of “30→ 5 = 25” to derive\nthe ﬁnal answer.Self-consistency [120] involves the model generating various independent\nreasoning paths through few-Shot CoT, ultimately selecting the most consistent answer\namong the outputs. This method enhances the performance of CoT prompts in both\narithmetic and commonsense reasoning tasks.Multimodal CoT [121] is a two-stage\nframework that integrates text and visual modalities. Initially, intermediate reasoning steps\nare generated through rationale generation based on multimodal data. Subsequently, the\nanswer inferences are intertwined, and the informative rationales are utilized to derive the\nﬁnal answer.\nGenerally, CoT relies on human-generated annotations, which may not always provide\nthe optimal solution for problem-solving. To overcome this limitation,active prompt[122]\nhas been proposed. Active prompt enhances model performance by intensively training\nthe model on questions with higher uncertainty levels. It evaluates the uncertainty of\nanswers by posing questions to the model, with or without CoT examples. Questions with\nhigh uncertainty are selected for human annotation, and newly annotated examples are\nused to reason through each question.Program-aided language models(PAL) [123] is a\ntechnique that employs the model to understand natural language problems and generate\nprograms as intermediate reasoning steps. Unlike CoT, PAL solves problems stepwise\nusing a program runtime such as Python rather than free-form text.\nTree of thoughts(ToT) [124] is a method whereby the model breaks down a problem"
  },
  {
    "question": "How does SayPlan utilize 3D scene graphs for task execution?",
    "chunk": "mental feedback during plan execution and revised the plan accordingly. The results\nindicated that the integration of programming language features substantially improved\ntask performance in contexts such as VirtualHome and real-world manipulation tasks in\nterms of success rate, goal conditions recall, and executability.Appl. Sci.2024, 14, 8868 21 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 10.ProgPrompt is a system that uses Python programming structures to provide environ-\nmental information and actions, enhancing the success rate of robot task planning through an error\nrecovery feedback mechanism and environmental state feedback [156].\nRana [157] introduced SayPlan, a scalable method for large-scale task planning using\nLLMs and based on a 3D Scene Graph (3DSG) representation. SayPlan involved the LLM\nsearching a collapsed 3D scene graph and task instructions to identify all relevant items\nand then locating the subgraph that contained the necessary items to complete the task.\nThe identiﬁed subgraph was subsequently used by the LLM to generate a high-level plan\nthat addressed the navigational aspect of the task. This plan was formatted as a JSON\n3D scene graph and subjected to a repetitive replanning process through feedback from\nthe scene graph simulator and a set of API calls for manipulation and operation until an\nexecutable plan was determined. SayPlan was tested in two large-scale environments,\nfeaturing up to three ﬂoors, 36 rooms, and 140 assets and objects, proving its capability\nto ground large-scale and long-horizon task plans from abstract and natural language\ninstructions, thereby enabling a mobile manipulator robot to execute these tasks.\nZeng [158], as shown in Figure11, proposed the Socratic model (SM), a modular\nframework that synergistically utilizes various forms of knowledge and employs multiple\npre-trained models to exchange information and leverage new multimodal capabilities. SM\noperates without ﬁne-tuning by integrating diverse pre-trained models and functions in\na zero-shot approach (e.g., using multimodal prompts), which enables it to harness new\nmultimodal capabilities. SM demonstrated state-of-the-art performance in zero-shot image\ncaptioning and video-to-text retrieval, and it effectively answered free-form questions about\negocentric video. Additionally, it supported interactions with external APIs and databases\n(e.g., web search) for multimodal assistive dialogue, robot perception, and planning, among"
  },
  {
    "question": "What are the generalization capabilities of the CaP framework?",
    "chunk": "dite trajectory planning. Additionally, environmental feedback, such as collision detection,\nprompts the LLM agent to reﬁne plans and waypoints contextually. This method achieved\na high success rate across all tasks in the RoCoBench (including duties such as sweeping\nthe ﬂoor), effectively adapting to variations in task semantics. In real-world experiments,\nspeciﬁcally the block-sorting task, RoCo demonstrated its ability to communicate and\ncollaborate with other robot agents to successfully complete the tasks.\nWang [146] proposed a novel paradigm for utilizing few-shot prompts in physical\nenvironments. This method involved gathering observation and action pairs from existing\nmodel-based or learning-based controllers to form the initial text prompts. Data included\nsensor readings, such as IMU and joint encoders, coupled with target joint positions.\nThese data formed the starting input for LLM inference. As the robot interacted with its\nenvironment and collected new observational data, these initial data were updated with\noutputs from the LLM. In the subsequent prompt engineering phase, observation and action\npairs, along with explanatory prompts, were crafted to enable the LLM to function as a\nfeedback policy. The explanatory prompts provided clear descriptions of the robot walking\ntask and control design details, while the observation and action prompts delineated the\nformat and signiﬁcance of each observation and action. This method allowed the LLM\nto directly output low-level target joint positions for robot walking. The approach was\ntested using the ANYmal robot in MuJoCo and Isaac Gym simulators for robot walking,\nindicating that the LLM could act as a low-level feedback controller for dynamic motion\ncontrol within sophisticated robot systems.\nLiang [147] introduced a new framework namedCode as Policies(CaP) that directly\nconstructs robot policies from executable code generated by a code LLM. This framework\nenabled the interpretation and execution of natural language instructions through an LLM,\nsupporting the creation of high-level policies for robots and accommodating a variety of\nrobotic tasks. Speciﬁcally, CaP interpreted natural language instructions through descrip-\ntions and formulated an action plan for the robot. Moreover, it utilized VLMs such as ViLD\nand MDETR to identify objects and ascertain their locations. Based on this information,\nthe framework controlled the robot’s movements to carry out speciﬁed tasks. The paper\ndemonstrated the CaP framework across diverse domains, including whiteboard drawing,\ntabletop manipulation, and mobile robot navigation and manipulation. Experimental\nresults showed that CaP achieved similar or better success rates than existing systems\nsuch as CLIPort, displaying notably strong generalization capabilities for new tasks. These\nﬁndings underscored the ﬂexibility and efﬁcacy of the CaP framework, establishing its\neffectiveness across various robotic systems.\nMirchandani [148], shown in Figure6, suggested that pre-trained LLMs could autore-\ngressively complete complex token sequences and function as general sequence modelers\nthrough in-context learning without needing additional training. Expanding on this con-\ncept, the study evaluated LLMs’ ability to operate as pattern machines in three domains:\nsequence transformation, sequence completion, and sequence improvement. In sequence\ntransformation, the research demonstrated that LLMs could generalize speciﬁc sequence\ntransformations using benchmarks such as ARC (abstraction and reasoning corpus) and\nPCFG (probabilistic context-free grammar), thereby proving their utility in spatial reasoning\ntasks for robotics. In sequence completion, the study examined whether LLMs could ﬁnish\npatterns in elementary functions (e.g., sinusoids), illustrating their utility in robotic tasks\nsuch as extending a wiping motion from kinesthetic demonstrations or creating drawings"
  },
  {
    "question": "How can robots manipulate objects using language models?",
    "chunk": "understand the situation and an LLM to propose possible tasks. By inputting the robot’s\noperational guidelines and safety constraints into the LLM as prompts, AutoRT assesses\nthe validity of the proposed tasks and the necessity for human intervention. Throughout\nthis process, AutoRT safely selects and executes feasible tasks while collecting relevant\ndata.\nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for\nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical causal-\nity in the real world, problem-solving through trial-and-error feedback, and code generation\nabilities. Eureka can autonomously generate reward functions for a variety of tasks and\nrobots without needing speciﬁc templates for each. This allows for the generation of\nhuman-level reward functions for diverse robots and tasks without human input. Further-\nmore, Eureka has demonstrated the ability to solve complex problems that were previously\nunsolved by expert-designed reward functions.\nGiven these research outcomes, integrating language models into robotic intelligence\npresents signiﬁcant potential to enhance robot capabilities and applications dramatically,\nthereby redeﬁning their roles in diverse industries and everyday life. Therefore, this survey\npaper explores recent research trends in LLM- and VLM-based robot intelligence, aiming to\nprovide a comprehensive understanding of future development possibilities by examining\nthe application of language models in various robotic research ﬁelds. It also seeks to\nhighlight research cases, identify current limitations, and suggest future research directions.\nTo chronicle this advancement in robotics research ﬁelds, this review paper presents\nthe following contributions:\n• This paper summarizes and introduces the foundational elements and tuning methods\nof LLM architecture.\n• It explores and arranges prompt techniques to enhance the problem-solving abilities\nof LLMs.\n• It reviews and encapsulates how LLMs and VLMs have been employed to augment\nrobot intelligence across ﬁve topics as shown in Figure1: (1) reward design for\nreinforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation,\nand (5) scene understanding.\nAppl. Sci. 2024 , 14 , x FOR PEER REVIEW 3 of 39 \n \nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which \nenable low-level actuator control using LLM s and VLMs, Google has introduced AutoRT \n[10]. AutoRT is a system where robots interact  with real-world objects to collect motion \ndata. It begins by exploring th e surrounding space to identify feasible tasks, then uses a \nVLM to understand the situation and an LLM to propose possible tasks. By inpu tting the \nrobot’s operational guidelines and safety co nstraints into the LLM as prompts, AutoRT \nassesses the validity of the proposed tasks and the necessity for human intervention. \nThroughout this process, AutoRT safely selects and executes feasible tasks while collecting \nrelevant data. \nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for \nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical cau-\nsality in the real world, problem-solving through trial-and-error feedback, and code gen-\neration abilities. Eureka can autonomously generate reward functions for a variety of tasks \nand robots without needing speci ﬁc templates for each. This allows for the generation of \nhuman-level reward functions for diverse robo ts and tasks without human input. Further-"
  },
  {
    "question": "What challenges do researchers face when integrating LLMs into robotic systems?",
    "chunk": "language processing but also extended to broader research areas. This study explored\nextensive LLM applications in the robotics literature, such as planning, manipulation, and\nscene understanding, as well as reinforcement learning automation frameworks such as\nEureka, and included robot actions in language models such as AutoRT. Moreover, the\nresearch direction of current generative AI models is transitioning towards multimodalAppl. Sci.2024, 14, 8868 30 of 39\nlanguage models, moving beyond information acquisition and cognition aspects such as\ntext, images, and videos to include actuator actions within large models in therobotics ﬁeld.\nWhile the surveyed studies indicated that LLMs play a promising role in the future\nof robotics, certain limitations were also identiﬁed. First, the increased computational\nresources and energy consumption associated with embedding LLMs into robotic systems\nmust be addressed. Second, biases in language models and ethical considerations are\nsigniﬁcant issues that need to be tackled in robotics. Therefore, continual efforts will be\nnecessary in future research to resolve these challenges.\nOverall, LLMs are valuable tools that can signiﬁcantly advance robotics. This review\nhas revealed that innovative robot applications are possible through the integration of\nLLMs and VLMs. Moreover, these foundation models are expected to serve as critical\nelements for future robot research and practical applications in the real world.\nAuthor Contributions:Conceptualization, S.S. and C.K.; methodology, S.S.; formal analysis, H.J.,\nH.L. and S.S.; investigation, H.J., H.L. and S.S.; resources, H.J., H.L., S.S. and C.K.; writing—original\ndraft preparation, H.J., H.L., S.S. and C.K.; writing—review and editing, H.J., H.L., S.S. and C.K.; vi-\nsualization, H.J. and H.L.; supervision, S.S. and C.K.; project administration, S.S.; funding acquisition,\nS.S. All authors have read and agreed to the published version of the manuscript.\nFunding: This work was supported by the Technology Innovation Program (RS-2024-00423702,\nA Meta-Humanoid with Hypermodal Cognitivity and Role Dexterity: Adroid4X) funded by the\nMinistry of Trade, Industry, and Energy (MOTIE, Korea) and Regional Innovation Strategy (RIS)\nthrough the National Research Foundation of Korea (NRF) funded by the Ministry of Education\n(MOE) (2023RIS-007).\nInstitutional Review Board Statement:Not applicable.\nInformed Consent Statement:Not applicable.\nData Availability Statement:No new data were created or analyzed in this study. Data sharing is\nnot applicable to this article.\nConﬂicts of Interest:The authors declare no conﬂicts of interest.\nReferences\n1. Hello GPT-4o. Available online:https://openai.com/index/hello-gpt-4o/ (accessed on 13 August 2024).\n2. Vemprala, S.H.; Bonatti, R.; Bucker, A.; Kapoor, A. ChatGPT for Robotics: Design Principles and Model Abilities.IEEE Access\n2024, 12, 55682–55696. [CrossRef]\n3. Hu, Y.; Xie, Q.; Jain, V.; Francis, J.; Patrikar, J.; Keetha, N.; Kim, S.; Xie, Y.; Zhang, T.; Zhao, S.; et al. Toward General-Purpose\nRobots via Foundation Models: A Survey and Meta-Analysis.arXiv 2023, arXiv:2312.08782."
  },
  {
    "question": "What are the current advancements in robot intelligence research?",
    "chunk": "signiﬁcant lack of adaptability to dynamically changing environments. However, LLMs help a robot\nintelligence system to improve its generalization ability in dynamic and complex real-world environ-\nments. Indeed, ﬁndings from ongoing robotics studies indicate that LLMs can signiﬁcantly improve\nrobots’ behavior planning and execution capabilities. Additionally, vision-language models (VLMs),\ntrained on extensive visual and linguistic data for the vision question answering (VQA) problem,\nexcel at integrating computer vision with natural language processing. VLMs can comprehend visual\ncontexts and execute actions through natural language. They also provide descriptions of scenes\nin natural language. Several studies have explored the enhancement of robot intelligence using\nmultimodal data, including object recognition and description by VLMs, along with the execution\nof language-driven commands integrated with visual information. This review paper thoroughly\ninvestigates how foundation models such as LLMs and VLMs have been employed to boost robot\nintelligence. For clarity, the research areas are categorized into ﬁve topics: reward design in rein-\nforcement learning, low-level control, high-level planning, manipulation, and scene understanding.\nThis review also summarizes studies that show how foundation models, such as the Eureka model\nfor automating reward function design in reinforcement learning, RT-2 for integrating visual data,\nlanguage, and robot actions in vision-language-action models, and AutoRT for generating feasible\ntasks and executing robot behavior policies via LLMs, have improved robot intelligence.\nKeywords: embodied intelligence; foundation model; large language model (LLM); vision-language\nmodel (VLM); vision-language-action (VLA) model; robotics\n1. Introduction\nTo enhance the intelligence of robots in real-world environments that interact with\nhumans, developing robots capable of perceiving, acting, and interacting like humans is\na crucial goal. The recent advancements in large language models (LLMs) such as GPT-\n4o [1] have signiﬁcantly altered the ﬁeld of robotic AI research. These LLMs, trained on\nvast amounts of textual data, have shown excellent performance in enabling robots to\ncommunicate with humans more naturally and efﬁciently. Moreover, beyond the impacts\non human–robot interaction (HRI), there is ongoing research aimed at surpassing the\nlimitations of traditional low-level robot control techniques and planning algorithms by\nutilizing the high-level situational awareness and knowledge-based planning capabilities\nof LLMs. Notably, the programming capabilities of ChatGPT in the research presented by\nMicrosoft’s ChatGPT for Robotics [2] have introduced a new paradigm for applying LLMs\nin the robotics ﬁeld.\nThe goal of robot intelligence is to enable robots to operate autonomously in complex\nenvironments, interact naturally with humans, and make high-level decisions. To promote\nAppl. Sci.2024, 14, 8868.https://doi.org/10.3390/app14198868 https://www.mdpi.com/journal/applsciAppl. Sci.2024, 14, 8868 2 of 39\nadvancements in robot intelligence, the adoption of foundation models, such as LLMs and\nvision-language models (VLMs), which boast large parameter scales and pre-training on\nmassive datasets, is accelerating. These foundation models can perform various tasks, such\nas complex language understanding and generation and visual perception, enabling robots\nto engage with their environment in a more human-like manner.\nWhile traditional robot intelligence systems are highly effective in structured and pre-\ndictable environments, they are signiﬁcantly limited in their ability to adapt to dynamically\nchanging and complex real-world scenarios. In general, the intelligence models used in"
  },
  {
    "question": "How do assertion statements help in the task planning process?",
    "chunk": "Figure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 9.LLM-Planner is a system that creates high-level plans based on natural language commands,\nsets subgoals to determine actions, and continuously updates the plan to reﬂect environmental\nchanges [155].\nSingh [156], as shown in Figure10, introduced ProgPrompt, a programmatic LLM\nprompt structure designed for generating plans across diverse situated environments,\nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that\nleveraged LLMs and included a Python programming structure to facilitate information\nabout the environment and executable actions. It featured a feedback mechanism, using\nexecutable program plan examples and assertion statements to mitigate errors, enhancing\ntask success rates. Additionally, ProgPrompt veriﬁed the current state through environ-\nmental feedback during plan execution and revised the plan accordingly. The results\nindicated that the integration of programming language features substantially improved\ntask performance in contexts such as VirtualHome and real-world manipulation tasks in\nterms of success rate, goal conditions recall, and executability.Appl. Sci.2024, 14, 8868 21 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 10.ProgPrompt is a system that uses Python programming structures to provide environ-\nmental information and actions, enhancing the success rate of robot task planning through an error"
  },
  {
    "question": "What advantages does low-rank adaptation offer in fine-tuning?",
    "chunk": "diverges from adapter tuning by adding trainable prompt vectors to the input layer. Preﬁx\ntuning [117] entails appending a sequence of preﬁxes to each transformer layer of the\nlanguage model, which consists of trainable continuous vectors. During ﬁne-tuning, the\nmodel focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM\nmodel inference.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 9 of 39 \n \nresources. Under these conditions, parameter-eﬃcient ﬁne-tuning (PEFT) oﬀers a method \ndesigned to eﬃciently conduct ﬁne-tuning of such LLMs [112]. \nAmong the methods of PEFT, there are four major approaches as shown in Figure 3: \nadapter tuning, prompt tuning, preﬁx tuning, and low-rank adaptation (LoRA). Adapter \ntuning [113,114] involves integrating small neural network modules, known as adapters, \ninto the core components of a transformer model, speciﬁcally into the attention and feed-\nforward layers. These adapters are inserted serially following these layers, allowing ﬁne-\ntuning of only the adapter modules according to speciﬁc task goals, while the parameters \nof the original language model remain unchanged. Consequently, adapter tuning e ﬀec-\ntively reduces the number of trainable parameters. Additionally, prompt tuning [115,116] \ndiverges from adapter tuning by adding trainable prompt vectors to the input layer. Preﬁx \ntuning [117] entails appending a sequence of preﬁxes to each transformer layer of the lan-\nguage model, which consists of trainable continuous vectors. During ﬁne-tuning, the \nmodel focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM \nmodel inference. \n \nFigure 3. An overview of four strategies for parameter-eﬃcient ﬁne-tuning: (a) Adapter Tuning, \n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5]. \nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a \nlow-rank constraint on transformer layers to approximate the update matrices through \ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates \nthe parameter updates using low-rank dec omposition matrices. The primary bene ﬁt of \nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning, \nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory \nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning. \nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119]. \n3.3. Prompt Techniques for Increasing LLM Performance \nTo enhance the performance of LLMs, the most straightforward approach involves \ntraining with additional data via ﬁne-tuning techniques, which mirrors supervised learn-\ning in conventional machine learning. Another method for improving performance in-\nvolves the use of in-context learning, which capitalizes on prompts for zero-shot learning, \na capability ﬁrst observed in LLMs with the advent of GPT-3. The adaptation of these \nprompts for speciﬁc tasks is known as prompt engineering. Fundamentally, prompt engi-"
  },
  {
    "question": "How do LLMs generalize sequence transformations for robotics?",
    "chunk": "reward-labeled trajectories as context and incorporating online interaction, LLM-based\nagents could explore small grids and reﬁne simple trajectories using human-in-the-loop\nmethods, such as optimizing a CartPole controller.Appl. Sci.2024, 14, 8868 17 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 17 of 39 \n \nsequence transformation, the research demonstrated that LLMs could generalize speci ﬁc \nsequence transformations using benchmarks such as ARC (abstraction and reasoning \ncorpus) and PCFG (probabilistic context-free grammar), thereby proving their utility in \nspatial reasoning tasks for robotics. In sequence completion, the study examined whether \nLLMs could ﬁnish pa tterns in elementary functions (e.g., sinusoids), illustrating their \nutility in robotic tasks such as extending a wiping motion from kinesthetic demonstrations \nor creating drawings on a whiteboard. Finally, in sequence improvement, the research \nrevealed that by utilizing reward-labeled trajectories as context and incorporating online \ninteraction, LLM-based agents could explore small grids and re ﬁne simple trajectories \nusing human-in-the-loop methods, such as optimizing a CartPole controller.  \n \nFigure 6. Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed \nin sequence transformation, completion, and improvement [148]. \n4.3. High-Level Planning (Including Decision-Making and Reasoning) \nThe abstraction and generalization capabilities of LLMs oﬀer eﬀective methodologies \nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various \nresearch outcomes have been realized in the ﬁelds of planning, decision-making, \nreasoning, and behavior trees within robotics. \nYoneda [149] introduced Statler, a framew ork designed to provide LLMs with an \nexplicit world state representation through a continuously maintained ‘memory’. The core \nof Statler consisted of two components: the world model reader and the world model \nwriter. These components interacted with and sustained the world state. The world model \nreader interpreted user commands and generated executable code based on the current \nstate representation, while the world model wr iter updated the system’s state according \nto execution outcomes. By facilitating access to the world state ‘memory’, Statler improved \nLLMs’ ability to reason about planning task s with extended time horizons, overcoming \nlimitations imposed by context length. \nMu [150], shown in Figure 7, introduced EmbodiedGPT, a model speci ﬁcally \ndesigned for Embodied AI, which leverages LLMs. This framework processes visual \nobservations and natural language to establish long-term plans and execute tasks in real-\nFigure 6.Pre-trained LLMs can act as general sequence modelers, and their abilities were assessed in\nsequence transformation, completion, and improvement [148].\n4.3. High-Level Planning (Including Decision-Making and Reasoning)\nThe abstraction and generalization capabilities of LLMs offer effective methodologies\nfor high-level planning tasks in robotic systems. Leveraging these capabilities, various\nresearch outcomes have been realized in the ﬁelds of planning, decision-making, reasoning,\nand behavior trees within robotics.\nYoneda [149] introduced Statler, a framework designed to provide LLMs with an\nexplicit world state representation through a continuously maintained ‘memory’. The core\nof Statler consisted of two components: the world model reader and the world model\nwriter. These components interacted with and sustained the world state. The world model"
  },
  {
    "question": "How do models like Eureka and AutoRT contribute to robot learning?",
    "chunk": "signiﬁcant lack of adaptability to dynamically changing environments. However, LLMs help a robot\nintelligence system to improve its generalization ability in dynamic and complex real-world environ-\nments. Indeed, ﬁndings from ongoing robotics studies indicate that LLMs can signiﬁcantly improve\nrobots’ behavior planning and execution capabilities. Additionally, vision-language models (VLMs),\ntrained on extensive visual and linguistic data for the vision question answering (VQA) problem,\nexcel at integrating computer vision with natural language processing. VLMs can comprehend visual\ncontexts and execute actions through natural language. They also provide descriptions of scenes\nin natural language. Several studies have explored the enhancement of robot intelligence using\nmultimodal data, including object recognition and description by VLMs, along with the execution\nof language-driven commands integrated with visual information. This review paper thoroughly\ninvestigates how foundation models such as LLMs and VLMs have been employed to boost robot\nintelligence. For clarity, the research areas are categorized into ﬁve topics: reward design in rein-\nforcement learning, low-level control, high-level planning, manipulation, and scene understanding.\nThis review also summarizes studies that show how foundation models, such as the Eureka model\nfor automating reward function design in reinforcement learning, RT-2 for integrating visual data,\nlanguage, and robot actions in vision-language-action models, and AutoRT for generating feasible\ntasks and executing robot behavior policies via LLMs, have improved robot intelligence.\nKeywords: embodied intelligence; foundation model; large language model (LLM); vision-language\nmodel (VLM); vision-language-action (VLA) model; robotics\n1. Introduction\nTo enhance the intelligence of robots in real-world environments that interact with\nhumans, developing robots capable of perceiving, acting, and interacting like humans is\na crucial goal. The recent advancements in large language models (LLMs) such as GPT-\n4o [1] have signiﬁcantly altered the ﬁeld of robotic AI research. These LLMs, trained on\nvast amounts of textual data, have shown excellent performance in enabling robots to\ncommunicate with humans more naturally and efﬁciently. Moreover, beyond the impacts\non human–robot interaction (HRI), there is ongoing research aimed at surpassing the\nlimitations of traditional low-level robot control techniques and planning algorithms by\nutilizing the high-level situational awareness and knowledge-based planning capabilities\nof LLMs. Notably, the programming capabilities of ChatGPT in the research presented by\nMicrosoft’s ChatGPT for Robotics [2] have introduced a new paradigm for applying LLMs\nin the robotics ﬁeld.\nThe goal of robot intelligence is to enable robots to operate autonomously in complex\nenvironments, interact naturally with humans, and make high-level decisions. To promote\nAppl. Sci.2024, 14, 8868.https://doi.org/10.3390/app14198868 https://www.mdpi.com/journal/applsciAppl. Sci.2024, 14, 8868 2 of 39\nadvancements in robot intelligence, the adoption of foundation models, such as LLMs and\nvision-language models (VLMs), which boast large parameter scales and pre-training on\nmassive datasets, is accelerating. These foundation models can perform various tasks, such\nas complex language understanding and generation and visual perception, enabling robots\nto engage with their environment in a more human-like manner.\nWhile traditional robot intelligence systems are highly effective in structured and pre-\ndictable environments, they are signiﬁcantly limited in their ability to adapt to dynamically\nchanging and complex real-world scenarios. In general, the intelligence models used in"
  },
  {
    "question": "What types of tasks can be addressed through instruction tuning?",
    "chunk": "quence and sequentially predicts output tokens individually. Examples of preﬁx decoder-\nbased LLMs include GLM-130B [108] and U-PaLM [109]. Additionally, various architec-\ntures have been proposed to address e ﬃciency challenges during training or inference \nwith long inputs, due to the quadratic computational complexity of the traditional trans-\nformer architecture. For instance, the Mixture-of-Experts (MoE) scaling method [34] \nsparsely activates a subset of the neural network for each input. \n \nFigure 2. Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx De-\ncoder (middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectan-\ngles represent attention between preﬁx tokens, attention between preﬁx and target tokens, atten-\ntion between target tokens, and masked attention [5]. \nIn terms of the tuning of LLMs, these models are essentially pre-trained on massive \ndatasets and require ﬁne-tuning for diﬀerent application domains. However, the consid-\nerable model size and number of parameters pose challenges for ﬁne-tuning on standard \ncomputers and GPUs. The subsequent sections will discuss methods to address these chal-\nlenges. \nLLM tuning is broadly divided into two cate gories based on the training objective. \nInstruction tuning is a form of supervised learning where the training data typically in-\nclude descriptions of tasks, inputs, and corresponding outputs. This type of tuning is de-\nsigned (1) to enhance the functional capabilities of LLMs, (2) to specialize them by training \nwith discipline-speci ﬁc information, and (3) to improve task generalization and con-\nsistency through a be tter understanding of natural language commands. Conversely, \nalignment tuning (or preference alignment) seeks to align the behavior of LLMs with hu-\nman values and preferences. Prominent methods include reinforcement learning from hu-\nman feedback (RLHF) [110], which involves ﬁne-tuning LLMs using human feedback to \nbetter reﬂect human values, and direct preference optimization (DPO) [111], focusing on \ntraining with pairs of human preferences th at usually include an input prompt and the \npreferred and non-preferred responses. \nFor both instruction tuning and alignment tuning, which involve training LLMs with \nextensively large model para meters, substantial GPU memo ry and computational re-\nsources are required, with high costs typi cally incurred when utilizing cloud-based \nFigure 2.Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx Decoder\n(middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectangles\nrepresent attention between preﬁx tokens, attention between preﬁx and target tokens, attention\nbetween target tokens, and masked attention [5].\nIn terms of the tuning of LLMs, these models are essentially pre-trained on mas-\nsive datasets and require ﬁne-tuning for different application domains. However, the\nconsiderable model size and number of parameters pose challenges for ﬁne-tuning on\nstandard computers and GPUs. The subsequent sections will discuss methods to address\nthese challenges.\nLLM tuning is broadly divided into two categories based on the training objective.\nInstruction tuning is a form of supervised learning where the training data typically include\ndescriptions of tasks, inputs, and corresponding outputs. This type of tuning is designed\n(1) to enhance the functional capabilities of LLMs, (2) to specialize them by training with"
  },
  {
    "question": "What limitations does the framework address in robotic task execution?",
    "chunk": "framework to assist robots in comprehendin g incomplete natural language instructions. \nThis framework enabled robots to receive instructions in natural language from humans, \nobserve their surroundings, and employ a commonsense reasoning method to \nautonomously infer missing information. LMCR utilized a model of commonsense \nreasoning learned from web-based text materials, allowing robots to understand \nincomplete instructions and autonomously execute tasks. The framework comprised three \nmain functions: language understanding, commonsense reasoning, and action planning. \nIn language understanding, LMCR translated human natural language instructions into a \nform interpretable by robots, parsing them into verb frames to convert them into \nexecutable structures. During the common sense reasoning phase, the robot analyzed \nsurrounding objects and employed a language model trained on large-scale unstructured \ntext materials to ﬁll in the missing details from the instructions. This model identiﬁed the \nFigure 7.After encoding visual features, they are mapped using visual tokens and text queries. A\nplan is then created with the LLaMA model and turned into task commands. The visual tokens are\nqueried and converted into low-level control commands to perform the task [150].\nChen [151] introduced the language-model-based commonsense reasoning (LMCR)\nframework to assist robots in comprehending incomplete natural language instructions.\nThis framework enabled robots to receive instructions in natural language from humans, ob-\nserve their surroundings, and employ a commonsense reasoning method to autonomously\ninfer missing information. LMCR utilized a model of commonsense reasoning learned\nfrom web-based text materials, allowing robots to understand incomplete instructions and\nautonomously execute tasks. The framework comprised three main functions: language\nunderstanding, commonsense reasoning, and action planning. In language understanding,\nLMCR translated human natural language instructions into a form interpretable by robots,\nparsing them into verb frames to convert them into executable structures. During the\ncommonsense reasoning phase, the robot analyzed surrounding objects and employed a\nlanguage model trained on large-scale unstructured text materials to ﬁll in the missing\ndetails from the instructions. This model identiﬁed the most suitable verb frame to com-\nplete the gaps. Subsequently, based on the completed verb frame, the robot formulated its\nactions using predeﬁned action plans for each verb to guide the movements of the robot\narm and execute the assignment. Experimental results showed that LMCR demonstrated\nsuperior generalization performance for novel concepts not presented in the training set\nand surpassed GCNGrasp, which depends on a predeﬁned graph structure for all concepts\nand their relationships. This indicated that LMCR was an effective tool, combining the se-\nmantic reasoning capabilities of language models with planning that adapted to the robot’s\nspeciﬁc environment and context, effectively managing complex and prolonged tasks.\nHuang [152] introduced a methodology named grounded decoding (GD), which offers\na method for generating LLM-based robot action plans. These plans enable robots to execute\nlong-term tasks across diverse physical environments. The methodology encompasses two\nprimary elements: linking the text generated by the language model to actionable taskAppl. Sci.2024, 14, 8868 19 of 39\ncommands in the physical world via GD and adjusting the tokens generated by the LLM\nto real-world conditions to formulate feasible commands. This approach synergizes the\nhigh-level semantic reasoning of LLMs with plans that are aligned with the robot’s physical\nenvironment and capabilities, thus facilitating the execution of complex and long-term\ntasks. The method addresses several limitations robots face in performing complex, long-\nterm tasks, such as a lack of physical world experience, an inability to process non-verbal"
  },
  {
    "question": "How can LLMs improve their understanding of natural language commands?",
    "chunk": "quence and sequentially predicts output tokens individually. Examples of preﬁx decoder-\nbased LLMs include GLM-130B [108] and U-PaLM [109]. Additionally, various architec-\ntures have been proposed to address e ﬃciency challenges during training or inference \nwith long inputs, due to the quadratic computational complexity of the traditional trans-\nformer architecture. For instance, the Mixture-of-Experts (MoE) scaling method [34] \nsparsely activates a subset of the neural network for each input. \n \nFigure 2. Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx De-\ncoder (middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectan-\ngles represent attention between preﬁx tokens, attention between preﬁx and target tokens, atten-\ntion between target tokens, and masked attention [5]. \nIn terms of the tuning of LLMs, these models are essentially pre-trained on massive \ndatasets and require ﬁne-tuning for diﬀerent application domains. However, the consid-\nerable model size and number of parameters pose challenges for ﬁne-tuning on standard \ncomputers and GPUs. The subsequent sections will discuss methods to address these chal-\nlenges. \nLLM tuning is broadly divided into two cate gories based on the training objective. \nInstruction tuning is a form of supervised learning where the training data typically in-\nclude descriptions of tasks, inputs, and corresponding outputs. This type of tuning is de-\nsigned (1) to enhance the functional capabilities of LLMs, (2) to specialize them by training \nwith discipline-speci ﬁc information, and (3) to improve task generalization and con-\nsistency through a be tter understanding of natural language commands. Conversely, \nalignment tuning (or preference alignment) seeks to align the behavior of LLMs with hu-\nman values and preferences. Prominent methods include reinforcement learning from hu-\nman feedback (RLHF) [110], which involves ﬁne-tuning LLMs using human feedback to \nbetter reﬂect human values, and direct preference optimization (DPO) [111], focusing on \ntraining with pairs of human preferences th at usually include an input prompt and the \npreferred and non-preferred responses. \nFor both instruction tuning and alignment tuning, which involve training LLMs with \nextensively large model para meters, substantial GPU memo ry and computational re-\nsources are required, with high costs typi cally incurred when utilizing cloud-based \nFigure 2.Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx Decoder\n(middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectangles\nrepresent attention between preﬁx tokens, attention between preﬁx and target tokens, attention\nbetween target tokens, and masked attention [5].\nIn terms of the tuning of LLMs, these models are essentially pre-trained on mas-\nsive datasets and require ﬁne-tuning for different application domains. However, the\nconsiderable model size and number of parameters pose challenges for ﬁne-tuning on\nstandard computers and GPUs. The subsequent sections will discuss methods to address\nthese challenges.\nLLM tuning is broadly divided into two categories based on the training objective.\nInstruction tuning is a form of supervised learning where the training data typically include\ndescriptions of tasks, inputs, and corresponding outputs. This type of tuning is designed\n(1) to enhance the functional capabilities of LLMs, (2) to specialize them by training with"
  },
  {
    "question": "Can you summarize the main advancements in reinforcement learning discussed in the document?",
    "chunk": "main randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 5.DrEureka leverages LLM to design reward functions and solves the sim-to-real problem\nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134].\nXie [136] introducedText2Reward, a framework that automatically generated dense\nreward functions for reinforcement learning using LLMs. Provided with a goal expressed\nin natural language, Text2Reward produced executable dense reward functions derived\nfrom a compact representation of the environment. This framework generated free-form\ndense reward codes and delivered performance comparable to or surpassing that of policies\ntrained with expert-designed codes across a variety of tasks, including 17 manipulator-\nrelated tasks and six novel locomotion behaviors. Additionally, Text2Reward incorporated\nuser feedback to iteratively enhance the generated reward functions, thereby increasing the\nsuccess rate of the learned policies.\nDi Palo [137] explored the use of LLMs and VLMs to improve reinforcement learning\nagents’ understanding of human intentions. They developed a framework that utilized\nlanguage as a primary inference tool, investigating how it could address key challengesAppl. Sci.2024, 14, 8868 14 of 39\nin reinforcement learning, such as efﬁcient exploration, data reuse in experience, skill\nscheduling, and observational learning. This framework employed LLMs and VLMs to\naddress these reinforcement learning challenges by (1) efﬁciently exploring environments\nwith sparse rewards, (2) reusing collected data to sequentially bootstrap the learning of\nnew tasks, (3) scheduling learned skills for novel tasks, and (4) acquiring knowledge from\nobserving expert agents.\nDu [138] developed success detectors that identiﬁed whether actions or tasks were\nsuccessfully completed, utilizing the large multimodal language model Flamingo and\nhuman reward annotations. The study on success detection spanned three distinct do-\nmains: (1) interactive language-conditioned agents in simulated households, (2) real-world\nrobotic manipulation tasks (inserting and removing small, medium, and large gears), and\n(3) “in-the-wild” human egocentric videos. These success detectors adapted to new lan-\nguage instructions and visual changes using VLMs such as Flamingo, which were trained\non a broad range of language and visual data. Furthermore, success detection was reframed\nas a VQA problem, enabling the tracking of task progress through multiple frames to ascer-\ntain whether tasks had been successfully completed. The proposed method proved to be\nmore accurate in detecting success compared to custom reward models in the ﬁrst two do-\nmains, even with new language instructions or visual changes. However, success detection\nin unseen real-world videos in the third domain posed a more challenging generalization\ntask, underscoring the need for additional research."
  },
  {
    "question": "How do LLMs help in decomposing navigation instructions?",
    "chunk": "Zhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow\ninstructions. NavGPT is a vision-language navigation system that employs an LLM to trans-\nlate visual inputs from a visual foundation model (VFM) into natural language. The LLM\nthen interprets the current state and makes informed decisions to reach the intended goal,\nbased on these converted visuals, navigation history, and potential future routes. NavGPT\nconducts various functions, including high-level planning, decomposing instructions into\nsub-goals, identifying landmarks in observed scenes, monitoring navigation progress, and\nmodifying plans as necessary. Although NavGPT’s performance on zero-shot tasks from\nthe R2R dataset has not yet matched that of trained models, it underscored the potential\nof utilizing multi-modality inputs with LLMs for visual navigation and tapping into the\nexplicit reasoning capabilities of LLMs to enhance learned models.\nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-\ntrained vision-language features with a 3D reconstruction of the physical world. VLMaps,\nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary\nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands.\nThese commands can be directly localized on a map and generate new obstacle maps in real-Appl. Sci.2024, 14, 8868 27 of 39\ntime, facilitated by sharing among various robot types. Extensive experiments conducted in\nboth simulated environments (using the Habitat simulator with the Matterport3D dataset\nand the AI2THOR simulator) and real-world settings (with the HSR mobile robot for\nindoor navigation) demonstrated that VLMs can navigate based on more complex language\ninstructions than previous methods. The reviewed papers in this study are summarized in\nTable 5.\nTable 5.Summary of the reviewed papers in this study.\nName Explanation Ref.\nReward Design in RL\n• Eureka automatically generates and improves reward\nfunctions based on the virtual environment source\ncode.\n• DrEureka builds reward-aware physics priors using\nEureka and supports effective operation in the real\nworld through domain randomization.\n• LLMs design and reﬁne reward functions based on\nnatural language input.\n• LLMs and VLMs integrate multimodal data to\ngenerate reward functions.\n[11,134,136–139,176–180]\nLow-level\nControl\n• Generating commands to control actuators capable of\nlow-level control.\n• RT-1 and RT-2 enable robots to perform complex tasks\nbased on language-vision data.\n• AutoRT establishes a system where robots can\nautonomously collect and utilize data.\n[8–10,144–148,181–183]\nHigh-level\nPlanning\n• LLMs provide an effective methodology for tasks\nrelated to high-level planning within robotic systems.\n• By using natural language, LLMs can formulate plans\nto solve tasks that require long-horizon reasoning.\n• LLMs assess the feasibility of actions to determine and\nexecute the optimal robotic behavior.\n• LLMs generate behavior trees to structure complex\nrobotic actions accurately.\n[149–160,184–207]\nManipulation\n• Using LLMs and VLMs to integrate language and\nvision data allows various manipulations.\n• LLMs interpret high-level instructions to generate the\nnecessary robot actions and assess their feasibility.\n• VLMs extract object information from images to assist\nin performing manipulations.\n[161–167,208–215]\nScene\nUnderstanding\n• To solve VQA problems, use VLMs to extract\nhigh-level information from vision data."
  },
  {
    "question": "What enhancements were made to EmbodiedGPT's training mode?",
    "chunk": "state representation, while the world model writer updated the system’s state according to\nexecution outcomes. By facilitating access to the world state ‘memory’, Statler improved\nLLMs’ ability to reason about planning tasks with extended time horizons, overcoming\nlimitations imposed by context length.\nMu [150], shown in Figure7, introduced EmbodiedGPT, a model speciﬁcally designed\nfor Embodied AI, which leverages LLMs. This framework processes visual observations\nand natural language to establish long-term plans and execute tasks in real-time. Em-\nbodiedGPT utilizes pre-trained vision transformers and the LLaMA language model to\nencode visual features and map them to the language modality. The generated plan was\nsubsequently converted into speciﬁc task commands using general visual tokens, encoded\nby the vision model. The framework’s functionality comprises (1) encoding current visual\nfeatures, (2) mapping visual features to the language modality via attention-based interac-\ntions between visual tokens and text queries or learnable embedded queries, (3) generating\nplans with the LLaMA language model and translating them into speciﬁc task commands,\nand (4) querying the encoded visual tokens from the vision model and translating them\ninto low-level control commands through a downstream policy network for task execu-\ntion. Experimental results, utilizing the MS-COCO dataset, revealed that EmbodiedGPT\nexcels in object recognition and understanding spatial relationships. Notably, implement-Appl. Sci.2024, 14, 8868 18 of 39\ning a closed-loop design and a “chain-of-thought” training mode signiﬁcantly enhanced\nEmbodiedGPT’s performance. These results demonstrate that EmbodiedGPT effectively\nhandles various autonomous tasks, exhibiting superior capability in object recognition,\nunderstanding spatial relationships, and generating logical, executable plans.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 18 of 39 \n \ntime. EmbodiedGPT utilizes pre-trained visi on transformers and the LLaMA language \nmodel to encode visual features and map them  to the language modality. The generated \nplan was subsequently converted into speci ﬁc task commands using general visual \ntokens, encoded by the vision model. The framework’s functionality comprises (1) \nencoding current visual features, (2) mapping visual features to the language modality \nvia a ttention-based interactions between visual tokens and text queries or learnable \nembedded queries, (3) generating plans with the LLaMA language model and translating \nthem into speciﬁc task commands, and (4) querying the encoded visual tokens from the \nvision model and translating them into low-level control commands through a \ndownstream policy network for task execution. Experimental results, utilizing the MS-\nCOCO dataset, revealed that EmbodiedG PT excels in object recognition and \nunderstanding spatial relationships. Notabl y, implementing a closed-loop design and a \n“chain-of-thought” training mode signi ﬁcantly enhanced EmbodiedGPT’s performance. \nThese results demonstrate that EmbodiedGPT e ﬀectively handles various autonomous \ntasks, exhibiting superior capability in object recognition, understanding spatial \nrelationships, and generating logical, executable plans. \n \nFigure 7. After encoding visual features, they are mappe d using visual tokens and text queries. A \nplan is then created with the LLaMA model and turned into task commands. The visual tokens are \nqueried and converted into low-level control commands to perform the task [150]. \nChen [151] introduced the language-model-based commonsense reasoning (LMCR)"
  },
  {
    "question": "What environments were tested for ELLM's performance?",
    "chunk": "Du [139] introduced theELLM (exploring with LLMs) framework, which provided\nguidelines for pre-training reinforcement learning using LLMs. ELLM utilized the natural\nlanguage processing capabilities of LLMs to deﬁne goals and furnish reward functions for\nreinforcement learning agents. This strategy enabled agents to undertake meaningful explo-\nration and learning within their environments. The paper assessed ELLM’s performance in\ntwo settings: Crafter, a 2D version of Minecraft, and Housekeep, involving the task of rear-\nranging household objects. Experimental results demonstrated that ELLM surpassed other\nmethods in both settings. In the Crafter setting, ELLM attained high performance through\ngoal-oriented learning, proving especially effective in scenarios with sparse reward signals.\nIn the Housekeep setting, the agent conducted sensible exploration by adhering to goals\nset by the LLM, achieving a high success rate. While the accuracy of goal setting by the\nLLM varied with the objects and locations, it generally showed high performance. These\nexperimental ﬁndings suggested that ELLM was successful in enhancing reinforcement\nlearning performance across diverse environments, highlighting the vital role of providing\nreward signals based on human commonsense.\n4.2. Low-Level Control\nResearch is also being conducted on generating commands that directly control a\nrobot’s actuators (i.e., enabling low-level control) through various applications of LLM\nmodels. Among these projects, the Google research team developed RT-1 [8], which consists\nof ﬁlm-conditioned EfﬁcientNet-B3, TokenLearner, and Transformer. RT-1 is a model that\nreceives images and natural language instructions at a rate of 3Hz and outputs discretized\nrobot actions. RT-1 was trained on a vast demo dataset with over 130k episodes from more\nthan 700 tests collected over 17 months using 13 robots.\nA notable feature of RT-1 is its ability to enhance performance by learning from\ndata gathered from heterogeneous robots or simulations. In the study [8], the authors\nevaluated the performance of a model trained exclusively on data from the EveryDay\nRobot (EDR) against a model trained using data from both EDR and Kuka IIWA robots.\nThey recorded a 12% improvement in the bin-picking test. Another experiment compared\nmodel performance using data from real environments and simulations for items not\nencountered in actual settings. The ﬁndings indicated that incorporating simulation data in\nRT-1 training enhances performance over using purely real environment data, suggesting\nthat RT-1 can substantially improve model performance by integrating diverse data from\nrobots of varied morphologies or simulations while sustaining existing task capabilities.Appl. Sci.2024, 14, 8868 15 of 39\nRT-2 [9] is deﬁned as a vision-language-action (VLA) model that facilitates ﬁne-grained\ncontrol of robots through vision and language commands. RT-2 is ﬁne-tuned with robotic\ntrajectory data based on VLM models such as PaLM-E [140], which has 12 billion parameters\nand is trained on VQA data, alongside PaLI-X [141], which has parameter sizes ranging\nfrom 5 billion to 55 billion. The RT-2 system operates as an integrated closed-loop robotic\nsystem that combines low-level and high-level control policies. Despite not explicitly\nlearning certain capabilities during pre-training, RT-2 exhibits improved task performance\nvia real-world generalization involving diverse objects, visual scenes, and instructional\ncontexts. The paper [9] quantitatively assesses RT-2’s emergent capabilities in areas such as\nreasoning, symbol understanding, and human recognition. Furthermore, applying chain-"
  },
  {
    "question": "What is the role of the LLaMA model in the EmbodiedGPT framework?",
    "chunk": "state representation, while the world model writer updated the system’s state according to\nexecution outcomes. By facilitating access to the world state ‘memory’, Statler improved\nLLMs’ ability to reason about planning tasks with extended time horizons, overcoming\nlimitations imposed by context length.\nMu [150], shown in Figure7, introduced EmbodiedGPT, a model speciﬁcally designed\nfor Embodied AI, which leverages LLMs. This framework processes visual observations\nand natural language to establish long-term plans and execute tasks in real-time. Em-\nbodiedGPT utilizes pre-trained vision transformers and the LLaMA language model to\nencode visual features and map them to the language modality. The generated plan was\nsubsequently converted into speciﬁc task commands using general visual tokens, encoded\nby the vision model. The framework’s functionality comprises (1) encoding current visual\nfeatures, (2) mapping visual features to the language modality via attention-based interac-\ntions between visual tokens and text queries or learnable embedded queries, (3) generating\nplans with the LLaMA language model and translating them into speciﬁc task commands,\nand (4) querying the encoded visual tokens from the vision model and translating them\ninto low-level control commands through a downstream policy network for task execu-\ntion. Experimental results, utilizing the MS-COCO dataset, revealed that EmbodiedGPT\nexcels in object recognition and understanding spatial relationships. Notably, implement-Appl. Sci.2024, 14, 8868 18 of 39\ning a closed-loop design and a “chain-of-thought” training mode signiﬁcantly enhanced\nEmbodiedGPT’s performance. These results demonstrate that EmbodiedGPT effectively\nhandles various autonomous tasks, exhibiting superior capability in object recognition,\nunderstanding spatial relationships, and generating logical, executable plans.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 18 of 39 \n \ntime. EmbodiedGPT utilizes pre-trained visi on transformers and the LLaMA language \nmodel to encode visual features and map them  to the language modality. The generated \nplan was subsequently converted into speci ﬁc task commands using general visual \ntokens, encoded by the vision model. The framework’s functionality comprises (1) \nencoding current visual features, (2) mapping visual features to the language modality \nvia a ttention-based interactions between visual tokens and text queries or learnable \nembedded queries, (3) generating plans with the LLaMA language model and translating \nthem into speciﬁc task commands, and (4) querying the encoded visual tokens from the \nvision model and translating them into low-level control commands through a \ndownstream policy network for task execution. Experimental results, utilizing the MS-\nCOCO dataset, revealed that EmbodiedG PT excels in object recognition and \nunderstanding spatial relationships. Notabl y, implementing a closed-loop design and a \n“chain-of-thought” training mode signi ﬁcantly enhanced EmbodiedGPT’s performance. \nThese results demonstrate that EmbodiedGPT e ﬀectively handles various autonomous \ntasks, exhibiting superior capability in object recognition, understanding spatial \nrelationships, and generating logical, executable plans. \n \nFigure 7. After encoding visual features, they are mappe d using visual tokens and text queries. A \nplan is then created with the LLaMA model and turned into task commands. The visual tokens are \nqueried and converted into low-level control commands to perform the task [150]. \nChen [151] introduced the language-model-based commonsense reasoning (LMCR)"
  },
  {
    "question": "How did Tang's approach enhance robot movement through foot contact patterns?",
    "chunk": "of-thought prompting techniques to RT-2 has proven effective in solving more complex\nsemantic inference tasks, such as using a rock as an improvised hammer or offering an\nenergy drink instead of a carbonated beverage to a thirsty person. In comparison with\nthe earlier study on RT-1, RT-2 demonstrates enhanced performance in both familiar and\nnovel tasks.\nAutoRT [10] is a follow-up study based on the research results of RT-1 and RT-2,\nestablishing an orchestration of large-scale robotic agents for data collection in real-world\nscenarios. AutoRT employed 53 robots to gather 77,000 real robot episodes over seven\nmonths through both teleoperation and autonomous robot policies. At the heart of AutoRT\nis a robust foundation model that generates ‘task proposals’ based on given visual observa-\ntions. Notably, AutoRT introduces a ‘Robot Constitution’ using constitutional prompting to\nensure actions during the task proposal process do not compromise the safety of the robot\nor nearby individuals. This Robot Constitution, inspired by Asimov’s three laws [142],\ncomprises basic rules, safety rules that identify unsafe or unwanted tasks, and embodiment\nrules that clarify the robot’s operational boundaries.\nAutoRT enhances data collection by initially scanning the surroundings to identify\ninteresting scenes or tasks (exploration). It interprets the given context through a VLM\nand proposes potential tasks via an LLM (task generation). Subsequently, tasks suggested\nby the LLM are screened (affordance) to assess their feasibility and the need for human\nintervention, employing the Robot Constitution. During this procedure, viable tasks are\nchosen and performed, while pertinent data are gathered (data collection). The collected\ndata are then assessed for (diversity scoring) the visual diversity of the robot trajectories\nand the linguistic diversity of the language instructions generated by AutoRT (LLM). The\naim of this diversity evaluation is to conﬁrm that, unlike simulations, real-world data\ncollection by robots is labor-intensive, making it essential to gather data across a broad\nspectrum of tasks. Experimental outcomes illustrate that AutoRT achieves higher visual\nand linguistic diversity compared to RT-1 or BC-Z [143].\nOther researchers include Tang [144], who developed an approach that connects\nnatural language user commands with a locomotion controller using foot contact patterns as\nan interface for low-level commands. This innovative interface translates human commands\ninto the robot’s foot contact patterns, allowing the robot to move at a speciﬁed speed with\nprecise timing for each foot’s contact with the ground. To achieve this, the robot used a\ncyclic sliding window to extract foot contact ﬂags from a pattern template, thus generating\nthe required foot contact patterns. During training, a random pattern generator created\nfoot contact patterns, and during testing, an LLM translated human commands into these\npatterns. The robot then adjusted its movements based on the foot contact patterns it\nlearned through deep reinforcement learning, closely adhering to the intended foot contact\npatterns and speed commands. This approach demonstrated a 50% higher success rate in\ntask evaluation (across 30 tasks, including standing still) compared to two baselines (which\nemployed discrete gaits and sinusoidal functions as interfaces), successfully solving 10\nmore tasks than the baselines.\nMandi [145] introduced a novel method for multi-robot collaboration that utilizes\nLLMs for both high-level communication and low-level path planning. In this method, the\nrobots employ the LLM to discuss and reason about task strategies. They generate sub-taskAppl. Sci.2024, 14, 8868 16 of 39\nplans and task space waypoint paths, which a multi-arm motion planner then uses to expe-"
  },
  {
    "question": "What is automatic prompt engineering and how does it work?",
    "chunk": "demonstrations and calls on external tools as necessary to integrate their outputs into the\nreasoning process. The model generalizes from demonstrations using tools to decompose\nnew tasks and learns to use tools effectively. Enhancing ART’s performance is possible\nby modifying the task library or incorporating new tools.Automatic prompt engineer\n(APE) [129] is a framework designed for the automatic generation and selection of com-\nmands. The model generates command candidates for a problem and selects the most\nsuitable one based on a scoring function, such as execution accuracy or log probability.\nDirectional stimulus prompting[130] is a technique that directs the model to consider\nand generate responses in a particular direction. By deploying a tunable policy LM (e.g.,\nT5 [47]), it creates directional stimulus prompts for each input and uses these as cues to\nsteer the model toward producing the desired outcomes [131]. ReAct combines reasoning\nwith action within the model. It enables the model to perform reasoning in generating\nanswers, take actions based on external sources (e.g., documents, articles, and news), and\nreﬁne reasoning based on observations of these actions. This process facilitates the creation,\nmaintenance, and modiﬁcation of action plans while incorporating additional information\nfrom interactions with external sources.Reﬂexion [132] augments language-based agents\nwith language feedback. Reﬂexion involves three models: the actor, the evaluator, and self-\nreﬂection. The actor initiates actions within a speciﬁc environment to generate task steps,\nthe evaluator assesses these steps, and self-reﬂection provides linguistic feedback, which\nthe actor uses to formulate new steps and achieve the task’s objective. The introduced\nprompt techniques are summarized in Table4.\nTable 4.Prompt Techniques.\nName Explanation Ref.\nZero-Shot Prompting Enabling the model to perform new tasks without any examples [ 53]\nFew-Shot Prompting Providing a few examples to enable performing new tasks [ 49]\nChain-of-Thought Explicitly generating intermediate reasoning steps to perform\nstep-by-step inference [41]\nSelf-Consistency\nGenerating various reasoning paths independently through\nFew-Shot CoT, with each path going through a prompt generation\nprocess to select the most consistent answer\n[120]\nGenerated Knowledge Prompting\nIntegrating knowledge and information relevant to a question, and\nthen providing it along with the question to generate accurate\nanswers\n[126]\nPrompt Chaining Dividing a task into sub-tasks and connecting prompts for each\nsub-task as input–output pairs [125]\nTree of Thoughts\nDividing a problem into subproblems with intermediate steps that\nserve as “thoughts” towards solving the problem, where each\nthought undergoes an inference process and self-evaluates its\nprogress towards solving the problem\n[124]\nRetrieval Augmented Generation Combining external information retrieval with natural language\ngeneration [127]\nAutomatic Reasoning and Tool-use Using external tools to automatically generate intermediate\nreasoning steps [128]\nAutomatic Prompt Engineer Automatically generating and selecting commands [ 129]\nActive Prompt Addressing the issue that the effectiveness may be limited by\nhuman annotations [122]\nDirectional Stimulus Prompting Guiding the model to think and generate responses in a speciﬁc\ndirection [130]\nProgram-Aided Language Models Using models to understand natural language problems and\ngenerate programs as intermediate reasoning steps [123]\nReAct Combining reasoning and actions within a mode [ 131]\nReﬂexion Enhancing language-based agents through language feedback [ 132]"
  },
  {
    "question": "How does the monitoring framework help detect anomalies in robot perception?",
    "chunk": "representation for natural language queries; then, an LLM-based object suggestion module\nreviews instructions, suggests relevant objects, and queries the scene for object availability\nand location. Using this information, the LLM planner devises plans uniquely tailored to\nthe scene’s context. NLMap equips robots with the ability to function without a predeﬁned\ncatalog of objects or actions, overcoming the constraints of earlier methods and enabling\nmore adaptable operations in environments with novel or absent objects.\nElhafsi [171] introduced a monitoring framework that employed an LLM with superior\ncontextual understanding and reasoning capabilities to detect edge cases and anomalies\nwithin vision-based policies. This framework monitored the robot’s perception stream\nthrough an LLM-based module, designed to detect semantic anomalies that might occur\nduring operations. By converting the robot’s visual observations into textual descriptions\nat regular intervals and integrating these into LLM prompts, it could pinpoint factors\nleading to policy errors, unsafe behavior, or task confusion. The conversion of visual\ninformation into natural language descriptions used various techniques, without restriction\nto any speciﬁc method. This ﬂexibility enabled both fully end-to-end policies and classical\nautonomy stacks using learned perception to align more closely with human intuition.\nThe ﬁndings indicated that semantic anomalies did not always correspond to semantically\nexplainable failures, and end-to-end policies could sometimes behave unpredictably.\nHon [172] introduced a new model family named 3D-LLM, which incorporated 3D\nworld information into LLMs. The 3D-LLM model utilized 3D point clouds and their\nfeatures as input, enabling it to handle a variety of spatially aware 3D tasks. These tasks\nincluded 3D captioning, dense captioning, 3D question answering, task decomposition, 3D\ngrounding, 3D-assisted dialogue, and navigation. The model used a 3D feature extractor to\nalign 3D features from multi-view images with language features, facilitating more precise\ntext generation and question answering based on spatial understanding. To train 3D-LLM,\na pre-trained 2D VLM formed the backbone, enhanced by the addition of 3D positional\nembeddings to better capture 3D spatial information. The model generated location tokens\nthrough linguistic descriptions of speciﬁc objects and was trained using 3D features as input.\nExperimental results showed that 3D-LLM excelled in various 3D-related tasks, achieving\napproximately a 9% higher BLEU-1 score compared to previous models on the ScanQA\ndataset. It demonstrated superior performance in 3D captioning, task composition, andAppl. Sci.2024, 14, 8868 26 of 39\n3D-assisted dialogue, outperforming 2D VLMs and displaying an improved understanding\nof object locations, shapes, and interactions.\nIn the extension of scene understanding using VLMs, the keyword VLN (vision-\nand-language navigation) is widely used in navigation-related research, where language\nfoundation models are increasingly utilized.\nShah [173], as shown in Figure13, introduced a robotic navigation system named\nLM-Nav, which capitalized on the advantages of training with large, unlabeled trajectory\ndatasets while providing a high-level interface for users. LM-Nav utilized three large-scale\npre-trained models: ViNG, CLIP, and GPT-3. Initially, the LLM translated natural language\ninstructions into a sequence of textual landmarks. The VLM integrated these textual\nlandmarks with images to identify the relevant images through probabilistic distribution.\nSubsequently, the VNM utilized these landmarks to plan and execute robot trajectories"
  },
  {
    "question": "What is the impact of commonsense reasoning on robotic planning?",
    "chunk": "• For scene understanding, estimate and identify objects\nand evaluate relationships between objects.\n• For navigation, convert natural language instructions\nand combine them with vision data to identify the\nimage through probability distributions.\n[168–175,216–223]\n5. Discussion and Future Directions\nThe review revealed two potentials of foundation models: (1) commonsense reasoning\nfor planning and (2) the ability to generate code.\nThe ﬁrst ﬁnding from this review study is the potential to enhance robot intelligence\nthrough foundation models. Beyond the studies mentioned here, numerous recent stud-\nies have shown that pre-trained models such as LLMs and VLMs can enhance various\naspects of robot intelligence, such as situational awareness, high-level task planning, and\nhuman interaction. LLMs allow communication with humans in natural languages, objectAppl. Sci.2024, 14, 8868 28 of 39\nutilization based on extensive information, and high-level planning using that information.\nVLMs can describe tasks in text and understand visual information. Furthermore, the\ninformation from VLMs can be supplemented by connecting to knowledge databases via\nLLMs. These capabilities are crucial for enhancing robot intelligence, broadening the scope\nof robot applications, and maximizing robot utility.\nThe second ﬁnding is the code generation capability of LLMs, which has the potential\nto automate the robot development process traditionally performed by humans. Addi-\ntionally, robots that can autonomously update their own algorithms are no longer just\nscience ﬁction. Although limitations exist for robots to self-update, frameworks such as\nEureka and DrEureka, which automatically enhanced reinforcement learning performance\nfor robot motion control, demonstrate the potential for future advancements. This suggests\nthat LLMs may not only enhance human interactions but could also pave the way for\nself-improvement without human intervention.\nWhile foundation models offer considerable potential for advancing robotics intelli-\ngence, several limitations and future considerations remain. These include (1) the speed\nof inference required for real-time applications, (2) the computational efﬁciency necessary\nfor embedded systems, (3) the ability to handle multi-modality information, and (4) the\nnecessity of addressing safety and ethical considerations.\nFirst of all, LLMs and VLMs hold considerable potential for enhancing robot intelli-\ngence. Nonetheless, several critical issues remain to be addressed. Foundation models,\ncharacterized as large-scale models pre-trained on extensive datasets, face challenges re-\nlated to real-time requirements and limited computational resources in robotic applications.\nMoreover, concerns such as personal information protection, privacy, and security from\nexternal attacks need resolution to enable cloud-based LLMs for robotics.\nSecondly, to enhance the computational efﬁciency and usability of LMs, there is\nongoing research into small language models (SLMs). Despite having fewer parameters,\nSLMs can achieve performance comparable to LLMs in speciﬁc applications. Several\nSLMs have been introduced, including DistilBERT [224], which is a compact version of\nGoogle’s BERT; Phi-3 [61], another SLM; Florence-2 [84], a small VLM model from Microsoft;\nMobileBERT [225], which is optimized for mobile platforms; and compact open-source\nversions of OpenAI’s GPT models such as GPT-Neo [226] and GPT-J [227]. Generally, SLMs\nare streamlined models with fewer parameters compared to LLMs, which can number in the\nbillions. SLMs utilize smaller, domain-speciﬁc datasets and require shorter training periods,\ntypically just a few weeks, unlike LLMs, which demand vast datasets for broad learning"
  },
  {
    "question": "What is the significance of autoregressive generation in LLMs?",
    "chunk": "2023-07 LLaMA2 Meta AI Research [ 70] 2024-07 Large2 Mistral AI [ 71]\n2023-09 Baichuan2 Baidu [ 72] 2024-07 Gemma2 Gemma Team,\nGoogle DeepMind [73]\n2023-10 Mistrial Mistral AI [ 74] 2024-08 EXAONE 3 LG AI Research [ 75]\n2024-01 DeepSeek-\nCoder DeepSeek-AI [ 76] 2024-08 Grok-2 and\nGrok-2 mini xAI [ 77]\nTable 3.Chronicle of VLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref.\n2020-05 DETR Facebook AI [ 78] 2023-04 LLaVA UW–Madison [ 79]\n2020-12 DeiT Facebook AI [ 80] 2023-04 MiniGPT-4 KAUST [ 81]\n2021-02 DALL-E OpenAI [ 82] 2023-09 GPT-4V OpenAI [ 83]\n2021-02 CLIP OpenAI [ 7] 2023-11 Florence-2 Microsoft [ 84]\n2021-03 Swin\nTransformer Microsoft [ 85] 2024-01 Lumiere Google Research [ 86]\n2021-05 SegFormer Univ. of Hong\nKong [87] 2024-01 Fuyu Adept [ 88]\n2021-06 Vision\nTransformer\nGoogle Research,\nBrain [89] 2024-03 Gemini 1.5 Gemini Team,\nGoogle [90]\n2021-06 BEiT HIT, Microsoft\nResearch [91] 2024-04 InternLMXComposer2 Shanghai AI Lab. [ 92]\n2021-11 ViTMAE Facebook AI [ 93] 2024-04 IDEFICS 2 Hugging Face [ 94]\n2021-12 Stable\nDiffusion LMU Munich, IWR [95] 2024-05 Ideﬁcs2 Hugging Face [ 96]\n2022-03 R3M Meta AI, Stanford\nUniv. [97] 2024-05 Chameleon Meta AI Research [ 98]\n2022-04 Flamingo DeepMind [ 99] 2024-07 InternLM-\nXComposer-2.5 Shanghai AI Lab. [ 98]\n2023-01 BLIP-2 Salesforce Research [100] 2024-07 PaliGemma Univ. of Hong\nKong [101]\n2023-04 SAM Meta AI Research [ 102] 2024-08 SAM 2 Meta AI [ 103]\n2023-04 DINOv2 Meta AI Research [ 104] 2024-08 Qwen-VL2 Alibaba Group [ 105]Appl. Sci.2024, 14, 8868 8 of 39\n3.2. LLM Architectures and Tunings\nThe architecture of LLMs fundamentally utilizes the transformer architecture, with\nthree representative types based on different transformer conﬁgurations as shown in\nFigure 2. Firstly, the prevalent encoder–decoder structure of transformers employs the\nencoder to process the input sequence and generate a latent representation through multi-\nhead self-attention layers; the decoder then uses cross-attention on this representation to\nautoregressively produce the target sequence. Notable encoder–decoder PLMs include\nT5 [47] and BART [33], with Flan-T5 [106] being an encoder–decoder-based LLM. Secondly,"
  },
  {
    "question": "What is the significance of breaking down instructions for robots?",
    "chunk": "paper details experiments in a simulated tabletop rearrangement, a mini-grid 2D maze,\nand real-world kitchen mobile manipulation settings to evaluate long-horizon reasoning\nperformance. Comparative experiments with SayCan revealed that while SayCan limits\nthe range of robot actions, GD can represent a wider array of actions. In contrast to\nCLIPort, which executes high-level language instructions directly, GD achieves enhanced\nperformance through detailed, step-by-step planning.\nHuang [153], as shown in Figure8, proposed the inner monologue method, which\nallowed LLMs to plan and adjust based on feedback from the environment. This approach\nenabled robots to formulate plans in dynamic environments, retry upon facing failure,\nor seek human feedback to reﬁne their strategies. The author clariﬁed that this method\nemerged from integrating the LLM’s high-level planning capabilities with perceptual feed-\nback and low-level control, thereby facilitating more adaptable and intelligent interactions.\nInner monologue integrated various feedback sources into the language model to assist the\nrobot in executing given instructions, including text-based indicators of the robot’s action\nsuccess or failure, object recognition and descriptions within the scene, the robot’s ability to\nask questions to gather additional information, breaking down instructions into multiple\nsteps to establish an execution plan, and enabling the robot to interact with humans to\nexecute and reﬁne the instructions. The inner monologue method was evaluated in both\nsimulated and real-world environments, such as tabletop rearrangement tasks and manip-\nulation tasks in a real kitchen. The results showed that inner monologue was an effective\nframework, enabling robots to act intelligently in complex interactive settings by effectively\nintegrating environmental feedback to plan and execute tasks.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 20 of 39 \n \n \nFigure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable"
  },
  {
    "question": "What is the significance of human feedback in training LLMs?",
    "chunk": "alignment tuning (or preference alignment) seeks to align the behavior of LLMs with hu-\nman values and preferences. Prominent methods include reinforcement learning from hu-\nman feedback (RLHF) [110], which involves ﬁne-tuning LLMs using human feedback to \nbetter reﬂect human values, and direct preference optimization (DPO) [111], focusing on \ntraining with pairs of human preferences th at usually include an input prompt and the \npreferred and non-preferred responses. \nFor both instruction tuning and alignment tuning, which involve training LLMs with \nextensively large model para meters, substantial GPU memo ry and computational re-\nsources are required, with high costs typi cally incurred when utilizing cloud-based \nFigure 2.Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx Decoder\n(middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectangles\nrepresent attention between preﬁx tokens, attention between preﬁx and target tokens, attention\nbetween target tokens, and masked attention [5].\nIn terms of the tuning of LLMs, these models are essentially pre-trained on mas-\nsive datasets and require ﬁne-tuning for different application domains. However, the\nconsiderable model size and number of parameters pose challenges for ﬁne-tuning on\nstandard computers and GPUs. The subsequent sections will discuss methods to address\nthese challenges.\nLLM tuning is broadly divided into two categories based on the training objective.\nInstruction tuning is a form of supervised learning where the training data typically include\ndescriptions of tasks, inputs, and corresponding outputs. This type of tuning is designed\n(1) to enhance the functional capabilities of LLMs, (2) to specialize them by training with\ndiscipline-speciﬁc information, and (3) to improve task generalization and consistency\nthrough a better understanding of natural language commands. Conversely, alignment\ntuning (or preference alignment) seeks to align the behavior of LLMs with human values\nand preferences. Prominent methods include reinforcement learning from human feedbackAppl. Sci.2024, 14, 8868 9 of 39\n(RLHF) [110], which involves ﬁne-tuning LLMs using human feedback to better reﬂect\nhuman values, and direct preference optimization (DPO) [111], focusing on training with\npairs of human preferences that usually include an input prompt and the preferred and\nnon-preferred responses.\nFor both instruction tuning and alignment tuning, which involve training LLMs\nwith extensively large model parameters, substantial GPU memory and computational\nresources are required, with high costs typically incurred when utilizing cloud-based\nresources. Under these conditions, parameter-efﬁcient ﬁne-tuning (PEFT) offers a method\ndesigned to efﬁciently conduct ﬁne-tuning of such LLMs [112].\nAmong the methods of PEFT, there are four major approaches as shown in Figure3:\nadapter tuning, prompt tuning, preﬁx tuning, and low-rank adaptation (LoRA). Adapter\ntuning [113,114] involves integrating small neural network modules, known as adapters,\ninto the core components of a transformer model, speciﬁcally into the attention and feed-\nforward layers. These adapters are inserted serially following these layers, allowing ﬁne-\ntuning of only the adapter modules according to speciﬁc task goals, while the parameters\nof the original language model remain unchanged. Consequently, adapter tuning effec-\ntively reduces the number of trainable parameters. Additionally, prompt tuning [115,116]"
  },
  {
    "question": "How does LLM-Planner compare to traditional models like HLSM?",
    "chunk": "Figure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable\nrobots to carry out instructions: (a) mobile manipulation and (b,c) tabletop manipulation, in both\nsimulated and real-world environments [153].Appl. Sci.2024, 14, 8868 20 of 39\nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn,\na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot\nbehavior trees (BTs) from textual descriptions. The developed model was compact enough\nto operate on a robot’s onboard microcomputer, while adept at constructing complex robot\nbehaviors. It provided structurally and logically correct BTs and demonstrated the ability\nto handle instructions that were not included in the training set.\nSong [155], as shown in Figure9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions\nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via\na low-level planner. It continuously updated environmental information as new objects\nwere detected during action implementation and revisited the LLM to adjust the plan if\nsubgoals failed or were delayed based on updated observations. This iterative process was\nrepeated until the subgoal was achieved, after which the system moved to the next goal.\nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated\ncompetitive performance with signiﬁcantly reduced training data and proved its ability to\ngeneralize in various tasks (e.g., ALFRED) with minimal examples.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39"
  },
  {
    "question": "How do robots modify their navigation plans based on real-time observations?",
    "chunk": "within the environment. During this process, the robot utilized a graph search algorithm to\ndetermine optimal trajectories and to navigate along these paths in the real world. This\nmethod demonstrated LM-Nav’s ability to perform long-horizon navigation in complex\noutdoor environments using natural language instructions.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 27 of 39 \n \n \nFigure 13. LM-Nav uses three pre-trained models: ( a) VNM builds a topological graph from \nobservations, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, \n(d) A graph search algorithm then ﬁnds the best robot trajectory, and ( e) the robot executes the \nplanned path [173]. \nZhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow \ninstructions. NavGPT is a vision-language na vigation system that employs an LLM to \ntranslate visual inputs from a visual foundation model (VFM) into natural language. The \nLLM then interprets the current state and makes informed decisions to reach the intended \ngoal, based on these converted visuals, naviga tion history, and potential future routes. \nNavGPT conducts various functions, incl uding high-level planning, decomposing \ninstructions into sub-goals, identifying landmarks in observed scenes, monitoring \nnavigation progress, and modifying plans as necessary. Although NavGPT’s performance \non zero-shot tasks from the R2R dataset has not yet matched that of trained models, it \nunderscored the potential of utilizing multi-modality inputs with LLMs for visual \nnavigation and tapping into the explicit reasoning capabilities of LLMs to enhance learned \nmodels. \nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-\ntrained vision-language features with a 3D reconstruction of the physical world. VLMaps, \nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary \nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands. \nThese commands can be directly localized on a map and generate new obstacle maps in \nreal-time, facilitated by sharing among various robot types. Extensive experiments \nconducted in both simulated environments (using the Habitat simulator with the \nMatterport3D dataset and the AI2THOR simulator) and real-world settings (with the HSR \nmobile robot for indoor navigation) demonstrated that VLMs can navigate based on more \ncomplex language instructions than previous methods. The reviewed papers in this study \nare summarized in Table 5. \nTable 5. Summary of the reviewed papers in this study. \nName Explanation Ref. \nReward Design in \nRL \n• Eureka automatically generates and im proves reward functions based on the \nvirtual environment source code.$• Dr Eureka builds reward-aware physics \npriors using Eureka and supports eﬀective operation in the real world through \ndomain randomization.$• LLMs design and re ﬁne reward functions based on \nnatural language input.$• LLMs and VLMs integrate multimodal data to \ngenerate reward functions. \n[11,134,136–139,176–\n180] \nFigure 13.LM-Nav uses three pre-trained models: (a) VNM builds a topological graph from observa-\ntions, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, (d)A\ngraph search algorithm then ﬁnds the best robot trajectory, and (e) the robot executes the planned\npath [173]."
  },
  {
    "question": "How do robots use feedback to improve their planning?",
    "chunk": "paper details experiments in a simulated tabletop rearrangement, a mini-grid 2D maze,\nand real-world kitchen mobile manipulation settings to evaluate long-horizon reasoning\nperformance. Comparative experiments with SayCan revealed that while SayCan limits\nthe range of robot actions, GD can represent a wider array of actions. In contrast to\nCLIPort, which executes high-level language instructions directly, GD achieves enhanced\nperformance through detailed, step-by-step planning.\nHuang [153], as shown in Figure8, proposed the inner monologue method, which\nallowed LLMs to plan and adjust based on feedback from the environment. This approach\nenabled robots to formulate plans in dynamic environments, retry upon facing failure,\nor seek human feedback to reﬁne their strategies. The author clariﬁed that this method\nemerged from integrating the LLM’s high-level planning capabilities with perceptual feed-\nback and low-level control, thereby facilitating more adaptable and intelligent interactions.\nInner monologue integrated various feedback sources into the language model to assist the\nrobot in executing given instructions, including text-based indicators of the robot’s action\nsuccess or failure, object recognition and descriptions within the scene, the robot’s ability to\nask questions to gather additional information, breaking down instructions into multiple\nsteps to establish an execution plan, and enabling the robot to interact with humans to\nexecute and reﬁne the instructions. The inner monologue method was evaluated in both\nsimulated and real-world environments, such as tabletop rearrangement tasks and manip-\nulation tasks in a real kitchen. The results showed that inner monologue was an effective\nframework, enabling robots to act intelligently in complex interactive settings by effectively\nintegrating environmental feedback to plan and execute tasks.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 20 of 39 \n \n \nFigure 8. Inner Monologue integrates various feedback sources into the language model to enable \nrobots to carry out instructions: ( a) mobile manipulation and ( b,c) tabletop manipulation, in both \nsimulated and real-world environments [153]. \nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn, \na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot \nbehavior trees (BTs) from textual descriptions. The developed model was compact enough \nto operate on a robot’s onboard microcompu ter, while adept at constructing complex \nrobot behaviors. It provided structurally an d logically correct BTs and demonstrated the \nability to handle instructions that were not included in the training set. \nSong [155], as shown in Figure 9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions \nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via \na low-level planner. It continuously updated environmental information as new objects \nwere detected during action implementation an d revisited the LLM to adjust the plan if \nsubgoals failed or were delayed based on updated observations. This iterative process was \nrepeated until the subgoal was achieved, after which the system moved to the next goal. \nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated \ncompetitive performance with signi ﬁcantly reduced training data and proved its ability \nto generalize in various tasks (e.g., ALFRED) with minimal examples. \nFigure 8.Inner Monologue integrates various feedback sources into the language model to enable"
  },
  {
    "question": "How do LLMs and VLMs help in controlling robots?",
    "chunk": "for speciﬁc tasks through reinforcement learning, even in complex environments. The\nlow-level controlcategory includes a research area in which LLMs and VLMs generate\ncommand sequences that directly control the robot’s actuators through natural language\nand visual input. Thehigh-level planningcategory is a research area where the LLM\nidentiﬁes the present circumstances and objective of the tasks, subsequently developing an\nexplainable plan based on the reasoning required for problem-solving. In this research area,\nthe LLM is also tasked with developing the optimal robot behavior plan, which entails\nevaluating the feasibility of the established plan. In themanipulation category, the LLM in-\nterprets high-level instructions and the VLM (and LLM) analyzes various conditions based\non their understanding of the surroundings to assist robot arms in performing the speciﬁc\ntasks. While this category can be broadly included in the high-level planning category,\nthere are numerous studies that are speciﬁcally related to manipulation with a robot arm,\nwhich is why the manipulation category was separated. Thescene understandingcategory\nrepresents a research area that seeks to combine LLMs and VLMs with the objective of\nassisting robots in comprehending their surrounding environment. This is accomplished\nby identifying objects based on natural language instructions and visual information, as\nwell as by evaluating the relationships between them. This research area is also closely\nrelated to the ﬁeld of autonomous visual navigation. From a boarder perspective, there is an\noverlap between the scene understanding category and the perception-related components\nof the high-level planning category. However, in this review, the scene understanding\ncategory was considered a distinct category due to its prevalence as an application of\nVLM models.\nTable 1 lists resources that aid in understanding robot intelligence based on language\nmodels. The review [5] examined recent advancements in LLMs with a particular emphasis\non four key areas: pre-training, adaptation tuning, utilization, and capacity evaluation. Fur-\nthermore, it provided a summary of the resources currently available for the development\nof LLMs and discussed potential future directions for research in this ﬁeld. The survey [12]\nconducted a comprehensive and systematic review of VLMs for visual recognition tasks.\nIt addressed the evolution of the visual recognition paradigm, the principal architectures\nand datasets, and the fundamental principles of VLMs. Moreover, the paper provided\nan overview of the pre-training, transfer learning, and knowledge distillation methods\nemployed in the context of VLMs. The review [3] examined the potential for leveraging\nexisting natural language processing and computer vision foundation models in robotics.\nIn addition, it explored the possibility of developing a robot-speciﬁc foundation model. The\nreview [13] presented an analysis of recent studies on language-based approaches to robotic\nmanipulation. It comprised an analysis of learning paradigms integrated with foundation\nmodels related to manipulation tasks, including semantic information extraction, environ-\nment and evaluation, auxiliary tasks, task representation, safety issues, and other pertinent\nconsiderations. The survey paper [14] presented an analysis of recent research articles that\nemployed foundation models to address robotics challenges. It investigated the extent to\nwhich foundation models enhanced robot performance in perception, decision-making, and\ncontrol. In addition, it examined the obstacles impeding the implementation of foundation\nmodels in robot autonomy and proposed avenues for future advancements. The review\npaper [15] presented a comprehensive review of the research in the ﬁeld of vision and\nlanguage navigation (VLN), encompassing tasks, evaluation metrics, and methodologies\nrelated to autonomous navigation.Appl. Sci.2024, 14, 8868 5 of 39\nTable 1.Useful Review Papers.\nTitle Keywords Ref."
  },
  {
    "question": "Can you explain how task instructions are processed in SayPlan?",
    "chunk": "other novel applications.Appl. Sci.2024, 14, 8868 22 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 22 of 39 \n \nFigure 10. ProgPrompt is a system that uses Pyth on programming structures to provide \nenvironmental information and actions, enhancing the success rate of robot task planning through \nan error recovery feedback mechanism and environmental state feedback [156]. \nRana [157] introduced SayPlan, a scalable method for large-scale task planning using \nLLMs and based on a 3D Scene Graph (3DSG) representation. SayPlan involved the LLM \nsearching a collapsed 3D scene graph and task instructions to identify all relevant items \nand then locating the subgraph that contained the necessary items to complete the task. \nThe identiﬁed subgraph was subsequently used by the LLM to generate a high-level plan \nthat addressed the navigational aspect of the task. This plan was formatted as a JSON 3D \nscene graph and subjected to a repetitive replanning process through feedback from the \nscene graph simulator and a set of API calls for manipulation and operation until an \nexecutable plan was determined. SayPlan wa s tested in two large-scale environments, \nfeaturing up to three ﬂoors, 36 rooms, and 140 assets and objects, proving its capability to \nground large-scale and long-horizon task plans from abstract and natural language \ninstructions, thereby enabling a mobile manipulator robot to execute these tasks. \nZeng [158], as shown in Figure 11, proposed the Socratic model (SM), a modular \nframework that synergistically utilizes various forms of knowledge and employs multiple \npre-trained models to exchange information and leverage new multimodal capabilities. \nSM operates without ﬁne-tuning by integrating diverse pre-trained models and functions \nin a zero-shot approach (e.g., using multimodal prompts), which enables it to harness new \nmultimodal capabilities. SM demonstrated state-of-the-art performance in zero-shot \nimage captioning and video-to-text retrieval, and it e ﬀectively answered free-form \nquestions about egocentric vi deo. Additionally, it supported interactions with external \nAPIs and databases (e.g., web search) for multimodal assistive dialogue, robot perception, \nand planning, among other novel applications. \n \nFigure 11. SM integrates various types of knowledge by using multiple pre-trained models and \nprovides meaningful results even in complex computer vision tasks such as image captioning, \ncontext inference, and activity prediction [158]. \nLin [159] introduced Text2Motion, a language-based framework designed to handle \nsequential manipulation tasks that requ ire long-horizon reasoning. Text2Motion \ninterpreted natural language instructions to formulate task plans and generated multiple \ncandidate skill sequences, evaluating the ge ometric feasibility of each sequence. By \nemploying a greedy search strategy, it selected  the optimal skill sequence to verify and \nexecute the ﬁnal plan. This method enabled Text2Motion to perform complex sequential \nmanipulation tasks with a higher success rate compared to existing language-based \nplanning methods, such as Saycan-gs an d Innermono-gs, and provided semantically \ngeneralized characteristics among skills with geometric relationships. \nFigure 11. SM integrates various types of knowledge by using multiple pre-trained models and\nprovides meaningful results even in complex computer vision tasks such as image captioning, context\ninference, and activity prediction [158].\nLin [159] introduced Text2Motion, a language-based framework designed to handle"
  },
  {
    "question": "What are word embeddings and how are they used in language models?",
    "chunk": "(RNNs) to model the probability of word sequences [24–26]. A key element of this stage\nis the development of word vectors, also known as word embeddings, which form word\nprediction models based on vectors that use a distributed representation of words [24,27].\nWord2vec, a simpliﬁed shallow neural network approach, was introduced to learn these\ndistributed word representations [28,29]. It proved highly effective across various NLP\ntasks by calculating meaningful similarities between word vectors. NLMs progressed\nfrom basic word sequence modeling to sophisticated techniques for representing language\nthrough word2vec.\nFollowing the NLM phase, the ﬁeld advanced to pre-trained language models (PLMs),\nwhich encompass models such as ELMO [30] and BERT [31]. PLMs, utilizing large-scale\ntext data, learn text patterns, structures, and meanings to develop pre-trained context-\nsensitive word representations. They have successfully executed a variety of language\nunderstanding and generation tasks using this acquired knowledge. ELMo [30] introduced\na pre-training method employing bidirectional LSTM (biLSTM) networks for modeling\ndeep contextualized word representations, optimizing performance through speciﬁc ﬁne-\ntuning of the trained biLSTM network for downstream tasks. ELMo is also characterized\nas a bidirectional language model for its dual-directional use of language models.\nAnother PLM model, BERT [31], leverages the transformer architecture [32], exhibiting\nremarkable effectiveness with self-attention mechanisms and parallel processing. BERT,\na pre-trained bidirectional language model, utilizes extensive unlabeled text data. The\nmethod of unsupervised learning-based pre-training in BERT comprises two primary tasks:\nmasked language models and next sentence prediction. PLMs that provide pre-trained\ncontext-aware word representations are profoundly effective in general-purpose semantic\nfeature extraction, facilitating enhancements in NLP task performance. Owing to these\ncharacteristics, numerous subsequent studies employing pre-training and ﬁne-tuning have\nbeen introduced, featuring varied structures [33,34] (e.g., BART [33] and GPT-2 [35]) and\nenhancing pre-training strategies [36–38].\nBased on subsequent studies, it has been found that increasing the model size or data\nsize of PLMs typically enhances the performance of LM models [39]. This has prompted\nresearch into training large-scale PLMs, such as GPT-3 with 175B parameters and PaLM\nwith 540B parameters. The focus of this research, grounded in scaling laws, primarily\ncenters on augmenting model sizes and exploring the capabilities of larger models. These\ncapabilities, known as the emergent abilities of LLMs, have sparked signiﬁcant interest. For\nexample, GPT-3 can address problems it has not been trained on with minimal examples\nthrough in-context learning, a feat GPT-2 ﬁnds challenging. Due to these characteristics,\nthe academic community commonly designates these large PLMs as LLMs [40–43]. Conse-\nquently, research in this area is highly active. Notably, since the introduction of OpenAI’s\nChatGPT, there has been a surge in the number of arXiv papers on LLMs. Following\nMicrosoft’s announcement [2] about integrating ChatGPT into robotics, a variety of studies\nhave explored the application of LLMs across different areas of robotics research. The\navailable LLM models are presented in chronological order in Table2. Additionally, Table3\nincludes the VLM models.Appl. Sci.2024, 14, 8868 7 of 39\nTable 2.Chronicle of LLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref."
  },
  {
    "question": "How do pre-trained language models enhance NLP tasks?",
    "chunk": "(RNNs) to model the probability of word sequences [24–26]. A key element of this stage\nis the development of word vectors, also known as word embeddings, which form word\nprediction models based on vectors that use a distributed representation of words [24,27].\nWord2vec, a simpliﬁed shallow neural network approach, was introduced to learn these\ndistributed word representations [28,29]. It proved highly effective across various NLP\ntasks by calculating meaningful similarities between word vectors. NLMs progressed\nfrom basic word sequence modeling to sophisticated techniques for representing language\nthrough word2vec.\nFollowing the NLM phase, the ﬁeld advanced to pre-trained language models (PLMs),\nwhich encompass models such as ELMO [30] and BERT [31]. PLMs, utilizing large-scale\ntext data, learn text patterns, structures, and meanings to develop pre-trained context-\nsensitive word representations. They have successfully executed a variety of language\nunderstanding and generation tasks using this acquired knowledge. ELMo [30] introduced\na pre-training method employing bidirectional LSTM (biLSTM) networks for modeling\ndeep contextualized word representations, optimizing performance through speciﬁc ﬁne-\ntuning of the trained biLSTM network for downstream tasks. ELMo is also characterized\nas a bidirectional language model for its dual-directional use of language models.\nAnother PLM model, BERT [31], leverages the transformer architecture [32], exhibiting\nremarkable effectiveness with self-attention mechanisms and parallel processing. BERT,\na pre-trained bidirectional language model, utilizes extensive unlabeled text data. The\nmethod of unsupervised learning-based pre-training in BERT comprises two primary tasks:\nmasked language models and next sentence prediction. PLMs that provide pre-trained\ncontext-aware word representations are profoundly effective in general-purpose semantic\nfeature extraction, facilitating enhancements in NLP task performance. Owing to these\ncharacteristics, numerous subsequent studies employing pre-training and ﬁne-tuning have\nbeen introduced, featuring varied structures [33,34] (e.g., BART [33] and GPT-2 [35]) and\nenhancing pre-training strategies [36–38].\nBased on subsequent studies, it has been found that increasing the model size or data\nsize of PLMs typically enhances the performance of LM models [39]. This has prompted\nresearch into training large-scale PLMs, such as GPT-3 with 175B parameters and PaLM\nwith 540B parameters. The focus of this research, grounded in scaling laws, primarily\ncenters on augmenting model sizes and exploring the capabilities of larger models. These\ncapabilities, known as the emergent abilities of LLMs, have sparked signiﬁcant interest. For\nexample, GPT-3 can address problems it has not been trained on with minimal examples\nthrough in-context learning, a feat GPT-2 ﬁnds challenging. Due to these characteristics,\nthe academic community commonly designates these large PLMs as LLMs [40–43]. Conse-\nquently, research in this area is highly active. Notably, since the introduction of OpenAI’s\nChatGPT, there has been a surge in the number of arXiv papers on LLMs. Following\nMicrosoft’s announcement [2] about integrating ChatGPT into robotics, a variety of studies\nhave explored the application of LLMs across different areas of robotics research. The\navailable LLM models are presented in chronological order in Table2. Additionally, Table3\nincludes the VLM models.Appl. Sci.2024, 14, 8868 7 of 39\nTable 2.Chronicle of LLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref."
  },
  {
    "question": "How do language-driven commands enhance robot functionality?",
    "chunk": "Citation: Jeong, H.; Lee, H.; Kim, C.;\nShin, S. A Survey of Robot\nIntelligence with Large Language\nModels. Appl. Sci.2024, 14, 8868.\nhttps://doi.org/10.3390/app14198868\nAcademic Editors: Luis Gracia and J.\nErnesto Solanes\nReceived: 6 September 2024\nRevised: 24 September 2024\nAccepted: 25 September 2024\nPublished: 2 October 2024\nCopyright: © 2024 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\napplied  sciences\nReview\nA Survey of Robot Intelligence with Large Language Models\nHyeongyo Jeong1,† , Haechan Lee1,† , Changwon Kim2,* and Sungtae Shin1,*\n1 Department of Mechanical Engineering, Dong-A University, Busan 49315, Republic of Korea\n2 School of Mechanical Engineering, Pukyong National University, Busan 48513, Republic of Korea\n* Correspondence: ckim@pknu.ac.kr (C.K.); stshin@dau.ac.kr (S.S.)\n† These authors contributed equally to this work.\nAbstract: Since the emergence of ChatGPT, research on large language models (LLMs) has actively\nprogressed across various ﬁelds. LLMs, pre-trained on vast text datasets, have exhibited exceptional\nabilities in understanding natural language and planning tasks. These abilities of LLMs are promis-\ning in robotics. In general, traditional supervised learning-based robot intelligence systems have a\nsigniﬁcant lack of adaptability to dynamically changing environments. However, LLMs help a robot\nintelligence system to improve its generalization ability in dynamic and complex real-world environ-\nments. Indeed, ﬁndings from ongoing robotics studies indicate that LLMs can signiﬁcantly improve\nrobots’ behavior planning and execution capabilities. Additionally, vision-language models (VLMs),\ntrained on extensive visual and linguistic data for the vision question answering (VQA) problem,\nexcel at integrating computer vision with natural language processing. VLMs can comprehend visual\ncontexts and execute actions through natural language. They also provide descriptions of scenes\nin natural language. Several studies have explored the enhancement of robot intelligence using\nmultimodal data, including object recognition and description by VLMs, along with the execution\nof language-driven commands integrated with visual information. This review paper thoroughly\ninvestigates how foundation models such as LLMs and VLMs have been employed to boost robot\nintelligence. For clarity, the research areas are categorized into ﬁve topics: reward design in rein-\nforcement learning, low-level control, high-level planning, manipulation, and scene understanding.\nThis review also summarizes studies that show how foundation models, such as the Eureka model\nfor automating reward function design in reinforcement learning, RT-2 for integrating visual data,\nlanguage, and robot actions in vision-language-action models, and AutoRT for generating feasible\ntasks and executing robot behavior policies via LLMs, have improved robot intelligence.\nKeywords: embodied intelligence; foundation model; large language model (LLM); vision-language\nmodel (VLM); vision-language-action (VLA) model; robotics\n1. Introduction\nTo enhance the intelligence of robots in real-world environments that interact with\nhumans, developing robots capable of perceiving, acting, and interacting like humans is"
  },
  {
    "question": "How does the integration of large datasets improve robotic navigation?",
    "chunk": "representation for natural language queries; then, an LLM-based object suggestion module\nreviews instructions, suggests relevant objects, and queries the scene for object availability\nand location. Using this information, the LLM planner devises plans uniquely tailored to\nthe scene’s context. NLMap equips robots with the ability to function without a predeﬁned\ncatalog of objects or actions, overcoming the constraints of earlier methods and enabling\nmore adaptable operations in environments with novel or absent objects.\nElhafsi [171] introduced a monitoring framework that employed an LLM with superior\ncontextual understanding and reasoning capabilities to detect edge cases and anomalies\nwithin vision-based policies. This framework monitored the robot’s perception stream\nthrough an LLM-based module, designed to detect semantic anomalies that might occur\nduring operations. By converting the robot’s visual observations into textual descriptions\nat regular intervals and integrating these into LLM prompts, it could pinpoint factors\nleading to policy errors, unsafe behavior, or task confusion. The conversion of visual\ninformation into natural language descriptions used various techniques, without restriction\nto any speciﬁc method. This ﬂexibility enabled both fully end-to-end policies and classical\nautonomy stacks using learned perception to align more closely with human intuition.\nThe ﬁndings indicated that semantic anomalies did not always correspond to semantically\nexplainable failures, and end-to-end policies could sometimes behave unpredictably.\nHon [172] introduced a new model family named 3D-LLM, which incorporated 3D\nworld information into LLMs. The 3D-LLM model utilized 3D point clouds and their\nfeatures as input, enabling it to handle a variety of spatially aware 3D tasks. These tasks\nincluded 3D captioning, dense captioning, 3D question answering, task decomposition, 3D\ngrounding, 3D-assisted dialogue, and navigation. The model used a 3D feature extractor to\nalign 3D features from multi-view images with language features, facilitating more precise\ntext generation and question answering based on spatial understanding. To train 3D-LLM,\na pre-trained 2D VLM formed the backbone, enhanced by the addition of 3D positional\nembeddings to better capture 3D spatial information. The model generated location tokens\nthrough linguistic descriptions of speciﬁc objects and was trained using 3D features as input.\nExperimental results showed that 3D-LLM excelled in various 3D-related tasks, achieving\napproximately a 9% higher BLEU-1 score compared to previous models on the ScanQA\ndataset. It demonstrated superior performance in 3D captioning, task composition, andAppl. Sci.2024, 14, 8868 26 of 39\n3D-assisted dialogue, outperforming 2D VLMs and displaying an improved understanding\nof object locations, shapes, and interactions.\nIn the extension of scene understanding using VLMs, the keyword VLN (vision-\nand-language navigation) is widely used in navigation-related research, where language\nfoundation models are increasingly utilized.\nShah [173], as shown in Figure13, introduced a robotic navigation system named\nLM-Nav, which capitalized on the advantages of training with large, unlabeled trajectory\ndatasets while providing a high-level interface for users. LM-Nav utilized three large-scale\npre-trained models: ViNG, CLIP, and GPT-3. Initially, the LLM translated natural language\ninstructions into a sequence of textual landmarks. The VLM integrated these textual\nlandmarks with images to identify the relevant images through probabilistic distribution.\nSubsequently, the VNM utilized these landmarks to plan and execute robot trajectories"
  },
  {
    "question": "What are actuator actions and how are they included in language models for robotics?",
    "chunk": "language processing but also extended to broader research areas. This study explored\nextensive LLM applications in the robotics literature, such as planning, manipulation, and\nscene understanding, as well as reinforcement learning automation frameworks such as\nEureka, and included robot actions in language models such as AutoRT. Moreover, the\nresearch direction of current generative AI models is transitioning towards multimodalAppl. Sci.2024, 14, 8868 30 of 39\nlanguage models, moving beyond information acquisition and cognition aspects such as\ntext, images, and videos to include actuator actions within large models in therobotics ﬁeld.\nWhile the surveyed studies indicated that LLMs play a promising role in the future\nof robotics, certain limitations were also identiﬁed. First, the increased computational\nresources and energy consumption associated with embedding LLMs into robotic systems\nmust be addressed. Second, biases in language models and ethical considerations are\nsigniﬁcant issues that need to be tackled in robotics. Therefore, continual efforts will be\nnecessary in future research to resolve these challenges.\nOverall, LLMs are valuable tools that can signiﬁcantly advance robotics. This review\nhas revealed that innovative robot applications are possible through the integration of\nLLMs and VLMs. Moreover, these foundation models are expected to serve as critical\nelements for future robot research and practical applications in the real world.\nAuthor Contributions:Conceptualization, S.S. and C.K.; methodology, S.S.; formal analysis, H.J.,\nH.L. and S.S.; investigation, H.J., H.L. and S.S.; resources, H.J., H.L., S.S. and C.K.; writing—original\ndraft preparation, H.J., H.L., S.S. and C.K.; writing—review and editing, H.J., H.L., S.S. and C.K.; vi-\nsualization, H.J. and H.L.; supervision, S.S. and C.K.; project administration, S.S.; funding acquisition,\nS.S. All authors have read and agreed to the published version of the manuscript.\nFunding: This work was supported by the Technology Innovation Program (RS-2024-00423702,\nA Meta-Humanoid with Hypermodal Cognitivity and Role Dexterity: Adroid4X) funded by the\nMinistry of Trade, Industry, and Energy (MOTIE, Korea) and Regional Innovation Strategy (RIS)\nthrough the National Research Foundation of Korea (NRF) funded by the Ministry of Education\n(MOE) (2023RIS-007).\nInstitutional Review Board Statement:Not applicable.\nInformed Consent Statement:Not applicable.\nData Availability Statement:No new data were created or analyzed in this study. Data sharing is\nnot applicable to this article.\nConﬂicts of Interest:The authors declare no conﬂicts of interest.\nReferences\n1. Hello GPT-4o. Available online:https://openai.com/index/hello-gpt-4o/ (accessed on 13 August 2024).\n2. Vemprala, S.H.; Bonatti, R.; Bucker, A.; Kapoor, A. ChatGPT for Robotics: Design Principles and Model Abilities.IEEE Access\n2024, 12, 55682–55696. [CrossRef]\n3. Hu, Y.; Xie, Q.; Jain, V.; Francis, J.; Patrikar, J.; Keetha, N.; Kim, S.; Xie, Y.; Zhang, T.; Zhao, S.; et al. Toward General-Purpose\nRobots via Foundation Models: A Survey and Meta-Analysis.arXiv 2023, arXiv:2312.08782."
  },
  {
    "question": "Can you describe the reward-aware physics priors mechanism in DrEureka?",
    "chunk": "explanations for reinforcement learning tasks. Eureka consists of three main components:\nenvironment-as-context, evolutionary search, andreward reﬂection. Environment-as-context\ngenerates executable reward functions in a zero-shot manner by utilizing virtual environ-\nment source (Python) code as context. Evolutionary search iteratively generates reward\nfunction candidates and proposes enhanced functions based on previously generated and\nbest-performing ones, while also creating new functions through mutation. Reward reﬂec-\ntion offers a text summary of reward function quality based on training statistics recorded\nduring reinforcement learning, which assists in generating subsequent reward functions\nas feedback for the performance of previous functions. The reward functions generated\noutperformed expert-generated functions in 83% of benchmark tests. Moreover, Eureka\nsolved the pen spinning problem where a robot hand must spin a pen as much as possi-\nble according to predeﬁned rotations, a task previously considered unsolvable through\nmanual reward engineering. Eureka introduces a universal reward function design algo-\nrithm based on a code LLM and in-context evolutionary search, facilitating human-level\nreward generation for various robots and tasks without the need for prompt engineering or\nhuman intervention.\nFollowing Eureka, DrEureka [134], shown in Figure5, was developed to address\nthe sim-to-real problem by automatically conﬁguring appropriate reward functions and\ndomain randomization for physical environments. DrEureka’s reward-aware physics pri-\nors mechanism deﬁnes the lower and upper bounds of physical environment parameters\nbased on policies trained through initial reinforcement learning, facilitating reinforcement\nlearning across various physical environment domains. This randomization enables the\ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation\nmanipulation using real robots, all without human supervision.Appl. Sci.2024, 14, 8868 13 of 39Appl. Sci. 2024, 14, x FOR PEER REVIEW 13 of 39 \n \n \nFigure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses expert-\ndesigned functions through iterative improvements [11]. \nFollowing Eureka, DrEureka [134], shown in Figure 5, was developed to address the \nsim-to-real problem by automatically con ﬁguring appropriate reward functions and do-\nmain randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses\nexpert-designed functions through iterative improvements [11].\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 13 of 39 \n \n \nFigure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses expert-"
  },
  {
    "question": "What techniques have been proposed to prevent robotic system malfunctions?",
    "chunk": "prompts have the potential to cause the entire robotic system to malfunction. To defend\nagainst this critical threat to the reliability and safety of robotic systems, various techniques\nhave been proposed, such as input validation, which ﬁlters the model’s input, and context\nlocking, which restricts access based on the history and content of the prompt. Furthermore,\nstrict guardrails that restrict harmful or unsafe outputs from models can be an alternative\nto improve the reliability of robotic systems. However, it is essential to recognize that the\nsecurity techniques may potentially lead to a decline in the performance of the robot system.\nConsequently, the trade-off between performance and safety must be carefully considered.\nSince the emergence of ChatGPT and Microsoft’s implementation of robot systems\nusing ChatGPT [2], artiﬁcial intelligence components have been applied more widely and\nintensively in robotics research. Despite existing challenges, it is expected that research\ninvolving foundation models to improve robot intelligence will persist across various\ndomains and methods, which will likely enhance the usability and market potential of\nrobot systems inﬂuenced by these advancements.\n6. Conclusions\nIn this paper, we have explored the potential impact and applicability of LLMs on\nrobotics research ﬁelds by summarizing studies that applied LLMs and VLMs to robots.\nFundamentally, LLMs can enhance the capability of robots in natural language processing\nto interact with humans and to improve the robots’ autonomy in various task scenarios.\nIn particular, the ability of LLMs to understand and generate natural language plays a\ncrucial role in enabling robots to comprehend and execute complex commands. This survey\nconﬁrmed that the scope of utilizing LLMs in robotics was not limited to simple natural\nlanguage processing but also extended to broader research areas. This study explored\nextensive LLM applications in the robotics literature, such as planning, manipulation, and\nscene understanding, as well as reinforcement learning automation frameworks such as\nEureka, and included robot actions in language models such as AutoRT. Moreover, the\nresearch direction of current generative AI models is transitioning towards multimodalAppl. Sci.2024, 14, 8868 30 of 39\nlanguage models, moving beyond information acquisition and cognition aspects such as\ntext, images, and videos to include actuator actions within large models in therobotics ﬁeld.\nWhile the surveyed studies indicated that LLMs play a promising role in the future\nof robotics, certain limitations were also identiﬁed. First, the increased computational\nresources and energy consumption associated with embedding LLMs into robotic systems\nmust be addressed. Second, biases in language models and ethical considerations are\nsigniﬁcant issues that need to be tackled in robotics. Therefore, continual efforts will be\nnecessary in future research to resolve these challenges.\nOverall, LLMs are valuable tools that can signiﬁcantly advance robotics. This review\nhas revealed that innovative robot applications are possible through the integration of\nLLMs and VLMs. Moreover, these foundation models are expected to serve as critical\nelements for future robot research and practical applications in the real world.\nAuthor Contributions:Conceptualization, S.S. and C.K.; methodology, S.S.; formal analysis, H.J.,\nH.L. and S.S.; investigation, H.J., H.L. and S.S.; resources, H.J., H.L., S.S. and C.K.; writing—original\ndraft preparation, H.J., H.L., S.S. and C.K.; writing—review and editing, H.J., H.L., S.S. and C.K.; vi-"
  },
  {
    "question": "How do visual changes impact the success detection of tasks?",
    "chunk": "agents’ understanding of human intentions. They developed a framework that utilized\nlanguage as a primary inference tool, investigating how it could address key challengesAppl. Sci.2024, 14, 8868 14 of 39\nin reinforcement learning, such as efﬁcient exploration, data reuse in experience, skill\nscheduling, and observational learning. This framework employed LLMs and VLMs to\naddress these reinforcement learning challenges by (1) efﬁciently exploring environments\nwith sparse rewards, (2) reusing collected data to sequentially bootstrap the learning of\nnew tasks, (3) scheduling learned skills for novel tasks, and (4) acquiring knowledge from\nobserving expert agents.\nDu [138] developed success detectors that identiﬁed whether actions or tasks were\nsuccessfully completed, utilizing the large multimodal language model Flamingo and\nhuman reward annotations. The study on success detection spanned three distinct do-\nmains: (1) interactive language-conditioned agents in simulated households, (2) real-world\nrobotic manipulation tasks (inserting and removing small, medium, and large gears), and\n(3) “in-the-wild” human egocentric videos. These success detectors adapted to new lan-\nguage instructions and visual changes using VLMs such as Flamingo, which were trained\non a broad range of language and visual data. Furthermore, success detection was reframed\nas a VQA problem, enabling the tracking of task progress through multiple frames to ascer-\ntain whether tasks had been successfully completed. The proposed method proved to be\nmore accurate in detecting success compared to custom reward models in the ﬁrst two do-\nmains, even with new language instructions or visual changes. However, success detection\nin unseen real-world videos in the third domain posed a more challenging generalization\ntask, underscoring the need for additional research.\nDu [139] introduced theELLM (exploring with LLMs) framework, which provided\nguidelines for pre-training reinforcement learning using LLMs. ELLM utilized the natural\nlanguage processing capabilities of LLMs to deﬁne goals and furnish reward functions for\nreinforcement learning agents. This strategy enabled agents to undertake meaningful explo-\nration and learning within their environments. The paper assessed ELLM’s performance in\ntwo settings: Crafter, a 2D version of Minecraft, and Housekeep, involving the task of rear-\nranging household objects. Experimental results demonstrated that ELLM surpassed other\nmethods in both settings. In the Crafter setting, ELLM attained high performance through\ngoal-oriented learning, proving especially effective in scenarios with sparse reward signals.\nIn the Housekeep setting, the agent conducted sensible exploration by adhering to goals\nset by the LLM, achieving a high success rate. While the accuracy of goal setting by the\nLLM varied with the objects and locations, it generally showed high performance. These\nexperimental ﬁndings suggested that ELLM was successful in enhancing reinforcement\nlearning performance across diverse environments, highlighting the vital role of providing\nreward signals based on human commonsense.\n4.2. Low-Level Control\nResearch is also being conducted on generating commands that directly control a\nrobot’s actuators (i.e., enabling low-level control) through various applications of LLM\nmodels. Among these projects, the Google research team developed RT-1 [8], which consists\nof ﬁlm-conditioned EfﬁcientNet-B3, TokenLearner, and Transformer. RT-1 is a model that\nreceives images and natural language instructions at a rate of 3Hz and outputs discretized\nrobot actions. RT-1 was trained on a vast demo dataset with over 130k episodes from more"
  },
  {
    "question": "What kind of data can robots learn from to improve their performance?",
    "chunk": "than 700 tests collected over 17 months using 13 robots.\nA notable feature of RT-1 is its ability to enhance performance by learning from\ndata gathered from heterogeneous robots or simulations. In the study [8], the authors\nevaluated the performance of a model trained exclusively on data from the EveryDay\nRobot (EDR) against a model trained using data from both EDR and Kuka IIWA robots.\nThey recorded a 12% improvement in the bin-picking test. Another experiment compared\nmodel performance using data from real environments and simulations for items not\nencountered in actual settings. The ﬁndings indicated that incorporating simulation data in\nRT-1 training enhances performance over using purely real environment data, suggesting\nthat RT-1 can substantially improve model performance by integrating diverse data from\nrobots of varied morphologies or simulations while sustaining existing task capabilities.Appl. Sci.2024, 14, 8868 15 of 39\nRT-2 [9] is deﬁned as a vision-language-action (VLA) model that facilitates ﬁne-grained\ncontrol of robots through vision and language commands. RT-2 is ﬁne-tuned with robotic\ntrajectory data based on VLM models such as PaLM-E [140], which has 12 billion parameters\nand is trained on VQA data, alongside PaLI-X [141], which has parameter sizes ranging\nfrom 5 billion to 55 billion. The RT-2 system operates as an integrated closed-loop robotic\nsystem that combines low-level and high-level control policies. Despite not explicitly\nlearning certain capabilities during pre-training, RT-2 exhibits improved task performance\nvia real-world generalization involving diverse objects, visual scenes, and instructional\ncontexts. The paper [9] quantitatively assesses RT-2’s emergent capabilities in areas such as\nreasoning, symbol understanding, and human recognition. Furthermore, applying chain-\nof-thought prompting techniques to RT-2 has proven effective in solving more complex\nsemantic inference tasks, such as using a rock as an improvised hammer or offering an\nenergy drink instead of a carbonated beverage to a thirsty person. In comparison with\nthe earlier study on RT-1, RT-2 demonstrates enhanced performance in both familiar and\nnovel tasks.\nAutoRT [10] is a follow-up study based on the research results of RT-1 and RT-2,\nestablishing an orchestration of large-scale robotic agents for data collection in real-world\nscenarios. AutoRT employed 53 robots to gather 77,000 real robot episodes over seven\nmonths through both teleoperation and autonomous robot policies. At the heart of AutoRT\nis a robust foundation model that generates ‘task proposals’ based on given visual observa-\ntions. Notably, AutoRT introduces a ‘Robot Constitution’ using constitutional prompting to\nensure actions during the task proposal process do not compromise the safety of the robot\nor nearby individuals. This Robot Constitution, inspired by Asimov’s three laws [142],\ncomprises basic rules, safety rules that identify unsafe or unwanted tasks, and embodiment\nrules that clarify the robot’s operational boundaries.\nAutoRT enhances data collection by initially scanning the surroundings to identify\ninteresting scenes or tasks (exploration). It interprets the given context through a VLM\nand proposes potential tasks via an LLM (task generation). Subsequently, tasks suggested\nby the LLM are screened (affordance) to assess their feasibility and the need for human\nintervention, employing the Robot Constitution. During this procedure, viable tasks are\nchosen and performed, while pertinent data are gathered (data collection). The collected\ndata are then assessed for (diversity scoring) the visual diversity of the robot trajectories\nand the linguistic diversity of the language instructions generated by AutoRT (LLM). The"
  },
  {
    "question": "How do LLMs help robots execute complex instructions?",
    "chunk": "4o [1] have signiﬁcantly altered the ﬁeld of robotic AI research. These LLMs, trained on\nvast amounts of textual data, have shown excellent performance in enabling robots to\ncommunicate with humans more naturally and efﬁciently. Moreover, beyond the impacts\non human–robot interaction (HRI), there is ongoing research aimed at surpassing the\nlimitations of traditional low-level robot control techniques and planning algorithms by\nutilizing the high-level situational awareness and knowledge-based planning capabilities\nof LLMs. Notably, the programming capabilities of ChatGPT in the research presented by\nMicrosoft’s ChatGPT for Robotics [2] have introduced a new paradigm for applying LLMs\nin the robotics ﬁeld.\nThe goal of robot intelligence is to enable robots to operate autonomously in complex\nenvironments, interact naturally with humans, and make high-level decisions. To promote\nAppl. Sci.2024, 14, 8868.https://doi.org/10.3390/app14198868 https://www.mdpi.com/journal/applsciAppl. Sci.2024, 14, 8868 2 of 39\nadvancements in robot intelligence, the adoption of foundation models, such as LLMs and\nvision-language models (VLMs), which boast large parameter scales and pre-training on\nmassive datasets, is accelerating. These foundation models can perform various tasks, such\nas complex language understanding and generation and visual perception, enabling robots\nto engage with their environment in a more human-like manner.\nWhile traditional robot intelligence systems are highly effective in structured and pre-\ndictable environments, they are signiﬁcantly limited in their ability to adapt to dynamically\nchanging and complex real-world scenarios. In general, the intelligence models used in\nthese robotic systems are based on supervised learning, which requires large amounts\nof labeled data. This process is inherently resource-intensive. Moreover, these models\nare designed for a speciﬁc environment and require reconﬁguration whenever the task or\nenvironment changes. This renders the robots challenging to adapt and scale to disparate\nenvironments [3]. For practical robot systems, it is essential that they are able to ﬂexibly\nrespond to the ever-changing physical environment. From this perspective, the gener-\nalization of affordable tasks, environmental adaptability, and the accuracy of execution,\nplanning, and reasoning capabilities remain signiﬁcant challenges for traditional robotic\nintelligence systems [4].\nHowever, LLMs and VLMs help a robot intelligence system to enhance its general-\nization capability in dynamic and complex real-world environments. LLMs can leverage\npre-trained knowledge from extensive datasets to augment their ability to generalize to\neveryday tasks that are typically expected of robots. Unlike the conventional supervised\nmodels, LLMs can utilize zero-shot and few-shot learning to help robots quickly adapt\nto new environments without additional training [5]. This has the advantage of signiﬁ-\ncantly reducing the need for costly data collection and labeling. In addition, robot systems\nequipped with LLMs can process complex instructions based on their ability to understand\nand generate natural language, which can improve human–robot interactions. Furthermore,\nLLMs can be integrated with multimodal sensors such as LiDAR, depth, voice, tactile, pro-\nprioception, and visual information, which enables robots to comprehensively understand\nand adapt to their environment [6].\nLLMs have demonstrated exceptional capabilities in processing and understanding\ntext-based information, signiﬁcantly enhancing robotic communication abilities. For in-\nstance, robots can accurately comprehend and execute natural language commands via\nLLMs, providing scalability and ﬂexibility beyond traditional word-based robotic com-"
  },
  {
    "question": "How do language models contribute to a robot's understanding of tasks?",
    "chunk": "framework to assist robots in comprehendin g incomplete natural language instructions. \nThis framework enabled robots to receive instructions in natural language from humans, \nobserve their surroundings, and employ a commonsense reasoning method to \nautonomously infer missing information. LMCR utilized a model of commonsense \nreasoning learned from web-based text materials, allowing robots to understand \nincomplete instructions and autonomously execute tasks. The framework comprised three \nmain functions: language understanding, commonsense reasoning, and action planning. \nIn language understanding, LMCR translated human natural language instructions into a \nform interpretable by robots, parsing them into verb frames to convert them into \nexecutable structures. During the common sense reasoning phase, the robot analyzed \nsurrounding objects and employed a language model trained on large-scale unstructured \ntext materials to ﬁll in the missing details from the instructions. This model identiﬁed the \nFigure 7.After encoding visual features, they are mapped using visual tokens and text queries. A\nplan is then created with the LLaMA model and turned into task commands. The visual tokens are\nqueried and converted into low-level control commands to perform the task [150].\nChen [151] introduced the language-model-based commonsense reasoning (LMCR)\nframework to assist robots in comprehending incomplete natural language instructions.\nThis framework enabled robots to receive instructions in natural language from humans, ob-\nserve their surroundings, and employ a commonsense reasoning method to autonomously\ninfer missing information. LMCR utilized a model of commonsense reasoning learned\nfrom web-based text materials, allowing robots to understand incomplete instructions and\nautonomously execute tasks. The framework comprised three main functions: language\nunderstanding, commonsense reasoning, and action planning. In language understanding,\nLMCR translated human natural language instructions into a form interpretable by robots,\nparsing them into verb frames to convert them into executable structures. During the\ncommonsense reasoning phase, the robot analyzed surrounding objects and employed a\nlanguage model trained on large-scale unstructured text materials to ﬁll in the missing\ndetails from the instructions. This model identiﬁed the most suitable verb frame to com-\nplete the gaps. Subsequently, based on the completed verb frame, the robot formulated its\nactions using predeﬁned action plans for each verb to guide the movements of the robot\narm and execute the assignment. Experimental results showed that LMCR demonstrated\nsuperior generalization performance for novel concepts not presented in the training set\nand surpassed GCNGrasp, which depends on a predeﬁned graph structure for all concepts\nand their relationships. This indicated that LMCR was an effective tool, combining the se-\nmantic reasoning capabilities of language models with planning that adapted to the robot’s\nspeciﬁc environment and context, effectively managing complex and prolonged tasks.\nHuang [152] introduced a methodology named grounded decoding (GD), which offers\na method for generating LLM-based robot action plans. These plans enable robots to execute\nlong-term tasks across diverse physical environments. The methodology encompasses two\nprimary elements: linking the text generated by the language model to actionable taskAppl. Sci.2024, 14, 8868 19 of 39\ncommands in the physical world via GD and adjusting the tokens generated by the LLM\nto real-world conditions to formulate feasible commands. This approach synergizes the\nhigh-level semantic reasoning of LLMs with plans that are aligned with the robot’s physical\nenvironment and capabilities, thus facilitating the execution of complex and long-term\ntasks. The method addresses several limitations robots face in performing complex, long-\nterm tasks, such as a lack of physical world experience, an inability to process non-verbal"
  },
  {
    "question": "How does increasing model size affect the performance of language models?",
    "chunk": "characteristics, numerous subsequent studies employing pre-training and ﬁne-tuning have\nbeen introduced, featuring varied structures [33,34] (e.g., BART [33] and GPT-2 [35]) and\nenhancing pre-training strategies [36–38].\nBased on subsequent studies, it has been found that increasing the model size or data\nsize of PLMs typically enhances the performance of LM models [39]. This has prompted\nresearch into training large-scale PLMs, such as GPT-3 with 175B parameters and PaLM\nwith 540B parameters. The focus of this research, grounded in scaling laws, primarily\ncenters on augmenting model sizes and exploring the capabilities of larger models. These\ncapabilities, known as the emergent abilities of LLMs, have sparked signiﬁcant interest. For\nexample, GPT-3 can address problems it has not been trained on with minimal examples\nthrough in-context learning, a feat GPT-2 ﬁnds challenging. Due to these characteristics,\nthe academic community commonly designates these large PLMs as LLMs [40–43]. Conse-\nquently, research in this area is highly active. Notably, since the introduction of OpenAI’s\nChatGPT, there has been a surge in the number of arXiv papers on LLMs. Following\nMicrosoft’s announcement [2] about integrating ChatGPT into robotics, a variety of studies\nhave explored the application of LLMs across different areas of robotics research. The\navailable LLM models are presented in chronological order in Table2. Additionally, Table3\nincludes the VLM models.Appl. Sci.2024, 14, 8868 7 of 39\nTable 2.Chronicle of LLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref.\n2018-06 GPT-1 OpenAI [ 44] 2024-02 OLMo Allen Institute for\nAI [45]\n2019-02 GPT-2 OpenAI [ 35] 2024-02 StarCoder2 Hugging Face [ 46]\n2019-10 T5 Google [ 47] 2024-03 Claude 3 Anthropic [ 48]\n2020-05 GPT-3 OpenAI [ 49] 2024-03 InternLM2 Shanghai AI Lab [ 50]\n2021-07 Codex OpenAI [ 51] 2024-03 Jamba AI21Labs [ 52]\n2021-09 FLAN Google [ 53] 2024-04 Stabe Code Stability AI [ 54]\n2021-10 T0 Hugging Face [ 37] 2024-04 HyperCLOVA Naver [ 55]\n2021-12 Gopher DeepMind [ 56] 2024-04 Grok-1.5 xAI [ 57]\n2022-03 InstructGPT OpenAI [ 58] 2024-04 Llama3 Meta AI Research [ 59]\n2022-04 PaLM Google [ 60] 2024-04 Phi-3 Microsoft [ 61]\n2022-05 OPT Meta AI Research [ 62] 2024-05 GPT-4o OpenAI [ 1]\n2023-02 LLaMA Meta AI Research [ 63] 2024-06 Claude 3.5 Anthropic [ 64]\n2023-03 Alpaca Stanford Univ. [ 65] 2024-07 GPT-4o mini OpenAI [ 66]\n2023-03 GPT-4 OpenAI [ 50] 2024-07 Falcon2-11B TII [ 67]"
  },
  {
    "question": "How do self-attention layers function in transformer architectures?",
    "chunk": "2023-07 LLaMA2 Meta AI Research [ 70] 2024-07 Large2 Mistral AI [ 71]\n2023-09 Baichuan2 Baidu [ 72] 2024-07 Gemma2 Gemma Team,\nGoogle DeepMind [73]\n2023-10 Mistrial Mistral AI [ 74] 2024-08 EXAONE 3 LG AI Research [ 75]\n2024-01 DeepSeek-\nCoder DeepSeek-AI [ 76] 2024-08 Grok-2 and\nGrok-2 mini xAI [ 77]\nTable 3.Chronicle of VLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref.\n2020-05 DETR Facebook AI [ 78] 2023-04 LLaVA UW–Madison [ 79]\n2020-12 DeiT Facebook AI [ 80] 2023-04 MiniGPT-4 KAUST [ 81]\n2021-02 DALL-E OpenAI [ 82] 2023-09 GPT-4V OpenAI [ 83]\n2021-02 CLIP OpenAI [ 7] 2023-11 Florence-2 Microsoft [ 84]\n2021-03 Swin\nTransformer Microsoft [ 85] 2024-01 Lumiere Google Research [ 86]\n2021-05 SegFormer Univ. of Hong\nKong [87] 2024-01 Fuyu Adept [ 88]\n2021-06 Vision\nTransformer\nGoogle Research,\nBrain [89] 2024-03 Gemini 1.5 Gemini Team,\nGoogle [90]\n2021-06 BEiT HIT, Microsoft\nResearch [91] 2024-04 InternLMXComposer2 Shanghai AI Lab. [ 92]\n2021-11 ViTMAE Facebook AI [ 93] 2024-04 IDEFICS 2 Hugging Face [ 94]\n2021-12 Stable\nDiffusion LMU Munich, IWR [95] 2024-05 Ideﬁcs2 Hugging Face [ 96]\n2022-03 R3M Meta AI, Stanford\nUniv. [97] 2024-05 Chameleon Meta AI Research [ 98]\n2022-04 Flamingo DeepMind [ 99] 2024-07 InternLM-\nXComposer-2.5 Shanghai AI Lab. [ 98]\n2023-01 BLIP-2 Salesforce Research [100] 2024-07 PaliGemma Univ. of Hong\nKong [101]\n2023-04 SAM Meta AI Research [ 102] 2024-08 SAM 2 Meta AI [ 103]\n2023-04 DINOv2 Meta AI Research [ 104] 2024-08 Qwen-VL2 Alibaba Group [ 105]Appl. Sci.2024, 14, 8868 8 of 39\n3.2. LLM Architectures and Tunings\nThe architecture of LLMs fundamentally utilizes the transformer architecture, with\nthree representative types based on different transformer conﬁgurations as shown in\nFigure 2. Firstly, the prevalent encoder–decoder structure of transformers employs the\nencoder to process the input sequence and generate a latent representation through multi-\nhead self-attention layers; the decoder then uses cross-attention on this representation to\nautoregressively produce the target sequence. Notable encoder–decoder PLMs include\nT5 [47] and BART [33], with Flan-T5 [106] being an encoder–decoder-based LLM. Secondly,"
  },
  {
    "question": "What are some examples of tasks used to test success detection models?",
    "chunk": "main randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 5.DrEureka leverages LLM to design reward functions and solves the sim-to-real problem\nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134].\nXie [136] introducedText2Reward, a framework that automatically generated dense\nreward functions for reinforcement learning using LLMs. Provided with a goal expressed\nin natural language, Text2Reward produced executable dense reward functions derived\nfrom a compact representation of the environment. This framework generated free-form\ndense reward codes and delivered performance comparable to or surpassing that of policies\ntrained with expert-designed codes across a variety of tasks, including 17 manipulator-\nrelated tasks and six novel locomotion behaviors. Additionally, Text2Reward incorporated\nuser feedback to iteratively enhance the generated reward functions, thereby increasing the\nsuccess rate of the learned policies.\nDi Palo [137] explored the use of LLMs and VLMs to improve reinforcement learning\nagents’ understanding of human intentions. They developed a framework that utilized\nlanguage as a primary inference tool, investigating how it could address key challengesAppl. Sci.2024, 14, 8868 14 of 39\nin reinforcement learning, such as efﬁcient exploration, data reuse in experience, skill\nscheduling, and observational learning. This framework employed LLMs and VLMs to\naddress these reinforcement learning challenges by (1) efﬁciently exploring environments\nwith sparse rewards, (2) reusing collected data to sequentially bootstrap the learning of\nnew tasks, (3) scheduling learned skills for novel tasks, and (4) acquiring knowledge from\nobserving expert agents.\nDu [138] developed success detectors that identiﬁed whether actions or tasks were\nsuccessfully completed, utilizing the large multimodal language model Flamingo and\nhuman reward annotations. The study on success detection spanned three distinct do-\nmains: (1) interactive language-conditioned agents in simulated households, (2) real-world\nrobotic manipulation tasks (inserting and removing small, medium, and large gears), and\n(3) “in-the-wild” human egocentric videos. These success detectors adapted to new lan-\nguage instructions and visual changes using VLMs such as Flamingo, which were trained\non a broad range of language and visual data. Furthermore, success detection was reframed\nas a VQA problem, enabling the tracking of task progress through multiple frames to ascer-\ntain whether tasks had been successfully completed. The proposed method proved to be\nmore accurate in detecting success compared to custom reward models in the ﬁrst two do-\nmains, even with new language instructions or visual changes. However, success detection\nin unseen real-world videos in the third domain posed a more challenging generalization\ntask, underscoring the need for additional research."
  },
  {
    "question": "What challenges arise in traditional pre-training and fine-tuning pipelines for robots?",
    "chunk": "on the current image, command, and identiﬁed object data. Experimental use of real mobile\nmanipulation robots showed that MOO could adapt to new object types and environments\nin a zero-shot fashion. Moreover, MOO responded to non-verbal cues such as pointing at\nspeciﬁc objects, extending its scope to open-world exploration and manipulation.Appl. Sci.2024, 14, 8868 23 of 39\nExisting VLMs often lack a comprehensive understanding of physical concepts such\nas material and fragility, which limits their effectiveness in robotic manipulation tasks. To\naddress this issue, Gao [162] introduced PhysObjects, an object-centric dataset featuring\n39.6K crowd-sourced annotations and 417K automated annotations of physical concepts.\nThe automated annotations involved assigning speciﬁc concept values to predeﬁned object\ncategories or continuous concepts such as material and fragility. Fine-tuning a VLM on\nPhysObjects enhanced comprehension of physical concepts by capturing human biases\nrelated to the visual appearance of objects. Integrating this physically grounded VLM\nwith an LLM-based robotic planner framework improved performance in tasks requiring\nreasoning about physical concepts.\nThe traditional pre-training and ﬁne-tuning pipeline often suffers from decreased\nlearning efﬁciency and challenges in generalizing to unseen objects and tasks due to its\nreliance on domain-speciﬁc action information and domain-general visual information. To\naddress these limitations, Wang [163] proposed a modular approach named ProgramPort,\nwhich utilizes the syntactic and semantic structure of language instructions. Wang’s\nframework incorporated a semantic parser to reconstruct executable programs, composed of\nfunctional modules based on vision and action across multiple modalities. Each functional\nmodule combined deterministic computation with learnable neural networks. Program\nexecution involved generating parameters for general manipulation primitives used by the\nrobot’s end effector. The entire module network was trainable with an end-to-end imitation\nlearning objective. Experimental results demonstrated that the model effectively separated\naction and perception, achieving enhanced zero-shot and compositional generalization\nacross various manipulation tasks, speciﬁcally 16 tasks related to robot manipulation.\nHa [164] proposed a framework aimed at robot skill acquisition. This framework\nprovided a comprehensive solution by utilizing language guidance, without necessitating\nexpert demonstrations or reward speciﬁcation/engineering. It consisted of two main\ncomponents. The ﬁrst component, scaling up language-guided data generation, employed\nLLMs to break down tasks into subtasks and generate a hierarchical plan or task tree. This\nplan was materialized into various robot trajectories using 6-DoF exploration primitives.\nThese trajectories were subsequently veriﬁed and retries were performed as needed until\nsuccess was achieved. This approach enhanced the success rate of data collection and\nmore effectively mitigated the low-level understanding gap in LLMs by incorporating retry\nprocesses as part of the robot’s experiences. The second component, distilling down to\nlanguage-conditioned visuomotor policy, transformed robot experiences into a policy that\ndeduced control sequences from visual observations and natural language task descriptions.\nBy extending diffusion policies, this component handled language-based conditioning for\nmulti-task learning. To assess long-horizon behavior, commonsense reasoning, tool use,\nand intuitive physics, a new multi-task benchmark comprising 18 tasks related to robot\nmanipulation across ﬁve domains (mailbox, transport, drawer, catapult, and bus balance)\nwas developed. This benchmark effectively supported the learning of retry behaviors in\nthe data collection process and enhanced success rates.\nHuang [165], as shown in Figure12, aimed to synthesize dense robot trajectories,\nincluding 6-DoF end-effector waypoints, for various manipulation tasks using an open set"
  },
  {
    "question": "How do visual observations influence the control sequences for robots?",
    "chunk": "learning objective. Experimental results demonstrated that the model effectively separated\naction and perception, achieving enhanced zero-shot and compositional generalization\nacross various manipulation tasks, speciﬁcally 16 tasks related to robot manipulation.\nHa [164] proposed a framework aimed at robot skill acquisition. This framework\nprovided a comprehensive solution by utilizing language guidance, without necessitating\nexpert demonstrations or reward speciﬁcation/engineering. It consisted of two main\ncomponents. The ﬁrst component, scaling up language-guided data generation, employed\nLLMs to break down tasks into subtasks and generate a hierarchical plan or task tree. This\nplan was materialized into various robot trajectories using 6-DoF exploration primitives.\nThese trajectories were subsequently veriﬁed and retries were performed as needed until\nsuccess was achieved. This approach enhanced the success rate of data collection and\nmore effectively mitigated the low-level understanding gap in LLMs by incorporating retry\nprocesses as part of the robot’s experiences. The second component, distilling down to\nlanguage-conditioned visuomotor policy, transformed robot experiences into a policy that\ndeduced control sequences from visual observations and natural language task descriptions.\nBy extending diffusion policies, this component handled language-based conditioning for\nmulti-task learning. To assess long-horizon behavior, commonsense reasoning, tool use,\nand intuitive physics, a new multi-task benchmark comprising 18 tasks related to robot\nmanipulation across ﬁve domains (mailbox, transport, drawer, catapult, and bus balance)\nwas developed. This benchmark effectively supported the learning of retry behaviors in\nthe data collection process and enhanced success rates.\nHuang [165], as shown in Figure12, aimed to synthesize dense robot trajectories,\nincluding 6-DoF end-effector waypoints, for various manipulation tasks using an open set\nof instructions and objects. Huang noted that LLMs were skilled at deriving affordances and\nconstraints from free-form language instructions. Further, by harnessing code generation\ncapabilities, Huang developed 3D value maps for the agent’s observation space through\ninteractions with VLMs. These 3D value maps were integrated into a model-based planning\nframework to generate closed-loop robot trajectories robust to dynamic perturbations in\na zero-shot approach. The proposed framework demonstrated efﬁcient learning of the\ndynamics model for scenes with contact-rich interactions and provided advantages in these\ncomplex scenarios.Appl. Sci.2024, 14, 8868 24 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 24 of 39 \n \nsuccess was achieved. This approach enhanced  t h e  s u c c e s s  r a t e  o f  d a t a  c o l l e c t i o n  a n d  \nmore e ﬀectively mitigated the low-level understanding gap in LLMs by incorporating \nretry processes as part of the robot’s experiences. The second component, distilling down \nto language-conditioned visuomotor policy, transformed robot experiences into a policy \nthat deduced control sequences from visual observations and natural language task \ndescriptions. By extending di ﬀusion policies, this component handled language-based \nconditioning for multi-task learning. To assess long-horizon behavior, commonsense \nreasoning, tool use, and intuitive physics, a new multi-task benchmark comprising 18 \ntasks related to robot manipulation across ﬁve domains (mailbox, transport, drawer, \ncatapult, and bus balance) was developed. This benchmark e ﬀectively supported the \nlearning of retry behaviors in the data collection process and enhanced success rates."
  },
  {
    "question": "What are some important evaluation metrics in vision-and-language navigation?",
    "chunk": "on four key areas: pre-training, adaptation tuning, utilization, and capacity evaluation. Fur-\nthermore, it provided a summary of the resources currently available for the development\nof LLMs and discussed potential future directions for research in this ﬁeld. The survey [12]\nconducted a comprehensive and systematic review of VLMs for visual recognition tasks.\nIt addressed the evolution of the visual recognition paradigm, the principal architectures\nand datasets, and the fundamental principles of VLMs. Moreover, the paper provided\nan overview of the pre-training, transfer learning, and knowledge distillation methods\nemployed in the context of VLMs. The review [3] examined the potential for leveraging\nexisting natural language processing and computer vision foundation models in robotics.\nIn addition, it explored the possibility of developing a robot-speciﬁc foundation model. The\nreview [13] presented an analysis of recent studies on language-based approaches to robotic\nmanipulation. It comprised an analysis of learning paradigms integrated with foundation\nmodels related to manipulation tasks, including semantic information extraction, environ-\nment and evaluation, auxiliary tasks, task representation, safety issues, and other pertinent\nconsiderations. The survey paper [14] presented an analysis of recent research articles that\nemployed foundation models to address robotics challenges. It investigated the extent to\nwhich foundation models enhanced robot performance in perception, decision-making, and\ncontrol. In addition, it examined the obstacles impeding the implementation of foundation\nmodels in robot autonomy and proposed avenues for future advancements. The review\npaper [15] presented a comprehensive review of the research in the ﬁeld of vision and\nlanguage navigation (VLN), encompassing tasks, evaluation metrics, and methodologies\nrelated to autonomous navigation.Appl. Sci.2024, 14, 8868 5 of 39\nTable 1.Useful Review Papers.\nTitle Keywords Ref.\nToward General-Purpose Robots via Foundation\nModels: A Survey and Meta-Analysis Foundation Models [ 3]\nA Survey of Large Language Models LLM [ 5]\nVision-Language Models for Vision Tasks: A Survey VLM [ 12]\nLanguage-conditioned Learning for Robotic\nManipulation: A Survey LLM, VLM, Manipulation [ 13]\nFoundation Models in Robotics: Applications,\nChallenges, and the Future Foundation Models [ 14]\nVision-and-Language Navigation: A Survey of Tasks,\nMethods, and Future Directions VLN [ 15]\n2. Review Protocol\nThis survey covered four databases: Web of Science, ScienceDirect, IEEE Xplore, and\narXiv. In fact, many of the articles surveyed had not been peer-reviewed and published at\nthe time of our search because the subject matter was relatively recent. Therefore, a consid-\nerable number of articles reviewed in this survey were sourced from the arXiv database.\nThe selection process of this study primarily relied on two iterations:\n• The titles and abstracts of the articles were reviewed to eliminate duplicates and\nirrelevant articles.\n• The full texts of the selected articles from the ﬁrst iteration were thoroughly examined\nand categorized.\n• Article searching began on 18 September 2023.\nRegarding the search queries,\n• the publication years were those after 2020,\n• the keywords of Robotics and LLM, which were ((“Robotic” OR “Robotics”) AND\n(“LLM” OR “LM” OR “Large Language Model” OR “Language Model”)), and relevant\njournal and conference articles written in English were considered.\nFrom these search criteria, recent studies utilizing language models in robotics research\nwere expected to be collected. Our aim is to provide a robust understanding of how\nlanguage models and their variants have been utilized to enhance robot intelligence in\nthe literature."
  },
  {
    "question": "How does the iterative improvement of reward functions enhance robotic performance?",
    "chunk": "Figure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses expert-\ndesigned functions through iterative improvements [11]. \nFollowing Eureka, DrEureka [134], shown in Figure 5, was developed to address the \nsim-to-real problem by automatically con ﬁguring appropriate reward functions and do-\nmain randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses\nexpert-designed functions through iterative improvements [11].\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 13 of 39 \n \n \nFigure 4. Eureka leverages LLM to generate reward functions for robotic tasks and surpasses expert-\ndesigned functions through iterative improvements [11]. \nFollowing Eureka, DrEureka [134], shown in Figure 5, was developed to address the \nsim-to-real problem by automatically con ﬁguring appropriate reward functions and do-\nmain randomization for physical environments. DrEureka’s reward-aware physics priors \nmechanism de ﬁnes the lower and upper bounds of physical environment parameters \nbased on policies trained through initial reinforcement learning, facilitating reinforcement \nlearning across various physical environment domains. This randomization enables the \ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation \nmanipulation using real robots, all without human supervision. \n \nFigure 5. DrEureka leverages LLM to design reward fu nctions and solves the sim-to-real problem \nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134]. \nXie [136] introduced Text2Reward, a framework that automatically generated dense \nreward functions for reinforcement learning using LLMs. Provided with a goal expressed \nFigure 5.DrEureka leverages LLM to design reward functions and solves the sim-to-real problem\nthrough its Reward-Aware Physics Priors mechanism and domain randomization [134].\nXie [136] introducedText2Reward, a framework that automatically generated dense\nreward functions for reinforcement learning using LLMs. Provided with a goal expressed\nin natural language, Text2Reward produced executable dense reward functions derived\nfrom a compact representation of the environment. This framework generated free-form\ndense reward codes and delivered performance comparable to or surpassing that of policies\ntrained with expert-designed codes across a variety of tasks, including 17 manipulator-\nrelated tasks and six novel locomotion behaviors. Additionally, Text2Reward incorporated\nuser feedback to iteratively enhance the generated reward functions, thereby increasing the\nsuccess rate of the learned policies.\nDi Palo [137] explored the use of LLMs and VLMs to improve reinforcement learning"
  },
  {
    "question": "What trends can we see in the release of AI models from 2020 to 2024?",
    "chunk": "AI [45]\n2019-02 GPT-2 OpenAI [ 35] 2024-02 StarCoder2 Hugging Face [ 46]\n2019-10 T5 Google [ 47] 2024-03 Claude 3 Anthropic [ 48]\n2020-05 GPT-3 OpenAI [ 49] 2024-03 InternLM2 Shanghai AI Lab [ 50]\n2021-07 Codex OpenAI [ 51] 2024-03 Jamba AI21Labs [ 52]\n2021-09 FLAN Google [ 53] 2024-04 Stabe Code Stability AI [ 54]\n2021-10 T0 Hugging Face [ 37] 2024-04 HyperCLOVA Naver [ 55]\n2021-12 Gopher DeepMind [ 56] 2024-04 Grok-1.5 xAI [ 57]\n2022-03 InstructGPT OpenAI [ 58] 2024-04 Llama3 Meta AI Research [ 59]\n2022-04 PaLM Google [ 60] 2024-04 Phi-3 Microsoft [ 61]\n2022-05 OPT Meta AI Research [ 62] 2024-05 GPT-4o OpenAI [ 1]\n2023-02 LLaMA Meta AI Research [ 63] 2024-06 Claude 3.5 Anthropic [ 64]\n2023-03 Alpaca Stanford Univ. [ 65] 2024-07 GPT-4o mini OpenAI [ 66]\n2023-03 GPT-4 OpenAI [ 50] 2024-07 Falcon2-11B TII [ 67]\n2023-05 StarCoder Hugging Face [ 68] 2024-07 Llama 3.1 405B Meta AI Research [ 69]\n2023-07 LLaMA2 Meta AI Research [ 70] 2024-07 Large2 Mistral AI [ 71]\n2023-09 Baichuan2 Baidu [ 72] 2024-07 Gemma2 Gemma Team,\nGoogle DeepMind [73]\n2023-10 Mistrial Mistral AI [ 74] 2024-08 EXAONE 3 LG AI Research [ 75]\n2024-01 DeepSeek-\nCoder DeepSeek-AI [ 76] 2024-08 Grok-2 and\nGrok-2 mini xAI [ 77]\nTable 3.Chronicle of VLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref.\n2020-05 DETR Facebook AI [ 78] 2023-04 LLaVA UW–Madison [ 79]\n2020-12 DeiT Facebook AI [ 80] 2023-04 MiniGPT-4 KAUST [ 81]\n2021-02 DALL-E OpenAI [ 82] 2023-09 GPT-4V OpenAI [ 83]\n2021-02 CLIP OpenAI [ 7] 2023-11 Florence-2 Microsoft [ 84]\n2021-03 Swin\nTransformer Microsoft [ 85] 2024-01 Lumiere Google Research [ 86]\n2021-05 SegFormer Univ. of Hong\nKong [87] 2024-01 Fuyu Adept [ 88]\n2021-06 Vision\nTransformer\nGoogle Research,\nBrain [89] 2024-03 Gemini 1.5 Gemini Team,\nGoogle [90]\n2021-06 BEiT HIT, Microsoft"
  },
  {
    "question": "What are the benefits of integrating language and visual instructions for robots?",
    "chunk": "LLM. This process allows the LLM to assess the robot’s current state and capabilities, \nultimately generating an interpretable action plan. SayCan was evaluated across 101 robot \ntasks, achieving an 84% plan success rate and a 74% execution success rate in a simulated \nkitchen environment. In a real kitchen setting, the plan success rate decreased slightly to \n81% and the execution success rate fell to 60%, demonstrating that the policy and value \nfunctions generalize well to real-world settings. \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM\nto generate 3D affordance and constraint maps and design robot trajectories without additional\ntraining [165].\nAhn [166] introduced a framework named SayCan, which integrates LLMs with rein-\nforcement learning value functions, enabling robots to follow high-level text instructions.\nSayCan comprises two primary components: Say, which uses an LLM for task-based\ndecision-making, and Can, which evaluates the feasibility of these decisions via reinforce-\nment learning. Say leverages task-based knowledge from the LLM and reinforcement\nlearning functionality to assess the feasibility of task execution by robots in real-world\nscenarios. The LLM determines the actions necessary to achieve high-level goals and\nevaluates the effectiveness of each action in fulﬁlling the instructions. Learned through re-\ninforcement learning, the affordance function estimates each action’s success probability in\nthe current state, conﬁrming the executability of actions proposed by the LLM. This process\nallows the LLM to assess the robot’s current state and capabilities, ultimately generating\nan interpretable action plan. SayCan was evaluated across 101 robot tasks, achieving an\n84% plan success rate and a 74% execution success rate in a simulated kitchen environment.\nIn a real kitchen setting, the plan success rate decreased slightly to 81% and the execution\nsuccess rate fell to 60%, demonstrating that the policy and value functions generalize well\nto real-world settings.\nHuang [167] introduced the Instruct2Act framework, which employs LLMs to se-\nquentially map multi-modality instructions to robot actions. The previous method, CaP,\ngenerated robot policy program code directly from in-context examples based on language\ninstructions. However, this approach was constrained by the capabilities of the generated\ncode and encountered difﬁculties with longer, more complex commands due to the required\nhigh precision of code. To overcome these limitations, Instruct2Act introduced a novel\nstrategy that used multi-modality models and LLMs to simultaneously address recognition,\ntask planning, and low-level control modules. Instruct2Act utilized the segment anything\nmodel for identifying potential objects in input images for multi-modality recognition\nand the CLIP model for object classiﬁcation. As a result, Instruct2Act developed an inte-\ngrated search system capable of managing various input modalities and instruction types,\nincluding both pure language instructions and combined language-visual instructions,\nfacilitating the integration of diverse instruction types into a uniﬁed architecture. Moreover,\nfor pointer-language instructions, the framework supported task segmentation based on\nthe user’s clicks.\n4.5. Scene Understanding in LLMs and VLMs\nTo address the VQA problem, robotics research increasingly uses pre-trained VLMs\nto derive high-level information from visual data. This method is advantageous for scene\nunderstanding as it helps determine affordances that describe the relationship between the\ncurrent state and the next action based on images from cameras. Related studies focus on\naspects of scene understanding.Appl. Sci.2024, 14, 8868 25 of 39"
  },
  {
    "question": "What are the challenges of integrating language models into robotic systems?",
    "chunk": "prompts have the potential to cause the entire robotic system to malfunction. To defend\nagainst this critical threat to the reliability and safety of robotic systems, various techniques\nhave been proposed, such as input validation, which ﬁlters the model’s input, and context\nlocking, which restricts access based on the history and content of the prompt. Furthermore,\nstrict guardrails that restrict harmful or unsafe outputs from models can be an alternative\nto improve the reliability of robotic systems. However, it is essential to recognize that the\nsecurity techniques may potentially lead to a decline in the performance of the robot system.\nConsequently, the trade-off between performance and safety must be carefully considered.\nSince the emergence of ChatGPT and Microsoft’s implementation of robot systems\nusing ChatGPT [2], artiﬁcial intelligence components have been applied more widely and\nintensively in robotics research. Despite existing challenges, it is expected that research\ninvolving foundation models to improve robot intelligence will persist across various\ndomains and methods, which will likely enhance the usability and market potential of\nrobot systems inﬂuenced by these advancements.\n6. Conclusions\nIn this paper, we have explored the potential impact and applicability of LLMs on\nrobotics research ﬁelds by summarizing studies that applied LLMs and VLMs to robots.\nFundamentally, LLMs can enhance the capability of robots in natural language processing\nto interact with humans and to improve the robots’ autonomy in various task scenarios.\nIn particular, the ability of LLMs to understand and generate natural language plays a\ncrucial role in enabling robots to comprehend and execute complex commands. This survey\nconﬁrmed that the scope of utilizing LLMs in robotics was not limited to simple natural\nlanguage processing but also extended to broader research areas. This study explored\nextensive LLM applications in the robotics literature, such as planning, manipulation, and\nscene understanding, as well as reinforcement learning automation frameworks such as\nEureka, and included robot actions in language models such as AutoRT. Moreover, the\nresearch direction of current generative AI models is transitioning towards multimodalAppl. Sci.2024, 14, 8868 30 of 39\nlanguage models, moving beyond information acquisition and cognition aspects such as\ntext, images, and videos to include actuator actions within large models in therobotics ﬁeld.\nWhile the surveyed studies indicated that LLMs play a promising role in the future\nof robotics, certain limitations were also identiﬁed. First, the increased computational\nresources and energy consumption associated with embedding LLMs into robotic systems\nmust be addressed. Second, biases in language models and ethical considerations are\nsigniﬁcant issues that need to be tackled in robotics. Therefore, continual efforts will be\nnecessary in future research to resolve these challenges.\nOverall, LLMs are valuable tools that can signiﬁcantly advance robotics. This review\nhas revealed that innovative robot applications are possible through the integration of\nLLMs and VLMs. Moreover, these foundation models are expected to serve as critical\nelements for future robot research and practical applications in the real world.\nAuthor Contributions:Conceptualization, S.S. and C.K.; methodology, S.S.; formal analysis, H.J.,\nH.L. and S.S.; investigation, H.J., H.L. and S.S.; resources, H.J., H.L., S.S. and C.K.; writing—original\ndraft preparation, H.J., H.L., S.S. and C.K.; writing—review and editing, H.J., H.L., S.S. and C.K.; vi-"
  },
  {
    "question": "What are the storage implications of using LoRA for fine-tuning?",
    "chunk": "model focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM \nmodel inference. \n \nFigure 3. An overview of four strategies for parameter-eﬃcient ﬁne-tuning: (a) Adapter Tuning, \n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5]. \nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a \nlow-rank constraint on transformer layers to approximate the update matrices through \ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates \nthe parameter updates using low-rank dec omposition matrices. The primary bene ﬁt of \nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning, \nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory \nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning. \nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119]. \n3.3. Prompt Techniques for Increasing LLM Performance \nTo enhance the performance of LLMs, the most straightforward approach involves \ntraining with additional data via ﬁne-tuning techniques, which mirrors supervised learn-\ning in conventional machine learning. Another method for improving performance in-\nvolves the use of in-context learning, which capitalizes on prompts for zero-shot learning, \na capability ﬁrst observed in LLMs with the advent of GPT-3. The adaptation of these \nprompts for speciﬁc tasks is known as prompt engineering. Fundamentally, prompt engi-\nneering (or prompting) entails supplying inputs  to the model to perform a distinct task, \ndesigning the input format to encapsulate the task’s purpose and context, and generating \nthe desired output. The four components of pr ompt engineering can be analyzed as fol-\nlows: within the prompt, “ Instructions” delineate the speci ﬁc tasks or directives for the \nmodel and “Context” provides external or additional contextual information that can tune \nthe model. Furthermore, “Input data” refers to the type of input or questions seeking an-\nswers, and “ Output data” deﬁnes the output type or format within the prompt, thereby \noptimizing the LLM’s performance for particular tasks. Various methodologies for creat-\ning prompts have been introduced, as described below. \nFigure 3. An overview of four strategies for parameter-efﬁcient ﬁne-tuning: (a) Adapter Tuning,\n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5].\nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a\nlow-rank constraint on transformer layers to approximate the update matrices through\ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates\nthe parameter updates using low-rank decomposition matrices. The primary beneﬁt of\nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning,\nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory\nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning.\nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119].\n3.3. Prompt Techniques for Increasing LLM Performance"
  },
  {
    "question": "What are the benefits of using language models for task planning in robots?",
    "chunk": "understand the situation and an LLM to propose possible tasks. By inputting the robot’s\noperational guidelines and safety constraints into the LLM as prompts, AutoRT assesses\nthe validity of the proposed tasks and the necessity for human intervention. Throughout\nthis process, AutoRT safely selects and executes feasible tasks while collecting relevant\ndata.\nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for\nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical causal-\nity in the real world, problem-solving through trial-and-error feedback, and code generation\nabilities. Eureka can autonomously generate reward functions for a variety of tasks and\nrobots without needing speciﬁc templates for each. This allows for the generation of\nhuman-level reward functions for diverse robots and tasks without human input. Further-\nmore, Eureka has demonstrated the ability to solve complex problems that were previously\nunsolved by expert-designed reward functions.\nGiven these research outcomes, integrating language models into robotic intelligence\npresents signiﬁcant potential to enhance robot capabilities and applications dramatically,\nthereby redeﬁning their roles in diverse industries and everyday life. Therefore, this survey\npaper explores recent research trends in LLM- and VLM-based robot intelligence, aiming to\nprovide a comprehensive understanding of future development possibilities by examining\nthe application of language models in various robotic research ﬁelds. It also seeks to\nhighlight research cases, identify current limitations, and suggest future research directions.\nTo chronicle this advancement in robotics research ﬁelds, this review paper presents\nthe following contributions:\n• This paper summarizes and introduces the foundational elements and tuning methods\nof LLM architecture.\n• It explores and arranges prompt techniques to enhance the problem-solving abilities\nof LLMs.\n• It reviews and encapsulates how LLMs and VLMs have been employed to augment\nrobot intelligence across ﬁve topics as shown in Figure1: (1) reward design for\nreinforcement learning, (2) low-level control, (3) high-level planning, (4) manipulation,\nand (5) scene understanding.\nAppl. Sci. 2024 , 14 , x FOR PEER REVIEW 3 of 39 \n \nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which \nenable low-level actuator control using LLM s and VLMs, Google has introduced AutoRT \n[10]. AutoRT is a system where robots interact  with real-world objects to collect motion \ndata. It begins by exploring th e surrounding space to identify feasible tasks, then uses a \nVLM to understand the situation and an LLM to propose possible tasks. By inpu tting the \nrobot’s operational guidelines and safety co nstraints into the LLM as prompts, AutoRT \nassesses the validity of the proposed tasks and the necessity for human intervention. \nThroughout this process, AutoRT safely selects and executes feasible tasks while collecting \nrelevant data. \nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for \nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical cau-\nsality in the real world, problem-solving through trial-and-error feedback, and code gen-\neration abilities. Eureka can autonomously generate reward functions for a variety of tasks \nand robots without needing speci ﬁc templates for each. This allows for the generation of \nhuman-level reward functions for diverse robo ts and tasks without human input. Further-"
  },
  {
    "question": "How does ProgPrompt utilize Python in its task planning?",
    "chunk": "robots to carry out instructions: (a) mobile manipulation and (b,c) tabletop manipulation, in both\nsimulated and real-world environments [153].Appl. Sci.2024, 14, 8868 20 of 39\nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn,\na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot\nbehavior trees (BTs) from textual descriptions. The developed model was compact enough\nto operate on a robot’s onboard microcomputer, while adept at constructing complex robot\nbehaviors. It provided structurally and logically correct BTs and demonstrated the ability\nto handle instructions that were not included in the training set.\nSong [155], as shown in Figure9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions\nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via\na low-level planner. It continuously updated environmental information as new objects\nwere detected during action implementation and revisited the LLM to adjust the plan if\nsubgoals failed or were delayed based on updated observations. This iterative process was\nrepeated until the subgoal was achieved, after which the system moved to the next goal.\nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated\ncompetitive performance with signiﬁcantly reduced training data and proved its ability to\ngeneralize in various tasks (e.g., ALFRED) with minimal examples.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 9.LLM-Planner is a system that creates high-level plans based on natural language commands,\nsets subgoals to determine actions, and continuously updates the plan to reﬂect environmental\nchanges [155].\nSingh [156], as shown in Figure10, introduced ProgPrompt, a programmatic LLM\nprompt structure designed for generating plans across diverse situated environments,\nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that\nleveraged LLMs and included a Python programming structure to facilitate information\nabout the environment and executable actions. It featured a feedback mechanism, using\nexecutable program plan examples and assertion statements to mitigate errors, enhancing"
  },
  {
    "question": "How does the affordance function work in evaluating robot actions?",
    "chunk": "LLM. This process allows the LLM to assess the robot’s current state and capabilities, \nultimately generating an interpretable action plan. SayCan was evaluated across 101 robot \ntasks, achieving an 84% plan success rate and a 74% execution success rate in a simulated \nkitchen environment. In a real kitchen setting, the plan success rate decreased slightly to \n81% and the execution success rate fell to 60%, demonstrating that the policy and value \nfunctions generalize well to real-world settings. \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM\nto generate 3D affordance and constraint maps and design robot trajectories without additional\ntraining [165].\nAhn [166] introduced a framework named SayCan, which integrates LLMs with rein-\nforcement learning value functions, enabling robots to follow high-level text instructions.\nSayCan comprises two primary components: Say, which uses an LLM for task-based\ndecision-making, and Can, which evaluates the feasibility of these decisions via reinforce-\nment learning. Say leverages task-based knowledge from the LLM and reinforcement\nlearning functionality to assess the feasibility of task execution by robots in real-world\nscenarios. The LLM determines the actions necessary to achieve high-level goals and\nevaluates the effectiveness of each action in fulﬁlling the instructions. Learned through re-\ninforcement learning, the affordance function estimates each action’s success probability in\nthe current state, conﬁrming the executability of actions proposed by the LLM. This process\nallows the LLM to assess the robot’s current state and capabilities, ultimately generating\nan interpretable action plan. SayCan was evaluated across 101 robot tasks, achieving an\n84% plan success rate and a 74% execution success rate in a simulated kitchen environment.\nIn a real kitchen setting, the plan success rate decreased slightly to 81% and the execution\nsuccess rate fell to 60%, demonstrating that the policy and value functions generalize well\nto real-world settings.\nHuang [167] introduced the Instruct2Act framework, which employs LLMs to se-\nquentially map multi-modality instructions to robot actions. The previous method, CaP,\ngenerated robot policy program code directly from in-context examples based on language\ninstructions. However, this approach was constrained by the capabilities of the generated\ncode and encountered difﬁculties with longer, more complex commands due to the required\nhigh precision of code. To overcome these limitations, Instruct2Act introduced a novel\nstrategy that used multi-modality models and LLMs to simultaneously address recognition,\ntask planning, and low-level control modules. Instruct2Act utilized the segment anything\nmodel for identifying potential objects in input images for multi-modality recognition\nand the CLIP model for object classiﬁcation. As a result, Instruct2Act developed an inte-\ngrated search system capable of managing various input modalities and instruction types,\nincluding both pure language instructions and combined language-visual instructions,\nfacilitating the integration of diverse instruction types into a uniﬁed architecture. Moreover,\nfor pointer-language instructions, the framework supported task segmentation based on\nthe user’s clicks.\n4.5. Scene Understanding in LLMs and VLMs\nTo address the VQA problem, robotics research increasingly uses pre-trained VLMs\nto derive high-level information from visual data. This method is advantageous for scene\nunderstanding as it helps determine affordances that describe the relationship between the\ncurrent state and the next action based on images from cameras. Related studies focus on\naspects of scene understanding.Appl. Sci.2024, 14, 8868 25 of 39"
  },
  {
    "question": "How does AutoRT gather real-world data with its robots?",
    "chunk": "of-thought prompting techniques to RT-2 has proven effective in solving more complex\nsemantic inference tasks, such as using a rock as an improvised hammer or offering an\nenergy drink instead of a carbonated beverage to a thirsty person. In comparison with\nthe earlier study on RT-1, RT-2 demonstrates enhanced performance in both familiar and\nnovel tasks.\nAutoRT [10] is a follow-up study based on the research results of RT-1 and RT-2,\nestablishing an orchestration of large-scale robotic agents for data collection in real-world\nscenarios. AutoRT employed 53 robots to gather 77,000 real robot episodes over seven\nmonths through both teleoperation and autonomous robot policies. At the heart of AutoRT\nis a robust foundation model that generates ‘task proposals’ based on given visual observa-\ntions. Notably, AutoRT introduces a ‘Robot Constitution’ using constitutional prompting to\nensure actions during the task proposal process do not compromise the safety of the robot\nor nearby individuals. This Robot Constitution, inspired by Asimov’s three laws [142],\ncomprises basic rules, safety rules that identify unsafe or unwanted tasks, and embodiment\nrules that clarify the robot’s operational boundaries.\nAutoRT enhances data collection by initially scanning the surroundings to identify\ninteresting scenes or tasks (exploration). It interprets the given context through a VLM\nand proposes potential tasks via an LLM (task generation). Subsequently, tasks suggested\nby the LLM are screened (affordance) to assess their feasibility and the need for human\nintervention, employing the Robot Constitution. During this procedure, viable tasks are\nchosen and performed, while pertinent data are gathered (data collection). The collected\ndata are then assessed for (diversity scoring) the visual diversity of the robot trajectories\nand the linguistic diversity of the language instructions generated by AutoRT (LLM). The\naim of this diversity evaluation is to conﬁrm that, unlike simulations, real-world data\ncollection by robots is labor-intensive, making it essential to gather data across a broad\nspectrum of tasks. Experimental outcomes illustrate that AutoRT achieves higher visual\nand linguistic diversity compared to RT-1 or BC-Z [143].\nOther researchers include Tang [144], who developed an approach that connects\nnatural language user commands with a locomotion controller using foot contact patterns as\nan interface for low-level commands. This innovative interface translates human commands\ninto the robot’s foot contact patterns, allowing the robot to move at a speciﬁed speed with\nprecise timing for each foot’s contact with the ground. To achieve this, the robot used a\ncyclic sliding window to extract foot contact ﬂags from a pattern template, thus generating\nthe required foot contact patterns. During training, a random pattern generator created\nfoot contact patterns, and during testing, an LLM translated human commands into these\npatterns. The robot then adjusted its movements based on the foot contact patterns it\nlearned through deep reinforcement learning, closely adhering to the intended foot contact\npatterns and speed commands. This approach demonstrated a 50% higher success rate in\ntask evaluation (across 30 tasks, including standing still) compared to two baselines (which\nemployed discrete gaits and sinusoidal functions as interfaces), successfully solving 10\nmore tasks than the baselines.\nMandi [145] introduced a novel method for multi-robot collaboration that utilizes\nLLMs for both high-level communication and low-level path planning. In this method, the\nrobots employ the LLM to discuss and reason about task strategies. They generate sub-taskAppl. Sci.2024, 14, 8868 16 of 39\nplans and task space waypoint paths, which a multi-arm motion planner then uses to expe-"
  },
  {
    "question": "Which organizations have contributed to the development of AI models recently?",
    "chunk": "AI [45]\n2019-02 GPT-2 OpenAI [ 35] 2024-02 StarCoder2 Hugging Face [ 46]\n2019-10 T5 Google [ 47] 2024-03 Claude 3 Anthropic [ 48]\n2020-05 GPT-3 OpenAI [ 49] 2024-03 InternLM2 Shanghai AI Lab [ 50]\n2021-07 Codex OpenAI [ 51] 2024-03 Jamba AI21Labs [ 52]\n2021-09 FLAN Google [ 53] 2024-04 Stabe Code Stability AI [ 54]\n2021-10 T0 Hugging Face [ 37] 2024-04 HyperCLOVA Naver [ 55]\n2021-12 Gopher DeepMind [ 56] 2024-04 Grok-1.5 xAI [ 57]\n2022-03 InstructGPT OpenAI [ 58] 2024-04 Llama3 Meta AI Research [ 59]\n2022-04 PaLM Google [ 60] 2024-04 Phi-3 Microsoft [ 61]\n2022-05 OPT Meta AI Research [ 62] 2024-05 GPT-4o OpenAI [ 1]\n2023-02 LLaMA Meta AI Research [ 63] 2024-06 Claude 3.5 Anthropic [ 64]\n2023-03 Alpaca Stanford Univ. [ 65] 2024-07 GPT-4o mini OpenAI [ 66]\n2023-03 GPT-4 OpenAI [ 50] 2024-07 Falcon2-11B TII [ 67]\n2023-05 StarCoder Hugging Face [ 68] 2024-07 Llama 3.1 405B Meta AI Research [ 69]\n2023-07 LLaMA2 Meta AI Research [ 70] 2024-07 Large2 Mistral AI [ 71]\n2023-09 Baichuan2 Baidu [ 72] 2024-07 Gemma2 Gemma Team,\nGoogle DeepMind [73]\n2023-10 Mistrial Mistral AI [ 74] 2024-08 EXAONE 3 LG AI Research [ 75]\n2024-01 DeepSeek-\nCoder DeepSeek-AI [ 76] 2024-08 Grok-2 and\nGrok-2 mini xAI [ 77]\nTable 3.Chronicle of VLM models.\nRelease Date Model Name Developer Ref. Release Date Model Name Developer Ref.\n2020-05 DETR Facebook AI [ 78] 2023-04 LLaVA UW–Madison [ 79]\n2020-12 DeiT Facebook AI [ 80] 2023-04 MiniGPT-4 KAUST [ 81]\n2021-02 DALL-E OpenAI [ 82] 2023-09 GPT-4V OpenAI [ 83]\n2021-02 CLIP OpenAI [ 7] 2023-11 Florence-2 Microsoft [ 84]\n2021-03 Swin\nTransformer Microsoft [ 85] 2024-01 Lumiere Google Research [ 86]\n2021-05 SegFormer Univ. of Hong\nKong [87] 2024-01 Fuyu Adept [ 88]\n2021-06 Vision\nTransformer\nGoogle Research,\nBrain [89] 2024-03 Gemini 1.5 Gemini Team,\nGoogle [90]\n2021-06 BEiT HIT, Microsoft"
  },
  {
    "question": "How does reinforcement learning automation frameworks like Eureka relate to robotics?",
    "chunk": "language processing but also extended to broader research areas. This study explored\nextensive LLM applications in the robotics literature, such as planning, manipulation, and\nscene understanding, as well as reinforcement learning automation frameworks such as\nEureka, and included robot actions in language models such as AutoRT. Moreover, the\nresearch direction of current generative AI models is transitioning towards multimodalAppl. Sci.2024, 14, 8868 30 of 39\nlanguage models, moving beyond information acquisition and cognition aspects such as\ntext, images, and videos to include actuator actions within large models in therobotics ﬁeld.\nWhile the surveyed studies indicated that LLMs play a promising role in the future\nof robotics, certain limitations were also identiﬁed. First, the increased computational\nresources and energy consumption associated with embedding LLMs into robotic systems\nmust be addressed. Second, biases in language models and ethical considerations are\nsigniﬁcant issues that need to be tackled in robotics. Therefore, continual efforts will be\nnecessary in future research to resolve these challenges.\nOverall, LLMs are valuable tools that can signiﬁcantly advance robotics. This review\nhas revealed that innovative robot applications are possible through the integration of\nLLMs and VLMs. Moreover, these foundation models are expected to serve as critical\nelements for future robot research and practical applications in the real world.\nAuthor Contributions:Conceptualization, S.S. and C.K.; methodology, S.S.; formal analysis, H.J.,\nH.L. and S.S.; investigation, H.J., H.L. and S.S.; resources, H.J., H.L., S.S. and C.K.; writing—original\ndraft preparation, H.J., H.L., S.S. and C.K.; writing—review and editing, H.J., H.L., S.S. and C.K.; vi-\nsualization, H.J. and H.L.; supervision, S.S. and C.K.; project administration, S.S.; funding acquisition,\nS.S. All authors have read and agreed to the published version of the manuscript.\nFunding: This work was supported by the Technology Innovation Program (RS-2024-00423702,\nA Meta-Humanoid with Hypermodal Cognitivity and Role Dexterity: Adroid4X) funded by the\nMinistry of Trade, Industry, and Energy (MOTIE, Korea) and Regional Innovation Strategy (RIS)\nthrough the National Research Foundation of Korea (NRF) funded by the Ministry of Education\n(MOE) (2023RIS-007).\nInstitutional Review Board Statement:Not applicable.\nInformed Consent Statement:Not applicable.\nData Availability Statement:No new data were created or analyzed in this study. Data sharing is\nnot applicable to this article.\nConﬂicts of Interest:The authors declare no conﬂicts of interest.\nReferences\n1. Hello GPT-4o. Available online:https://openai.com/index/hello-gpt-4o/ (accessed on 13 August 2024).\n2. Vemprala, S.H.; Bonatti, R.; Bucker, A.; Kapoor, A. ChatGPT for Robotics: Design Principles and Model Abilities.IEEE Access\n2024, 12, 55682–55696. [CrossRef]\n3. Hu, Y.; Xie, Q.; Jain, V.; Francis, J.; Patrikar, J.; Keetha, N.; Kim, S.; Xie, Y.; Zhang, T.; Zhao, S.; et al. Toward General-Purpose\nRobots via Foundation Models: A Survey and Meta-Analysis.arXiv 2023, arXiv:2312.08782."
  },
  {
    "question": "What innovative applications of LLMs in robotics have been identified?",
    "chunk": "prompts have the potential to cause the entire robotic system to malfunction. To defend\nagainst this critical threat to the reliability and safety of robotic systems, various techniques\nhave been proposed, such as input validation, which ﬁlters the model’s input, and context\nlocking, which restricts access based on the history and content of the prompt. Furthermore,\nstrict guardrails that restrict harmful or unsafe outputs from models can be an alternative\nto improve the reliability of robotic systems. However, it is essential to recognize that the\nsecurity techniques may potentially lead to a decline in the performance of the robot system.\nConsequently, the trade-off between performance and safety must be carefully considered.\nSince the emergence of ChatGPT and Microsoft’s implementation of robot systems\nusing ChatGPT [2], artiﬁcial intelligence components have been applied more widely and\nintensively in robotics research. Despite existing challenges, it is expected that research\ninvolving foundation models to improve robot intelligence will persist across various\ndomains and methods, which will likely enhance the usability and market potential of\nrobot systems inﬂuenced by these advancements.\n6. Conclusions\nIn this paper, we have explored the potential impact and applicability of LLMs on\nrobotics research ﬁelds by summarizing studies that applied LLMs and VLMs to robots.\nFundamentally, LLMs can enhance the capability of robots in natural language processing\nto interact with humans and to improve the robots’ autonomy in various task scenarios.\nIn particular, the ability of LLMs to understand and generate natural language plays a\ncrucial role in enabling robots to comprehend and execute complex commands. This survey\nconﬁrmed that the scope of utilizing LLMs in robotics was not limited to simple natural\nlanguage processing but also extended to broader research areas. This study explored\nextensive LLM applications in the robotics literature, such as planning, manipulation, and\nscene understanding, as well as reinforcement learning automation frameworks such as\nEureka, and included robot actions in language models such as AutoRT. Moreover, the\nresearch direction of current generative AI models is transitioning towards multimodalAppl. Sci.2024, 14, 8868 30 of 39\nlanguage models, moving beyond information acquisition and cognition aspects such as\ntext, images, and videos to include actuator actions within large models in therobotics ﬁeld.\nWhile the surveyed studies indicated that LLMs play a promising role in the future\nof robotics, certain limitations were also identiﬁed. First, the increased computational\nresources and energy consumption associated with embedding LLMs into robotic systems\nmust be addressed. Second, biases in language models and ethical considerations are\nsigniﬁcant issues that need to be tackled in robotics. Therefore, continual efforts will be\nnecessary in future research to resolve these challenges.\nOverall, LLMs are valuable tools that can signiﬁcantly advance robotics. This review\nhas revealed that innovative robot applications are possible through the integration of\nLLMs and VLMs. Moreover, these foundation models are expected to serve as critical\nelements for future robot research and practical applications in the real world.\nAuthor Contributions:Conceptualization, S.S. and C.K.; methodology, S.S.; formal analysis, H.J.,\nH.L. and S.S.; investigation, H.J., H.L. and S.S.; resources, H.J., H.L., S.S. and C.K.; writing—original\ndraft preparation, H.J., H.L., S.S. and C.K.; writing—review and editing, H.J., H.L., S.S. and C.K.; vi-"
  },
  {
    "question": "What is the process of creating and modifying action plans in ReAct?",
    "chunk": "demonstrations and calls on external tools as necessary to integrate their outputs into the\nreasoning process. The model generalizes from demonstrations using tools to decompose\nnew tasks and learns to use tools effectively. Enhancing ART’s performance is possible\nby modifying the task library or incorporating new tools.Automatic prompt engineer\n(APE) [129] is a framework designed for the automatic generation and selection of com-\nmands. The model generates command candidates for a problem and selects the most\nsuitable one based on a scoring function, such as execution accuracy or log probability.\nDirectional stimulus prompting[130] is a technique that directs the model to consider\nand generate responses in a particular direction. By deploying a tunable policy LM (e.g.,\nT5 [47]), it creates directional stimulus prompts for each input and uses these as cues to\nsteer the model toward producing the desired outcomes [131]. ReAct combines reasoning\nwith action within the model. It enables the model to perform reasoning in generating\nanswers, take actions based on external sources (e.g., documents, articles, and news), and\nreﬁne reasoning based on observations of these actions. This process facilitates the creation,\nmaintenance, and modiﬁcation of action plans while incorporating additional information\nfrom interactions with external sources.Reﬂexion [132] augments language-based agents\nwith language feedback. Reﬂexion involves three models: the actor, the evaluator, and self-\nreﬂection. The actor initiates actions within a speciﬁc environment to generate task steps,\nthe evaluator assesses these steps, and self-reﬂection provides linguistic feedback, which\nthe actor uses to formulate new steps and achieve the task’s objective. The introduced\nprompt techniques are summarized in Table4.\nTable 4.Prompt Techniques.\nName Explanation Ref.\nZero-Shot Prompting Enabling the model to perform new tasks without any examples [ 53]\nFew-Shot Prompting Providing a few examples to enable performing new tasks [ 49]\nChain-of-Thought Explicitly generating intermediate reasoning steps to perform\nstep-by-step inference [41]\nSelf-Consistency\nGenerating various reasoning paths independently through\nFew-Shot CoT, with each path going through a prompt generation\nprocess to select the most consistent answer\n[120]\nGenerated Knowledge Prompting\nIntegrating knowledge and information relevant to a question, and\nthen providing it along with the question to generate accurate\nanswers\n[126]\nPrompt Chaining Dividing a task into sub-tasks and connecting prompts for each\nsub-task as input–output pairs [125]\nTree of Thoughts\nDividing a problem into subproblems with intermediate steps that\nserve as “thoughts” towards solving the problem, where each\nthought undergoes an inference process and self-evaluates its\nprogress towards solving the problem\n[124]\nRetrieval Augmented Generation Combining external information retrieval with natural language\ngeneration [127]\nAutomatic Reasoning and Tool-use Using external tools to automatically generate intermediate\nreasoning steps [128]\nAutomatic Prompt Engineer Automatically generating and selecting commands [ 129]\nActive Prompt Addressing the issue that the effectiveness may be limited by\nhuman annotations [122]\nDirectional Stimulus Prompting Guiding the model to think and generate responses in a speciﬁc\ndirection [130]\nProgram-Aided Language Models Using models to understand natural language problems and\ngenerate programs as intermediate reasoning steps [123]\nReAct Combining reasoning and actions within a mode [ 131]\nReﬂexion Enhancing language-based agents through language feedback [ 132]"
  },
  {
    "question": "What is the zero-shot approach for identifying objects in a room?",
    "chunk": "In a real kitchen setting, the plan success rate decreased slightly to 81% and the execution\nsuccess rate fell to 60%, demonstrating that the policy and value functions generalize well\nto real-world settings.\nHuang [167] introduced the Instruct2Act framework, which employs LLMs to se-\nquentially map multi-modality instructions to robot actions. The previous method, CaP,\ngenerated robot policy program code directly from in-context examples based on language\ninstructions. However, this approach was constrained by the capabilities of the generated\ncode and encountered difﬁculties with longer, more complex commands due to the required\nhigh precision of code. To overcome these limitations, Instruct2Act introduced a novel\nstrategy that used multi-modality models and LLMs to simultaneously address recognition,\ntask planning, and low-level control modules. Instruct2Act utilized the segment anything\nmodel for identifying potential objects in input images for multi-modality recognition\nand the CLIP model for object classiﬁcation. As a result, Instruct2Act developed an inte-\ngrated search system capable of managing various input modalities and instruction types,\nincluding both pure language instructions and combined language-visual instructions,\nfacilitating the integration of diverse instruction types into a uniﬁed architecture. Moreover,\nfor pointer-language instructions, the framework supported task segmentation based on\nthe user’s clicks.\n4.5. Scene Understanding in LLMs and VLMs\nTo address the VQA problem, robotics research increasingly uses pre-trained VLMs\nto derive high-level information from visual data. This method is advantageous for scene\nunderstanding as it helps determine affordances that describe the relationship between the\ncurrent state and the next action based on images from cameras. Related studies focus on\naspects of scene understanding.Appl. Sci.2024, 14, 8868 25 of 39\nChen [168] explored methods to integrate commonsense into scene understanding\nusing LLMs and introduced three paradigms for classifying room types within indoor\nenvironments based on included objects. The zero-shot approach utilized a pre-trained\nlanguage model to identify the objects in a room and estimate their types. The feed-\nforward classiﬁer approach involved inputting sentences that listed a room’s objects into\nthe language model to generate embedding vectors, which were subsequently input into a\npre-trained shallow multilayer perceptron to predict each room type. Lastly, the classiﬁer\napproach embedded images of rooms alongside textual descriptions to identify the best-\nmatching description, thereby determining the room type. These paradigms demonstrated\nthe capacity to generalize to objects not presented in the training set and to make inferences\nwithin a space larger than that deﬁned by the trained object labels.\nYang [169] introduced the innovative zero-shot, open-vocabulary, LLM-based 3D\nvisual grounding pipeline called LLM-Grounder. This method breaks down complex natu-\nral language queries into semantic components and uses visual grounding tools such as\nOpenScene or LERF to locate objects within 3D scenes. Subsequently, the LLM evaluates\nspatial and commonsense relationships among these objects to achieve the ﬁnal grounding.\nRemarkably, LLM-Grounder operates without labeled training data and has proven its ca-\npacity to adapt to new 3D scenes and diverse text queries, enhancing grounding capabilities\nfor complex language queries and establishing itself as an effective solution.\nChen [170] developed NLMap, an open-vocabulary, queryable scene representation\nsystem. Designed to accumulate and incorporate contextual data within a scene repre-\nsentation for natural language queries, this system allows an LLM planner to visualize"
  },
  {
    "question": "How do VLMs enhance a robot's situational awareness?",
    "chunk": "mand systems. Consequently, robots can respond more adaptably and intelligently in\ninteractions with human users, allowing them to engage in complex problem-solving and\ndecision-making processes beyond simple mechanical tasks.\nAdditionally, LLMs not only enhance a robot’s communication skills to improve HRI\nusability but also boost the robot’s planning abilities. Planning involves setting goals\nand devising a sequence of actions to achieve them, which are essential in determining a\nrobot’s autonomy and efﬁciency. LLMs interpret natural language from users and complex\ncommands, enabling robots to establish and execute suitable plans in various situations.\nMoreover, LLMs adapt ﬂexibly to new situations through a zero-shot approach and utilize\npast data for learning. These capabilities indicate that robots can play a vital role in\nautonomously navigating changing environments and resolving unexpected issues.\nMoreover, VLMs such as CLIP [7], which are trained to solve vision question answering\n(VQA) tasks, have the ability to process visual and linguistic information simultaneously.\nThis ability allows robots to visually perceive their surroundings and integrate this infor-\nmation into linguistic descriptions, enabling more sophisticated situational awareness. For\ninstance, using VLMs, a robot can recognize objects and provide descriptions, as well as\nunderstand and execute user commands based on visual cues. This integrated approach\nsigniﬁcantly enhances a robot’s autonomy and interaction capabilities.\nIn practice, building on the capabilities of predecessors RT-1 [8] and RT-2 [9], which en-\nable low-level actuator control using LLMs and VLMs, Google has introduced AutoRT [10].\nAutoRT is a system where robots interact with real-world objects to collect motion data. ItAppl. Sci.2024, 14, 8868 3 of 39\nbegins by exploring the surrounding space to identify feasible tasks, then uses a VLM to\nunderstand the situation and an LLM to propose possible tasks. By inputting the robot’s\noperational guidelines and safety constraints into the LLM as prompts, AutoRT assesses\nthe validity of the proposed tasks and the necessity for human intervention. Throughout\nthis process, AutoRT safely selects and executes feasible tasks while collecting relevant\ndata.\nNvidia has also introduced Eureka (Evolution-driven Universal REward Kit for\nAgent) [11], a system that automatically designs reward functions for reinforcement learn-\ning problems using the capabilities of LLMs, which include understanding physical causal-\nity in the real world, problem-solving through trial-and-error feedback, and code generation\nabilities. Eureka can autonomously generate reward functions for a variety of tasks and\nrobots without needing speciﬁc templates for each. This allows for the generation of\nhuman-level reward functions for diverse robots and tasks without human input. Further-\nmore, Eureka has demonstrated the ability to solve complex problems that were previously\nunsolved by expert-designed reward functions.\nGiven these research outcomes, integrating language models into robotic intelligence\npresents signiﬁcant potential to enhance robot capabilities and applications dramatically,\nthereby redeﬁning their roles in diverse industries and everyday life. Therefore, this survey\npaper explores recent research trends in LLM- and VLM-based robot intelligence, aiming to\nprovide a comprehensive understanding of future development possibilities by examining\nthe application of language models in various robotic research ﬁelds. It also seeks to\nhighlight research cases, identify current limitations, and suggest future research directions.\nTo chronicle this advancement in robotics research ﬁelds, this review paper presents\nthe following contributions:\n• This paper summarizes and introduces the foundational elements and tuning methods\nof LLM architecture.\n• It explores and arranges prompt techniques to enhance the problem-solving abilities\nof LLMs."
  },
  {
    "question": "Can you explain what prompt engineering is in the context of LLMs?",
    "chunk": "model focuses on identifying the optimal preﬁx vectors, which are retained for use in LLM \nmodel inference. \n \nFigure 3. An overview of four strategies for parameter-eﬃcient ﬁne-tuning: (a) Adapter Tuning, \n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5]. \nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a \nlow-rank constraint on transformer layers to approximate the update matrices through \ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates \nthe parameter updates using low-rank dec omposition matrices. The primary bene ﬁt of \nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning, \nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory \nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning. \nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119]. \n3.3. Prompt Techniques for Increasing LLM Performance \nTo enhance the performance of LLMs, the most straightforward approach involves \ntraining with additional data via ﬁne-tuning techniques, which mirrors supervised learn-\ning in conventional machine learning. Another method for improving performance in-\nvolves the use of in-context learning, which capitalizes on prompts for zero-shot learning, \na capability ﬁrst observed in LLMs with the advent of GPT-3. The adaptation of these \nprompts for speciﬁc tasks is known as prompt engineering. Fundamentally, prompt engi-\nneering (or prompting) entails supplying inputs  to the model to perform a distinct task, \ndesigning the input format to encapsulate the task’s purpose and context, and generating \nthe desired output. The four components of pr ompt engineering can be analyzed as fol-\nlows: within the prompt, “ Instructions” delineate the speci ﬁc tasks or directives for the \nmodel and “Context” provides external or additional contextual information that can tune \nthe model. Furthermore, “Input data” refers to the type of input or questions seeking an-\nswers, and “ Output data” deﬁnes the output type or format within the prompt, thereby \noptimizing the LLM’s performance for particular tasks. Various methodologies for creat-\ning prompts have been introduced, as described below. \nFigure 3. An overview of four strategies for parameter-efﬁcient ﬁne-tuning: (a) Adapter Tuning,\n(b) Preﬁx Tuning, (c) Prompt Tuning, and (d) Low-Rank Adaptation [5].\nIn practice, a commonly employed method for LLM ﬁne-tuning, LoRA [118], uses a\nlow-rank constraint on transformer layers to approximate the update matrices through\ntraining. This method keeps the original LLM parameter matrices ﬁxed and approximates\nthe parameter updates using low-rank decomposition matrices. The primary beneﬁt of\nLoRA is a substantial reduction in the memory and storage requirements for ﬁne-tuning,\nsuch as VRAM. Additionally, quantization methods, which directly minimize the memory\nsize required for parameter representation, are frequently utilized in LLM ﬁne-tuning.\nSpeciﬁcally, the practice of merging LoRA with quantization is known as QLoRA [119].\n3.3. Prompt Techniques for Increasing LLM Performance"
  },
  {
    "question": "How does ProgPrompt improve task performance in robotic environments?",
    "chunk": "robots to carry out instructions: (a) mobile manipulation and (b,c) tabletop manipulation, in both\nsimulated and real-world environments [153].Appl. Sci.2024, 14, 8868 20 of 39\nLykov [154] introduced a novel approach to autonomous robot control named LLM-\nBRAIn, which facilitated the command-based generation of robot behaviors. LLM-BRAIn,\na transformer-based LLM, ﬁne-tuned the Stanford Alpaca 7B model to generate robot\nbehavior trees (BTs) from textual descriptions. The developed model was compact enough\nto operate on a robot’s onboard microcomputer, while adept at constructing complex robot\nbehaviors. It provided structurally and logically correct BTs and demonstrated the ability\nto handle instructions that were not included in the training set.\nSong [155], as shown in Figure9, proposed LLM-Planner, a system designed for few-\nshot planning in embodied agents. LLM-Planner processed natural language instructions\nto generate high-level plans, selected subgoals from these plans, and identiﬁed actions via\na low-level planner. It continuously updated environmental information as new objects\nwere detected during action implementation and revisited the LLM to adjust the plan if\nsubgoals failed or were delayed based on updated observations. This iterative process was\nrepeated until the subgoal was achieved, after which the system moved to the next goal.\nCompared to traditional models such as HLSM and FILM, LLM-Planner demonstrated\ncompetitive performance with signiﬁcantly reduced training data and proved its ability to\ngeneralize in various tasks (e.g., ALFRED) with minimal examples.\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 9.LLM-Planner is a system that creates high-level plans based on natural language commands,\nsets subgoals to determine actions, and continuously updates the plan to reﬂect environmental\nchanges [155].\nSingh [156], as shown in Figure10, introduced ProgPrompt, a programmatic LLM\nprompt structure designed for generating plans across diverse situated environments,\nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that\nleveraged LLMs and included a Python programming structure to facilitate information\nabout the environment and executable actions. It featured a feedback mechanism, using\nexecutable program plan examples and assertion statements to mitigate errors, enhancing"
  },
  {
    "question": "Can you explain the differences between instruction tuning and alignment tuning?",
    "chunk": "quence and sequentially predicts output tokens individually. Examples of preﬁx decoder-\nbased LLMs include GLM-130B [108] and U-PaLM [109]. Additionally, various architec-\ntures have been proposed to address e ﬃciency challenges during training or inference \nwith long inputs, due to the quadratic computational complexity of the traditional trans-\nformer architecture. For instance, the Mixture-of-Experts (MoE) scaling method [34] \nsparsely activates a subset of the neural network for each input. \n \nFigure 2. Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx De-\ncoder (middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectan-\ngles represent attention between preﬁx tokens, attention between preﬁx and target tokens, atten-\ntion between target tokens, and masked attention [5]. \nIn terms of the tuning of LLMs, these models are essentially pre-trained on massive \ndatasets and require ﬁne-tuning for diﬀerent application domains. However, the consid-\nerable model size and number of parameters pose challenges for ﬁne-tuning on standard \ncomputers and GPUs. The subsequent sections will discuss methods to address these chal-\nlenges. \nLLM tuning is broadly divided into two cate gories based on the training objective. \nInstruction tuning is a form of supervised learning where the training data typically in-\nclude descriptions of tasks, inputs, and corresponding outputs. This type of tuning is de-\nsigned (1) to enhance the functional capabilities of LLMs, (2) to specialize them by training \nwith discipline-speci ﬁc information, and (3) to improve task generalization and con-\nsistency through a be tter understanding of natural language commands. Conversely, \nalignment tuning (or preference alignment) seeks to align the behavior of LLMs with hu-\nman values and preferences. Prominent methods include reinforcement learning from hu-\nman feedback (RLHF) [110], which involves ﬁne-tuning LLMs using human feedback to \nbetter reﬂect human values, and direct preference optimization (DPO) [111], focusing on \ntraining with pairs of human preferences th at usually include an input prompt and the \npreferred and non-preferred responses. \nFor both instruction tuning and alignment tuning, which involve training LLMs with \nextensively large model para meters, substantial GPU memo ry and computational re-\nsources are required, with high costs typi cally incurred when utilizing cloud-based \nFigure 2.Attention patterns in three mainstream architectures: Causal Decoder (left), Preﬁx Decoder\n(middle), and Encoder–Decoder (right). The blue, green, yellow, and grey rounded rectangles\nrepresent attention between preﬁx tokens, attention between preﬁx and target tokens, attention\nbetween target tokens, and masked attention [5].\nIn terms of the tuning of LLMs, these models are essentially pre-trained on mas-\nsive datasets and require ﬁne-tuning for different application domains. However, the\nconsiderable model size and number of parameters pose challenges for ﬁne-tuning on\nstandard computers and GPUs. The subsequent sections will discuss methods to address\nthese challenges.\nLLM tuning is broadly divided into two categories based on the training objective.\nInstruction tuning is a form of supervised learning where the training data typically include\ndescriptions of tasks, inputs, and corresponding outputs. This type of tuning is designed\n(1) to enhance the functional capabilities of LLMs, (2) to specialize them by training with"
  },
  {
    "question": "Can you explain the greedy search strategy in robotic planning?",
    "chunk": "sequential manipulation tasks that require long-horizon reasoning. Text2Motion interpreted\nnatural language instructions to formulate task plans and generated multiple candidate\nskill sequences, evaluating the geometric feasibility of each sequence. By employing a\ngreedy search strategy, it selected the optimal skill sequence to verify and execute the ﬁnal\nplan. This method enabled Text2Motion to perform complex sequential manipulation tasks\nwith a higher success rate compared to existing language-based planning methods, such\nas Saycan-gs and Innermono-gs, and provided semantically generalized characteristics\namong skills with geometric relationships.\nWu [160] investigated personalization in-home cleaning robots that organize and tidy\nspaces, using an LLM to convert user-provided object placement locations into generalized\nrules. By using a camera to identify objects and CLIP to categorize them, TidyBot efﬁciently\nrelocated objects according to these rules. This method attained an impressive accuracy\nof 91.2% for unseen objects in a benchmark dataset, which encompassed a variety of\nobjects, receptacles, and example placements of both “seen” and “unseen” objects across\n96 scenarios. Additionally, it achieved an 85% success rate in removing objects during\nreal-world tests.\n4.4. Manipulation by LLMs\nIn robotics research, the manipulation domain, which includes robotic arms and\nend effectors, encompasses various areas that beneﬁt from foundation models such as\nLLMs for language-based interactions and VLMs for object handling. Among the studies\nintegrating manipulation with foundation models, Stone [161] introduced an approach\ncalled manipulation of open-world objects (MOO). This approach determined whether a\nrobot could follow instructions involving unseen object categories by linking pre-trained\nmodels to robotic policies. MOO utilized pre-trained vision-language models to derive\nobject information from language commands and images, guiding the robot’s actions based\non the current image, command, and identiﬁed object data. Experimental use of real mobile\nmanipulation robots showed that MOO could adapt to new object types and environments\nin a zero-shot fashion. Moreover, MOO responded to non-verbal cues such as pointing at\nspeciﬁc objects, extending its scope to open-world exploration and manipulation.Appl. Sci.2024, 14, 8868 23 of 39\nExisting VLMs often lack a comprehensive understanding of physical concepts such\nas material and fragility, which limits their effectiveness in robotic manipulation tasks. To\naddress this issue, Gao [162] introduced PhysObjects, an object-centric dataset featuring\n39.6K crowd-sourced annotations and 417K automated annotations of physical concepts.\nThe automated annotations involved assigning speciﬁc concept values to predeﬁned object\ncategories or continuous concepts such as material and fragility. Fine-tuning a VLM on\nPhysObjects enhanced comprehension of physical concepts by capturing human biases\nrelated to the visual appearance of objects. Integrating this physically grounded VLM\nwith an LLM-based robotic planner framework improved performance in tasks requiring\nreasoning about physical concepts.\nThe traditional pre-training and ﬁne-tuning pipeline often suffers from decreased\nlearning efﬁciency and challenges in generalizing to unseen objects and tasks due to its\nreliance on domain-speciﬁc action information and domain-general visual information. To\naddress these limitations, Wang [163] proposed a modular approach named ProgramPort,\nwhich utilizes the syntactic and semantic structure of language instructions. Wang’s\nframework incorporated a semantic parser to reconstruct executable programs, composed of\nfunctional modules based on vision and action across multiple modalities. Each functional\nmodule combined deterministic computation with learnable neural networks. Program\nexecution involved generating parameters for general manipulation primitives used by the"
  },
  {
    "question": "What advancements have been made in dynamic simulation for robots?",
    "chunk": "4. Language Models for Robotic Intelligence\n4.1. Reward Design in Reinforcement Learning\nResearch in reinforcement learning, closely associated with the ﬁeld of robotics, has\nactively incorporated studies using LLM models. Speciﬁcally, Nvidia has developed a\nGPU-based multi-environment reinforcement learning platform. Utilizing its Omniverse\n3D virtual environment platform, Nvidia created Isaac Sim, which is dedicated to robot\nsimulation. Isaac Sim published research ﬁndings on Isaac Gym (Preview), which achieved\nsigniﬁcant reductions in reinforcement learning training times through GPU-based multi-\nenvironment approaches. Subsequently, Isaac Gym (Preview)’s features were integrated\ninto Isaac Sim and released as Omni Isaac Gym. Later, Nvidia introduced Orbit [133],\nfacilitating the simulation of PhysX 5.1-based cloth, soft-body, ﬂuid, and rigid-body dy-\nnamics, along with RGBD, LiDAR, and contact sensor simulation. Orbit also incorporates\nvarious robot platforms into the simulation environment. Recently, Orbit was updated\nto Isaac Lab and integrated into Isaac Sim 4.0. Nvidia has continuously advanced dy-\nnamic simulation environment technologies for reinforcement learning using GPU parallel\ncomputation. Leveraging this GPU reinforcement learning, they launched Eureka [11],\nwhich automates the design of reward functions for reinforcement learning using LLMs.\nFollowing this, Nvidia introduced DrEureka [134], an automated platform addressing the\nSim2Real problem [135] in reinforcement learning based on Eureka.\nEureka (Evolution-driven Universal REward Kit for Agent) [11], shown in Figure4,\nautomatically generates reward functions for various tasks using different robots, elimi-\nnating the need for speciﬁc templates or tailored reward functions for the robot’s form or\nexplanations for reinforcement learning tasks. Eureka consists of three main components:\nenvironment-as-context, evolutionary search, andreward reﬂection. Environment-as-context\ngenerates executable reward functions in a zero-shot manner by utilizing virtual environ-\nment source (Python) code as context. Evolutionary search iteratively generates reward\nfunction candidates and proposes enhanced functions based on previously generated and\nbest-performing ones, while also creating new functions through mutation. Reward reﬂec-\ntion offers a text summary of reward function quality based on training statistics recorded\nduring reinforcement learning, which assists in generating subsequent reward functions\nas feedback for the performance of previous functions. The reward functions generated\noutperformed expert-generated functions in 83% of benchmark tests. Moreover, Eureka\nsolved the pen spinning problem where a robot hand must spin a pen as much as possi-\nble according to predeﬁned rotations, a task previously considered unsolvable through\nmanual reward engineering. Eureka introduces a universal reward function design algo-\nrithm based on a code LLM and in-context evolutionary search, facilitating human-level\nreward generation for various robots and tasks without the need for prompt engineering or\nhuman intervention.\nFollowing Eureka, DrEureka [134], shown in Figure5, was developed to address\nthe sim-to-real problem by automatically conﬁguring appropriate reward functions and\ndomain randomization for physical environments. DrEureka’s reward-aware physics pri-\nors mechanism deﬁnes the lower and upper bounds of physical environment parameters\nbased on policies trained through initial reinforcement learning, facilitating reinforcement\nlearning across various physical environment domains. This randomization enables the\ntrained model to excel in actual environments. Consequently, DrEureka achieved bench-\nmark success in real-world quadruped locomotion with walking globe and cube-rotation"
  },
  {
    "question": "What is the significance of the exploration phase in AutoRT's data collection?",
    "chunk": "of-thought prompting techniques to RT-2 has proven effective in solving more complex\nsemantic inference tasks, such as using a rock as an improvised hammer or offering an\nenergy drink instead of a carbonated beverage to a thirsty person. In comparison with\nthe earlier study on RT-1, RT-2 demonstrates enhanced performance in both familiar and\nnovel tasks.\nAutoRT [10] is a follow-up study based on the research results of RT-1 and RT-2,\nestablishing an orchestration of large-scale robotic agents for data collection in real-world\nscenarios. AutoRT employed 53 robots to gather 77,000 real robot episodes over seven\nmonths through both teleoperation and autonomous robot policies. At the heart of AutoRT\nis a robust foundation model that generates ‘task proposals’ based on given visual observa-\ntions. Notably, AutoRT introduces a ‘Robot Constitution’ using constitutional prompting to\nensure actions during the task proposal process do not compromise the safety of the robot\nor nearby individuals. This Robot Constitution, inspired by Asimov’s three laws [142],\ncomprises basic rules, safety rules that identify unsafe or unwanted tasks, and embodiment\nrules that clarify the robot’s operational boundaries.\nAutoRT enhances data collection by initially scanning the surroundings to identify\ninteresting scenes or tasks (exploration). It interprets the given context through a VLM\nand proposes potential tasks via an LLM (task generation). Subsequently, tasks suggested\nby the LLM are screened (affordance) to assess their feasibility and the need for human\nintervention, employing the Robot Constitution. During this procedure, viable tasks are\nchosen and performed, while pertinent data are gathered (data collection). The collected\ndata are then assessed for (diversity scoring) the visual diversity of the robot trajectories\nand the linguistic diversity of the language instructions generated by AutoRT (LLM). The\naim of this diversity evaluation is to conﬁrm that, unlike simulations, real-world data\ncollection by robots is labor-intensive, making it essential to gather data across a broad\nspectrum of tasks. Experimental outcomes illustrate that AutoRT achieves higher visual\nand linguistic diversity compared to RT-1 or BC-Z [143].\nOther researchers include Tang [144], who developed an approach that connects\nnatural language user commands with a locomotion controller using foot contact patterns as\nan interface for low-level commands. This innovative interface translates human commands\ninto the robot’s foot contact patterns, allowing the robot to move at a speciﬁed speed with\nprecise timing for each foot’s contact with the ground. To achieve this, the robot used a\ncyclic sliding window to extract foot contact ﬂags from a pattern template, thus generating\nthe required foot contact patterns. During training, a random pattern generator created\nfoot contact patterns, and during testing, an LLM translated human commands into these\npatterns. The robot then adjusted its movements based on the foot contact patterns it\nlearned through deep reinforcement learning, closely adhering to the intended foot contact\npatterns and speed commands. This approach demonstrated a 50% higher success rate in\ntask evaluation (across 30 tasks, including standing still) compared to two baselines (which\nemployed discrete gaits and sinusoidal functions as interfaces), successfully solving 10\nmore tasks than the baselines.\nMandi [145] introduced a novel method for multi-robot collaboration that utilizes\nLLMs for both high-level communication and low-level path planning. In this method, the\nrobots employ the LLM to discuss and reason about task strategies. They generate sub-taskAppl. Sci.2024, 14, 8868 16 of 39\nplans and task space waypoint paths, which a multi-arm motion planner then uses to expe-"
  },
  {
    "question": "How are RGB-D data utilized in the robot's decision-making process?",
    "chunk": "including 6-DoF end-eﬀector waypoints, for various manipulation tasks using an open set \nof instructions and objects. Huang note d that LLMs were skilled at deriving a ﬀordances \nand constraints from free-form language in structions. Further, by harnessing code \ngeneration capabilities, Huang developed 3D value maps for the agent’s observation \nspace through interactions with VLMs. Thes e 3D value maps were integrated into a \nmodel-based planning framework to generate closed-loop robot trajectories robust to \ndynamic perturbations in a zero-shot approach. The proposed framework demonstrated \neﬃcient learning of the dynamics model for sc enes with contact-rich interactions and \nprovided advantages in these complex scenarios. \n \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM to \ngenerate 3D a ﬀordance and constraint maps and design robot trajectories without additional \ntraining [165]. \nAhn [166] introduced a framework named SayCan, which integrates LLMs with \nreinforcement learning value functions, enabling robots to follow high-level text \ninstructions. SayCan comprises two primary components: Say, which uses an LLM for \ntask-based decision-making, and Can, which evaluates the feasibility of these decisions \nvia reinforcement learning. Say leverages task-based knowledge from the LLM and \nreinforcement learning functionality to assess the feasibility of task execution by robots in \nreal-world scenarios. The LLM determines th e actions necessary to achieve high-level \ngoals and evaluates the eﬀectiveness of each action in fulﬁlling the instructions. Learned \nthrough reinforcement learning, the a ﬀordance function estimate s each action’s success \nprobability in the current state, con ﬁrming the executability of actions proposed by the \nLLM. This process allows the LLM to assess the robot’s current state and capabilities, \nultimately generating an interpretable action plan. SayCan was evaluated across 101 robot \ntasks, achieving an 84% plan success rate and a 74% execution success rate in a simulated \nkitchen environment. In a real kitchen setting, the plan success rate decreased slightly to \n81% and the execution success rate fell to 60%, demonstrating that the policy and value \nfunctions generalize well to real-world settings. \nFigure 12. Based on language instructions and RGB-D data, the LLM interacts with the VLM\nto generate 3D affordance and constraint maps and design robot trajectories without additional\ntraining [165].\nAhn [166] introduced a framework named SayCan, which integrates LLMs with rein-\nforcement learning value functions, enabling robots to follow high-level text instructions.\nSayCan comprises two primary components: Say, which uses an LLM for task-based\ndecision-making, and Can, which evaluates the feasibility of these decisions via reinforce-\nment learning. Say leverages task-based knowledge from the LLM and reinforcement\nlearning functionality to assess the feasibility of task execution by robots in real-world\nscenarios. The LLM determines the actions necessary to achieve high-level goals and\nevaluates the effectiveness of each action in fulﬁlling the instructions. Learned through re-\ninforcement learning, the affordance function estimates each action’s success probability in\nthe current state, conﬁrming the executability of actions proposed by the LLM. This process\nallows the LLM to assess the robot’s current state and capabilities, ultimately generating\nan interpretable action plan. SayCan was evaluated across 101 robot tasks, achieving an\n84% plan success rate and a 74% execution success rate in a simulated kitchen environment."
  },
  {
    "question": "How do robots create obstacle maps in real-time?",
    "chunk": "trained vision-language features with a 3D reconstruction of the physical world. VLMaps, \nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary \nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands. \nThese commands can be directly localized on a map and generate new obstacle maps in \nreal-time, facilitated by sharing among various robot types. Extensive experiments \nconducted in both simulated environments (using the Habitat simulator with the \nMatterport3D dataset and the AI2THOR simulator) and real-world settings (with the HSR \nmobile robot for indoor navigation) demonstrated that VLMs can navigate based on more \ncomplex language instructions than previous methods. The reviewed papers in this study \nare summarized in Table 5. \nTable 5. Summary of the reviewed papers in this study. \nName Explanation Ref. \nReward Design in \nRL \n• Eureka automatically generates and im proves reward functions based on the \nvirtual environment source code.$• Dr Eureka builds reward-aware physics \npriors using Eureka and supports eﬀective operation in the real world through \ndomain randomization.$• LLMs design and re ﬁne reward functions based on \nnatural language input.$• LLMs and VLMs integrate multimodal data to \ngenerate reward functions. \n[11,134,136–139,176–\n180] \nFigure 13.LM-Nav uses three pre-trained models: (a) VNM builds a topological graph from observa-\ntions, (b) LLM converts instructions into landmarks, (c) VLM matches landmarks to images, (d)A\ngraph search algorithm then ﬁnds the best robot trajectory, and (e) the robot executes the planned\npath [173].\nZhou [174] introduced NavGPT, an LLM-based navigation agent designed to follow\ninstructions. NavGPT is a vision-language navigation system that employs an LLM to trans-\nlate visual inputs from a visual foundation model (VFM) into natural language. The LLM\nthen interprets the current state and makes informed decisions to reach the intended goal,\nbased on these converted visuals, navigation history, and potential future routes. NavGPT\nconducts various functions, including high-level planning, decomposing instructions into\nsub-goals, identifying landmarks in observed scenes, monitoring navigation progress, and\nmodifying plans as necessary. Although NavGPT’s performance on zero-shot tasks from\nthe R2R dataset has not yet matched that of trained models, it underscored the potential\nof utilizing multi-modality inputs with LLMs for visual navigation and tapping into the\nexplicit reasoning capabilities of LLMs to enhance learned models.\nHuang [175] introduced VLMaps, a spatial map representation that integrates pre-\ntrained vision-language features with a 3D reconstruction of the physical world. VLMaps,\nwhen combined with an LLM, translate spatially organized sequences of open-vocabulary\nnavigation goals (e.g., “between the sofa and the TV”) into natural language commands.\nThese commands can be directly localized on a map and generate new obstacle maps in real-Appl. Sci.2024, 14, 8868 27 of 39\ntime, facilitated by sharing among various robot types. Extensive experiments conducted in\nboth simulated environments (using the Habitat simulator with the Matterport3D dataset\nand the AI2THOR simulator) and real-world settings (with the HSR mobile robot for\nindoor navigation) demonstrated that VLMs can navigate based on more complex language\ninstructions than previous methods. The reviewed papers in this study are summarized in\nTable 5."
  },
  {
    "question": "What is the significance of using human reward annotations in success detection?",
    "chunk": "agents’ understanding of human intentions. They developed a framework that utilized\nlanguage as a primary inference tool, investigating how it could address key challengesAppl. Sci.2024, 14, 8868 14 of 39\nin reinforcement learning, such as efﬁcient exploration, data reuse in experience, skill\nscheduling, and observational learning. This framework employed LLMs and VLMs to\naddress these reinforcement learning challenges by (1) efﬁciently exploring environments\nwith sparse rewards, (2) reusing collected data to sequentially bootstrap the learning of\nnew tasks, (3) scheduling learned skills for novel tasks, and (4) acquiring knowledge from\nobserving expert agents.\nDu [138] developed success detectors that identiﬁed whether actions or tasks were\nsuccessfully completed, utilizing the large multimodal language model Flamingo and\nhuman reward annotations. The study on success detection spanned three distinct do-\nmains: (1) interactive language-conditioned agents in simulated households, (2) real-world\nrobotic manipulation tasks (inserting and removing small, medium, and large gears), and\n(3) “in-the-wild” human egocentric videos. These success detectors adapted to new lan-\nguage instructions and visual changes using VLMs such as Flamingo, which were trained\non a broad range of language and visual data. Furthermore, success detection was reframed\nas a VQA problem, enabling the tracking of task progress through multiple frames to ascer-\ntain whether tasks had been successfully completed. The proposed method proved to be\nmore accurate in detecting success compared to custom reward models in the ﬁrst two do-\nmains, even with new language instructions or visual changes. However, success detection\nin unseen real-world videos in the third domain posed a more challenging generalization\ntask, underscoring the need for additional research.\nDu [139] introduced theELLM (exploring with LLMs) framework, which provided\nguidelines for pre-training reinforcement learning using LLMs. ELLM utilized the natural\nlanguage processing capabilities of LLMs to deﬁne goals and furnish reward functions for\nreinforcement learning agents. This strategy enabled agents to undertake meaningful explo-\nration and learning within their environments. The paper assessed ELLM’s performance in\ntwo settings: Crafter, a 2D version of Minecraft, and Housekeep, involving the task of rear-\nranging household objects. Experimental results demonstrated that ELLM surpassed other\nmethods in both settings. In the Crafter setting, ELLM attained high performance through\ngoal-oriented learning, proving especially effective in scenarios with sparse reward signals.\nIn the Housekeep setting, the agent conducted sensible exploration by adhering to goals\nset by the LLM, achieving a high success rate. While the accuracy of goal setting by the\nLLM varied with the objects and locations, it generally showed high performance. These\nexperimental ﬁndings suggested that ELLM was successful in enhancing reinforcement\nlearning performance across diverse environments, highlighting the vital role of providing\nreward signals based on human commonsense.\n4.2. Low-Level Control\nResearch is also being conducted on generating commands that directly control a\nrobot’s actuators (i.e., enabling low-level control) through various applications of LLM\nmodels. Among these projects, the Google research team developed RT-1 [8], which consists\nof ﬁlm-conditioned EfﬁcientNet-B3, TokenLearner, and Transformer. RT-1 is a model that\nreceives images and natural language instructions at a rate of 3Hz and outputs discretized\nrobot actions. RT-1 was trained on a vast demo dataset with over 130k episodes from more"
  },
  {
    "question": "What is VLN and how is it related to robotic navigation?",
    "chunk": "representation for natural language queries; then, an LLM-based object suggestion module\nreviews instructions, suggests relevant objects, and queries the scene for object availability\nand location. Using this information, the LLM planner devises plans uniquely tailored to\nthe scene’s context. NLMap equips robots with the ability to function without a predeﬁned\ncatalog of objects or actions, overcoming the constraints of earlier methods and enabling\nmore adaptable operations in environments with novel or absent objects.\nElhafsi [171] introduced a monitoring framework that employed an LLM with superior\ncontextual understanding and reasoning capabilities to detect edge cases and anomalies\nwithin vision-based policies. This framework monitored the robot’s perception stream\nthrough an LLM-based module, designed to detect semantic anomalies that might occur\nduring operations. By converting the robot’s visual observations into textual descriptions\nat regular intervals and integrating these into LLM prompts, it could pinpoint factors\nleading to policy errors, unsafe behavior, or task confusion. The conversion of visual\ninformation into natural language descriptions used various techniques, without restriction\nto any speciﬁc method. This ﬂexibility enabled both fully end-to-end policies and classical\nautonomy stacks using learned perception to align more closely with human intuition.\nThe ﬁndings indicated that semantic anomalies did not always correspond to semantically\nexplainable failures, and end-to-end policies could sometimes behave unpredictably.\nHon [172] introduced a new model family named 3D-LLM, which incorporated 3D\nworld information into LLMs. The 3D-LLM model utilized 3D point clouds and their\nfeatures as input, enabling it to handle a variety of spatially aware 3D tasks. These tasks\nincluded 3D captioning, dense captioning, 3D question answering, task decomposition, 3D\ngrounding, 3D-assisted dialogue, and navigation. The model used a 3D feature extractor to\nalign 3D features from multi-view images with language features, facilitating more precise\ntext generation and question answering based on spatial understanding. To train 3D-LLM,\na pre-trained 2D VLM formed the backbone, enhanced by the addition of 3D positional\nembeddings to better capture 3D spatial information. The model generated location tokens\nthrough linguistic descriptions of speciﬁc objects and was trained using 3D features as input.\nExperimental results showed that 3D-LLM excelled in various 3D-related tasks, achieving\napproximately a 9% higher BLEU-1 score compared to previous models on the ScanQA\ndataset. It demonstrated superior performance in 3D captioning, task composition, andAppl. Sci.2024, 14, 8868 26 of 39\n3D-assisted dialogue, outperforming 2D VLMs and displaying an improved understanding\nof object locations, shapes, and interactions.\nIn the extension of scene understanding using VLMs, the keyword VLN (vision-\nand-language navigation) is widely used in navigation-related research, where language\nfoundation models are increasingly utilized.\nShah [173], as shown in Figure13, introduced a robotic navigation system named\nLM-Nav, which capitalized on the advantages of training with large, unlabeled trajectory\ndatasets while providing a high-level interface for users. LM-Nav utilized three large-scale\npre-trained models: ViNG, CLIP, and GPT-3. Initially, the LLM translated natural language\ninstructions into a sequence of textual landmarks. The VLM integrated these textual\nlandmarks with images to identify the relevant images through probabilistic distribution.\nSubsequently, the VNM utilized these landmarks to plan and execute robot trajectories"
  },
  {
    "question": "What feedback mechanisms are used in ProgPrompt to reduce errors?",
    "chunk": "Figure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 9.LLM-Planner is a system that creates high-level plans based on natural language commands,\nsets subgoals to determine actions, and continuously updates the plan to reﬂect environmental\nchanges [155].\nSingh [156], as shown in Figure10, introduced ProgPrompt, a programmatic LLM\nprompt structure designed for generating plans across diverse situated environments,\nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that\nleveraged LLMs and included a Python programming structure to facilitate information\nabout the environment and executable actions. It featured a feedback mechanism, using\nexecutable program plan examples and assertion statements to mitigate errors, enhancing\ntask success rates. Additionally, ProgPrompt veriﬁed the current state through environ-\nmental feedback during plan execution and revised the plan accordingly. The results\nindicated that the integration of programming language features substantially improved\ntask performance in contexts such as VirtualHome and real-world manipulation tasks in\nterms of success rate, goal conditions recall, and executability.Appl. Sci.2024, 14, 8868 21 of 39\nAppl. Sci. 2024, 14, x FOR PEER REVIEW 21 of 39 \n \n \nFigure 9. LLM-Planner is a system that creates high-level plans based on natural language \ncommands, sets subgoals to determine actions, and continuously updates the plan to re ﬂect \nenvironmental changes [155]. \nSingh [156], as shown in Figure 10, introduced ProgPrompt, a programmatic LLM \nprompt structure designed for generating plans across diverse situated environments, \nrobot capabilities, and tasks. ProgPrompt functioned as a robot task-planning system that \nleveraged LLMs and included a Python progra mming structure to facilitate information \nabout the environment and executable action s. It featured a feedback mechanism, using \nexecutable program plan examples and assertion statements to mitigate errors, enhancing \ntask success rates. Additionally, ProgPrompt veri ﬁed the current state through \nenvironmental feedback during plan execut ion and revised the plan accordingly. The \nresults indicated that the integration of programming language features substantially \nimproved task performance in contexts such as VirtualHome and real-world \nmanipulation tasks in terms of success rate, goal conditions recall, and executability. \n \nFigure 10.ProgPrompt is a system that uses Python programming structures to provide environ-\nmental information and actions, enhancing the success rate of robot task planning through an error"
  },
  {
    "question": "What limitations do existing VLMs have in understanding physical concepts?",
    "chunk": "on the current image, command, and identiﬁed object data. Experimental use of real mobile\nmanipulation robots showed that MOO could adapt to new object types and environments\nin a zero-shot fashion. Moreover, MOO responded to non-verbal cues such as pointing at\nspeciﬁc objects, extending its scope to open-world exploration and manipulation.Appl. Sci.2024, 14, 8868 23 of 39\nExisting VLMs often lack a comprehensive understanding of physical concepts such\nas material and fragility, which limits their effectiveness in robotic manipulation tasks. To\naddress this issue, Gao [162] introduced PhysObjects, an object-centric dataset featuring\n39.6K crowd-sourced annotations and 417K automated annotations of physical concepts.\nThe automated annotations involved assigning speciﬁc concept values to predeﬁned object\ncategories or continuous concepts such as material and fragility. Fine-tuning a VLM on\nPhysObjects enhanced comprehension of physical concepts by capturing human biases\nrelated to the visual appearance of objects. Integrating this physically grounded VLM\nwith an LLM-based robotic planner framework improved performance in tasks requiring\nreasoning about physical concepts.\nThe traditional pre-training and ﬁne-tuning pipeline often suffers from decreased\nlearning efﬁciency and challenges in generalizing to unseen objects and tasks due to its\nreliance on domain-speciﬁc action information and domain-general visual information. To\naddress these limitations, Wang [163] proposed a modular approach named ProgramPort,\nwhich utilizes the syntactic and semantic structure of language instructions. Wang’s\nframework incorporated a semantic parser to reconstruct executable programs, composed of\nfunctional modules based on vision and action across multiple modalities. Each functional\nmodule combined deterministic computation with learnable neural networks. Program\nexecution involved generating parameters for general manipulation primitives used by the\nrobot’s end effector. The entire module network was trainable with an end-to-end imitation\nlearning objective. Experimental results demonstrated that the model effectively separated\naction and perception, achieving enhanced zero-shot and compositional generalization\nacross various manipulation tasks, speciﬁcally 16 tasks related to robot manipulation.\nHa [164] proposed a framework aimed at robot skill acquisition. This framework\nprovided a comprehensive solution by utilizing language guidance, without necessitating\nexpert demonstrations or reward speciﬁcation/engineering. It consisted of two main\ncomponents. The ﬁrst component, scaling up language-guided data generation, employed\nLLMs to break down tasks into subtasks and generate a hierarchical plan or task tree. This\nplan was materialized into various robot trajectories using 6-DoF exploration primitives.\nThese trajectories were subsequently veriﬁed and retries were performed as needed until\nsuccess was achieved. This approach enhanced the success rate of data collection and\nmore effectively mitigated the low-level understanding gap in LLMs by incorporating retry\nprocesses as part of the robot’s experiences. The second component, distilling down to\nlanguage-conditioned visuomotor policy, transformed robot experiences into a policy that\ndeduced control sequences from visual observations and natural language task descriptions.\nBy extending diffusion policies, this component handled language-based conditioning for\nmulti-task learning. To assess long-horizon behavior, commonsense reasoning, tool use,\nand intuitive physics, a new multi-task benchmark comprising 18 tasks related to robot\nmanipulation across ﬁve domains (mailbox, transport, drawer, catapult, and bus balance)\nwas developed. This benchmark effectively supported the learning of retry behaviors in\nthe data collection process and enhanced success rates.\nHuang [165], as shown in Figure12, aimed to synthesize dense robot trajectories,\nincluding 6-DoF end-effector waypoints, for various manipulation tasks using an open set"
  }
]